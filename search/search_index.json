{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M22504 Class Notes We have been actively developing our website, and this is version 2. We shall be implementing quite a bit of notes here so please, stay tuned. Contributions Guide Clone the repository on our Class' GitHub Organization Page . Run setup.sh or setup.cmd . Open /doc on your Markdown editor (suggestion: use Obsidian or Vim) and make your changes. If you created a new directory or file, make sure to add it to mkdocs.yml so it knows it's there. To deploy the site, go to the root directory and run either deploy.sh or deploy.cmd via the difference command lines (PowerShell Support coming soon). Remember to push whatever changes you've made so that they don't get erased. See changes on this website .","title":"Home"},{"location":"#m22504-class-notes","text":"We have been actively developing our website, and this is version 2. We shall be implementing quite a bit of notes here so please, stay tuned.","title":"M22504 Class Notes"},{"location":"#contributions-guide","text":"Clone the repository on our Class' GitHub Organization Page . Run setup.sh or setup.cmd . Open /doc on your Markdown editor (suggestion: use Obsidian or Vim) and make your changes. If you created a new directory or file, make sure to add it to mkdocs.yml so it knows it's there. To deploy the site, go to the root directory and run either deploy.sh or deploy.cmd via the difference command lines (PowerShell Support coming soon). Remember to push whatever changes you've made so that they don't get erased. See changes on this website .","title":"Contributions Guide"},{"location":"EL5131/","text":"EL5131 Topics Unit 1: Globalization Unit 1: US-China Conflicts Unit 1: Hybrid Workplaces Unit 1: Global Inequality Unit 1: The 4th Industrial Revolution Unit 2: Climate Change Unit 2: The Pandemic","title":"Home"},{"location":"EL5131/#el5131","text":"","title":"EL5131"},{"location":"EL5131/#topics","text":"Unit 1: Globalization Unit 1: US-China Conflicts Unit 1: Hybrid Workplaces Unit 1: Global Inequality Unit 1: The 4th Industrial Revolution Unit 2: Climate Change Unit 2: The Pandemic","title":"Topics"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/","text":"Stats and Quotes Science Examples Medicine and Pharmacology The National Heart Centre Singapore has proposed that scans of patients\u2019 hearts could be turned into 3D models. This has long-standing benefits as it helps cardiac surgeons make better surgery plans, especially for non-invasive procedures, and can also be used as a training tool for young doctors. Most importantly, the arrival of such technology would make animal testing obsolete; finally, human health need not be predicated on the suffering of animals, which is a step forward for mankind. Medco Health Solutions, one of America\u2019s largest pharmacy benefit managers until it was acquired by Express Solutions in 2011, was leading the way in making the provision of personal genomics services to the masses a reality. It had already achieved successes with personalized treatment of Warfarin, a widely used blood thinner to prevent clots, and Tamoxifen for breast cancer. Essentially, matching the right drugs to the patients has obvious clinical benefits, but also makes good economic sense. By reducing the occurrence of misdiagnosis, long hospitalization periods and the need for follow-up treatment, personalized healthcare can generate significant cost savings. In the United Kingdom, the National Healthcare System is collaborating with academia and industry to analyse the genetic material of persons with rare disorders through the 100,000 Genomes Project. It is hoped that such information will shed light on and save lives from disorders which were previously unknown. Ethics The high profile suicide of Dr Yoshiki Sasai, a disgruntled stem-cell biologist at the Riken Centre for Developmental Biology in Japan, after allegedly fabricating data for clinical trials published in international journals, is just the tip of the iceberg. Institutes place immense pressure on scientists to produce transformative research, at the expense of their well-being and integrity. Many scientists have fabricated research published in academic journals, with even peer-reviewed scientific journals to verify the results of such experiments. This may have far-reaching consequences in the future as fabrications not only cast doubt on the credibility of medical research but leave scientists vulnerable to making critical mistakes. In 2007, British bases in Basra were attacked by terrorist who had used aerial footage from Google Earth to identify target locations. In his bestselling book \u2018Homo Deus: A Brief History of Tomorrow\u2019, Professor Yuval Noah Harari commented that \u201cduring the Agricultural Revolution, humankind silenced animals and plants, and turned the animist grand opera into a dialogue between man and gods. During the Scientific Revolution, humankind silenced the gods too.\u201d In fact, the very premise of Harari\u2019s seemingly outlandish claims is that modern science has made men gods; although the best and the brightest understand that scientific research simply reflects the inadequacy of human knowledge, the accessibility and transformative power of modern science has granted us an immeasurable amount of power which we are wont to abuse. Genetic Engineering Aldous Huxley\u2019s Brave New World describes a dystopian world where infants are grown in vats and sorted according to their intelligence levels. Even back in 1937, when the novel was first published, the fear of dehumanization and infringing upon the sanctity of human life was already gaining traction among readers. The Green Revolution, which involved high yielding varieties (HYVs) and genetically modified crops, resulted in an exponential increase in global cereal production by 280% and in global consumption per capita by 25% between 1961 and 2004. Besides addressing hunger, the Green Revolution addressed other aspects of food security, such as nutritional inadequacy, with genetically modified crops. Golden Rice, touted to be a genetic marvel, was efficient in meeting the dietary requirements of those afflicted with Vitamin A deficiency. In societies where widespread famine has struck and there seemed to be no hope for improvement, the Green Revolution was an enormous breakthrough for poverty reduction efforts and indeed shines as a success story. Inequality and Discimination \"Talking Books\" the godfather of audiobooks was developed in 1930 in US for people who have impaired vision. Some hacked the system to increase reading speed, leading to audio time-stretching technology This shows disabilities driving developments They are key players not an afterthought in innovation Peer-Review Processes in Research Process Single Blind Double Blind How Authors are unaware of who the Reviewers are, but the Reviewers know who the Authors are Authors and Reviewers are both unaware of who the other is Why is DB Better? The name of the authors is a considerable factor in the peer review process. Anonymity ensures that quality of paper is assessed rather than the name of the authors Examples Peter Higgs, whose seminal paper postulating the existence of the Higgs Boson, was rejected by the Physics Letters Journal, one infamous for employing single blind protocol in peer review processes, although later it was proven to be true and Higgs even won a Nobel Prize for his work back in 2013. NYU Professor Yann LeCun, who is internationally regarded as one of the pioneers of modern AI development, recently posted on many Social Media sites with the proud admission that one of his most recent papers was rejected by the NeurIPS journal, one graded on double blind protocol. Stats COVID-19 Vaccination high-income economies with around 16% of the world's population have secured over 70% of 5 major vaccine doses for 2021 \u2003 Source: Challenges in ensuring global access to COVID-19 vaccines: production, affordability, allocation, and deployment \u2003 Notes: Research paper on theLancet, found on Fortune. Both are not so well known sources Covax has pledged to send 200 million vaccine shots to some 92 countries. However only 30 million has been received. \u2003 Article: The Straits Times - Problems at biggest vaccine maker in Indialeave world short on Covid-19 shots Serum's CEO, Adar Poonawalla, pledged 400 million doses for low and middle-income countries by the end of 2020. As of Jan 2021, only 70 million shots were manufactured. \u2003 Article: The Straits Times - Problems at biggest vaccine maker in Indialeave world short on Covid-19 shots \u2003 Notes: Mr Poonawalla mentions that the company is uncertain about when it would receive a licence from India and did not have enough warehouse space. Inequality and Discimination 58% of Women with STEM Degree have STEM Jobs, 70% of men with STEM Degree have STEM jobs \u2003 Source: Study by Nanyang Technological University \u2003 Notes: Dataset of 738 Singaporeans Medicine and Pharmacology About half of 117 million American adults are on medication for preventable chronic diseases such as diabetes. According to research conducted by health research firm Quintile IMS, the number of prescriptions filled for Americans increased by an astronomical 85% between 1997 and 2016, from 2.4 billion to 5.4 billion a year, even as the population increased by a mere 21% during this period. In Singapore, 10.5% of the adult population currently receive treatment for diabetes management. The Health Promotion Board estimates that by 2050, 1 in 3 Singaporeans will develop Type 2 diabetes. This worrying trend compelled the then Health Minister Gan Kim Yong to declare a \u2018war\u2019 on diabetes in 2016. Research conducted by the American Society of Plastic Surgeons suggest that Americans underwent close to 17.1 million cosmetic procedures in 2017 alone. Across the Pacific Ocean, the plastic surgery industry is booming, with more than 22 million undergoing cosmetic surgery in 2017. In the cases of polio and smallpox, where in collaboration with the World Health Organization, medical resources were able to reduce the prevalence of such diseases. The Global Polio Eradication Initiative reduced the number of cases of polio by 99% and more than 2 billion children have been immunized. The negative effects of smallpox were also short-lived as it was officially eradicated in 1980. It has been estimated that at least 20 million people would have died of smallpox had it not been eradicated. Technology Definitions Cancel Culture \"Cancel\": To effectively end their career or revoke their cultural cachet, whether through boycotts of their work or disciplinary action from an employer. \"Cancel Culture\": The culture of cancelling people who have done something bad in the eyes of the public, which often involves unfair attempts of online vigilantism, often effectively destroying one's life in the process. Big Tech Big Tech / FAGMA : Word/Acronym used to describe the five largest tech companies in the world, namely Facebook, Amazon, Google, Microsoft and Apple Sometimes includes Netflix, Telsa and Nvidia (FAANG) Big companies that have technology of mass public usage, and have a big market share. Apple was the first of these to achieve a two trillion dollar valuation in August 2020 The combined net worth of these five, in addition to five other top tech companies, rounds up to a whopping nine trillion dollars. The current stock market value of the Big Five ($9.3 trillion) is more than the value of the next 27 most valuable U.S. companies put together, including corporate giants like Tesla, Walmart and JPMorgan Chase, according to data from S&P Global Market Intelligence. BATX : Acronym used to describe China's four largest tech companies, that being Baidu, Alibaba, Tencent and Xiaomi. ByteDance is often counted into this list, despite it's name being missing from the actual acronym Beijing has, over the past two years, started to crack down on these companies, especially Alibaba. Data Collection : Collection of data for mass usage, eg MOST cookies, data stored on websites - Data is not distributed to alternative areas AI Theory of Affordance : a term coined by psychologist James J. Gibson. It states that when intelligent beings look at the world they perceive not simply objects and their relationships but also their possibilities . Eg, when we sit on a chair, the chair \"affords\" the possibility of sitting, whereas the same cannot be said about water, which instead \"affords\" the possibility of drinking or swimming in. The Coded Gaze : Defined by MIT grad student Joy Buolamwini as the bias in machine learning systems that involve lower accuracy for individuals of different skin tones and genders. Surveillence and Security \"Surveillance\": Either specifically highly targeted or aimless but covert monitoring of individuals, usually through audiovisual data collection \"CCTV\": inherent yet embraced form of surveillance in public spaces, less disconcerting \"Online tracking\": Infringement of rights in the form of sharing of data to other sources (very concerning) \"Stalkerware\": Defined by its ability to hide and stealthily record information from the device. As the name implies, it is used for stalking. Quotes \u201cAll of the biggest technological inventions created by man say little about his intelligence, but speak volumes about his laziness.\u201d ~ (widely attributed to) Mark Kennedy Cancel Culture \"Canceling is a way to acknowledge that you don\u2019t have to have the power to change structural inequality, You don\u2019t even have to have the power to change all of public sentiment. But as an individual, you can still have power beyond measure.\" ~ Charity Hudley \u201cAll I know is just what I read in the papers, and that\u2019s an alibi for my ignorance.\u201d ~ Will Rogers Morals and Ethics In surveys, people say they want an autonomous vehicle to protect pedestrians, even if it means sacrificing its passengers but also that they wouldn\u2019t buy selfdriving vehicles programmed to act in this way. \u2003 Source: Nature, Bonnefon, J. et al. Science 352, 1573\u20131576 (2016) \"Technology is not neutral\" \u2003 Source: Sam Gregory, program director of the human rights nonprofit Witness \u2003 Notes: On software created with intent of malicious usage \"Science in itself is morally neutral; it becomes good or evil according as it is applied\" \u2003 Source: Aldous Huxley Examples Social Media The Arab Spring of 2011 , where social media was used to rapidly spread awareness of and coordinate the protests. The uprising led to the overthrow of Ben Ali, Mubarak and Gaddafi in Tunisia, Egypt and Libya respectively, and civil war in most others. Notwithstanding the fact that those governments were oppressive and an uprising may perhaps be justified, the fact remains that the use of social media could turn the people against their leaders with unprecedented intensity and speed. In China, Facebook was used during the July 2009 Urumqi Riots by the minority Muslims in Xinjiang which led to the deaths of almost 200 people and almost 2,000 injured, in turn leading to the ban of the social media platform nation-wide. Netizens in China use \u201cMay 35th1989\u201d to refer to the date Tiananmen Incident\u2014\u201cJuly 4th2019\u201d\u2014which itself is a banned phrase as the incident remains taboo. In August 2019, the New York Times reported that China was using LinkedIn to recruit spies in the US, by posing as businessmen or academics inviting American targets to travel to China for an academic conference or presentation, or a business opportunity. From there, the Chinese agents start building a rapport with their targets and slowly convince them to hand over state secrets. Microsoft recently stopped LinkedIn's services in China due to lack of cooperation. The US is also a victim of Russia\u2019s use of Facebook to interfere in the 2016 Presidential Election that saw the election of Donald Trump\u2014at the time perceived by Russia as their preferred candidate. Exacerbating these problems is the proliferation of \u201cfake news\u201d, which many experts suspect not only influenced the outcome of the 2016 US Presidential Election, but the 2016 Brexit vote in the UK as well. In Singapore, the government passed a new piece of legislation in 2019\u2014the Protection from Online Falsehoods and Manipulation Act\u2014proving that social media poses a serious enough challenge to governments hoping to protect their citizens from malicious content. Internet users are particularly vulnerable to confirmation bias as online algorithms detect users\u2019 personal opinions based on their online behaviour, and upload relevant posts on their news feed accordingly. An extreme case happened in Turkey in 2016, when President Recep Tayyip Erdogan managed to survive a military coup by appealing to his supporters via a live stream on FaceTime. In 2011, American congressman Anthony Weiner was forced to resign after a sexually suggestive photo he sent to a woman via Twitter was leaked to the public. Launched in 1997, this website Six Degrees enabled users to upload a profile and make friends with other users. It was the predecessor of the many social networking sites to come, and its creation heralded the advent of a new era \u2013 an era in which social media would emerge as a new medium of communication. Cognizant of the increasing online presence of Singaporeans, the Singapore government created a website called eGov2015 in a bid to ensure that feedback from Singaporeans from all walks of life can be heard, and to facilitate greater co-creation and collaboration between the government and the people. The ALS Ice Bucket Challenge, started in the summer of 2014, raised $11.5 million for its cause. Over 17 million people uploaded their challenge videos to Facebook, and thus the cause was also championed by celebrities such as Taylor Swift, Bill Gates and Mark Zuckerberg. The challenge was started in a bid to raise awareness for the disease known as amyotrophic lateral sclerosis, and the use of social media spreading this challenge resulted in its resounding success.\\ The Trashtag Challenge, where social media users pick up litter, clean their environment and later post a photo of their accomplishments on social media. It has allowed users a better insight of the sheer scale of the plastics pollution problem. It is a good example of how social media can go further than raising awareness to mobilizing individuals to act, producing tangible results. Big Tech The NSA Scandal : It was revealed in 2013 that America's NSA and their British counterpart had been stealing information secretively from users of Google, Yahoo and many other tech products, with the creators of the products being completely oblivious to this happening. This led to widespread fear regarding government surveillance and stigmated views against Big Tech. Classified as \"surveillance\", since the data was used as a form of targeted monitoring to triangulate individuals based on the others' relative positions in order to identify potential threats. The Facebook-Cambridge Analytica Scandal : In 2016, after Trump's victory at the Election, it was revealed that the company it was working with, the British political firm Cambridge Analytica, had been surreptitiously stealing and harvesting data from Facebook users and in an effort to manipulate and brainwash them, by showing people against Trump's campaign more positive articles about his work in an act of political advertising to influence their decision to work. This massive scandal led to widespread stigma against Big Tech and especially Facebook, which had yet to come into the line of fire. However, Facebook was the most severely hit, with the years that followed being some of its worst. The Rohingya Genocide and Facebook's Involvement : The Rohingya Genocide happened as a result of the overly stigmatized views of people against the Rohingya Muslims, a set of muslims often considered some of the, if not the most discriminated people in the world. During the genocide, many of the military officers went on Facebook posing as popular individuals and purposely incited hate speech, which was one of the main catalysts for the truly inhumane acts that followed. The Christchurch Shootings : In March 2019, a man openly streamed his video on Facebook Live as he went and shot nearly 51 people in 2 mosques in Christchurch, New Zealand, in an effort to spread hate speech towards the Muslim population in New Zealand. The videos surfaced on Facebook, with many audience members surprisingly cheering on for the act to continue, leaving a bleak shot of the white nationalism ever present in Oceania today. While this may not be fully attributed to Facebook, it was nonetheless a display of one of Facebook's weakest moments. The Facebook Whistleblower, Frances Haughen : An ex-employee of Facebook, Frances Haughen had been secretly copying and storing hundreds of internal files in Facebook with regards to research it had done on the impacts of its algorithms to better serve their purpose as a tech company, and shared it with the Wall Street Journal (WSJ), leading to the creation of the \"Facebook Files\" series at the WSJ. Research done by Facebook itself found that angrier posts often got higher engagements , hence in 2018, Facebook's algorithms were reconfigured to give priority to these posts in user feeds. An implication of this would entail that more individuals would be angrier while reading the post and it is definitely possible that it may have led to incitement of more hate speech. This is incredibly problematic, but Facebook's insistence to focus on the profit made by more user retention due to this algorithm led to this being left to the wayside by the developers. During the sudden appearance of the pandemic, a large number of posts with contradictory information was shared on these social media platforms that left many users excessively confused. In fact, a study by the National Centre of Infectious Diseases (NCID) revealed that 6 in 10 Singaporeans received fake information regarding the pandemic. This information overload, or \"infodemic\", as termed by the World Health Organisation (WHO), has led to the spread of excessive amounts of misinformation. During events like the excessive aftermath of George Floyd's death and the Taliban's take-over of Afghanistan, an incredible number of posts led to these events being blown out of proportion. The Capitol Riots and Trump's Ban : After the absolutely horrific events of January 6th 2021, where a mob of individuals barreled their way into the US Capitol Building in support of Trump after his recent loss in the presidential elections, Trump was permanently banned from multiple social media platforms, like Facebook and Twitter. YouTube also banned him, and Apple and Google both refused to offer the Parler, which was commonly associated with Trump and his followers, on their respective mobile app stores. This was followed officially with the impeachment of Trump just a few short days of his term, but it also painted a picture of just how powerful Big Tech Countries, with their act of censorship making them act more like a government than a group of businesses. WhatsApp's Privacy Settings : In the beginning of 2021, millions of people around the globe received a message on their phones that WhatsApp was updating their privacy settings such that certain data would be stored by WhatsApp. Being an application recently acquired by Facebook, this message led to immediate terror and uproar amongst the general populace, with many worldwide immediately deleting the app and switching to Telegram and Signal, the latter of which was instigated by Tesla CEO Elon Musk. While these concerns ended up being overexaggerated, it was still notable that the general populace has become more receptive to privacy concerns and the impacts that these settings could have. Theranos has been a very clear indicator of how much trust we have in technology today. The scandal was one of the many such cases involving market fraud and the misleading of shareholders over the past few years and has been helmed as Silicon Valley\u2019s Greatest Disaster. With an idea as incredible as was the founder\u2019s, Elizabeth Holmes, who possessed a very limited amount of knowledge regarding medical trials and entrepreneurial methods and a small patent developed within a few five days, Theranos seemed doomed from the start, yet it grew from a simple start-up in 2003 to a highly valued company with a valuation of about 9 billion dollars in 2014. In a few short years, the medical company had grown to the point that Holmes was now the richest self-made female billionare in the world, yet all it had was a disappointing line-up of inaccurate products with fabricated results that left many flabbergasted. Holmes had somehow managed to convince stakeholders and even the board of directors of her own company to let her continue the fraud. Cancel Culture J.K. Rowling faced intense criticism after voicing transphobic beliefs, got cancelled. In June 2020, she published a transphobic manifesto, sales increased in her home country (Britain) Prejudice and Discrimination Mr Saidullah Karimi, a orthopedic technician and Afghan refugee, built a fully-functioning robot called Athena from recyclable parts to stand as a symbol of what refugees can accomplish and contribute to society if given the chance. In fact, refugees are often denied work by companies in Europe and the US since they are said to use different technology in their homeland, which is far from the truth. AI An AI chatbot developed in South Korea named Luda Lee, initially popular for its cheerful disposition and ability to talk like a human being, quickly became the source of controversy after it made inappropriate comments regarding women's rights, lesbianism and disability, joining a long list of such chatbots infamous for their polarising opinions, often contrary to public opinion, including Microsoft's Tay, Japan's Rinna and China's BabyQ. The company behind it was also sued for disseminating personal information of the users. Research done by the University of Montreal suggests that AI has better clinical decision-making capability than do humans, but generally in a more secondary sense, i.e. in the form of a second opinion. A horrifying application of AI has been tools developed by many that utilise DeepFake technology in the form of false shaming individuals. For example, a software called DeepNude was recently taken down which involved taking in the image of a face, and mapping that onto a Pornographic Film using DeepFake technology. \u2003 Source: https://www.technologyreview.com/2021/09/13/1035449/ai-deepfake-app-face-swaps-women-into-porn/ (MIT Technology Review, 13th September 2021) AI Dungeon, a text generator game utilising AI, was used by a few individuals to generate child pornography. \u2003 Source: Vikram Ramanathan, 21st September 2021 Baidu's Xiaodu is an all-inclusive technology to aid an oncoming \"Silver Tsunami\", providing many crucial services like access to emergency care and emotional support. AI algorithms developed by Big Tech and integrated into commerical and consumer products which drive the daily function of workplaces and people's lifestyles such as Google's search engine may gain more affordance than it should. Gebru Timnit, a researcher at Google's AI labs, grew wary of how quickly such technology were deployed and gaining dominance, and had criticise how black box language models developed by Google and competitors such as Microsoft's GPT-3, may for instance be trained on abusive language of the internet and develop biases yet elbow out promising alternatives, and later cause the future of AI technology to be built on shaky foundations... (List examples from other sections; e.g. coded gaze, stanford facial recognition; google's featured snippets). \u2003 Source: https://www.technologyreview.com/2021/06/14/1026148/ai-big-tech-timnit-gebru-paper-ethics/ (MIT Technology Review on Timnit Gebru's sacking, 14th June 2021) Use of AI in courts: see Science and Technology/Tech/AI/AI in courts. Example: Advisory AI: Civil Resolution Tribunal (CRT) in British Columbia, Canada for disputes relating to subsidised housing. Predictive AI: Correctional Offender Management Profiling for Alternative Sanctions for predicting recidivism, has been known to overestimate recidivism among African American defendants compared to Caucasian Americans (though this may be natural consequence of greater concentrations of black felons in some areas), judges using the tool detain people more than before although it was intended to suade judges towards the opposite outcome. This human control was an issue in the Loomis case, before the Supreme Court of Wisconsin.16 At stake were: (1) whether the use of the result of a risk assessment by an instrument such as COMPAS, where the operation is a business secret, violates the defendant\u2019s right to a fair trial because the secret operation deprives a defendant of the opportunity to test the accuracy and scientific value of the risk assessment, and (2) whether it violates the right to a fair trial to rely on such a risk assessment because it includes gender and race in the assessment of the risk of recurrence. The Wisconsin Supreme Court dismissed Loomis\u2019 objections, but said that the judge should give reasons as to how he or she uses COMPAS. The case was referred to the Supreme Court of the United States, which decided not to hear the case. Pros: Reduced Recidivism, streamlines process and makes it more accessible (cheaper and faster), possibly reduce bias of human jurors (more for felonies since human jurors are supposed to be competent), cases like divorce and employment contract terminations are predictable and the job of a court proceeding is often merely to check all the terms are fulfilled. Cons: In the US, predictive tools are offered commericially and workings are thus business secrets. Ethical Principles (mostly pertaining to previous point): Outlined by The Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe: Respect for fundamental rights. Ensure that design and implementation of AI services and tools are compatible with fundamental rights such as privacy, equal treatment and fair trial. Equal treatment. Avoid discrimination between individuals and groups of individuals. The example of COMPAS above shows that discrimination and unjustified distinction between individuals and groups, is a real risk. The data used by the algorithm may be the cause, and the prejudice may also be embedded in the algorithm itself. Data security. When processing judicial decisions and data, certified sources and data that cannot be altered should be used, with models that are multidisciplinary in design, in a secure technological environment. Transparency. Data processing methods should be made transparent and comprehensible, and external audits should be allowed. The requirement of transparency is now established case law. The user of an algorithm must make public the choices made, and the data and assumptions used, in a complete, timely and appropriate manner so that these choices, data and assumptions are accessible to third parties. Such full, timely and appropriate disclosure should make it possible to assess the choices made and the data, reasoning and assumptions used, so as to ensure effective legal protection against decisions based on those choices, data, reasoning and assumptions, with the possibility of judicial review by the courts. AI under user control. The algorithm may not be used as a prescription, i.e. the computer does not prescribe anything and cannot decide by itself. Users must know and understand what the AI does, and the users must be in control of the choices they make. This means that users must be able to deviate from the outcome of the algorithm without difficulty. What can happen when IT is blindly relied upon is shown by an example from the courts in the United Kingdom.18 There, a relatively simple piece of IT determines the financial capacity of (ex)-spouses in maintenance proceedings. The parties fill in a PDF form, and the IT calculates the resulting capacity. Due to a small mistake, which went unnoticed, incorrect calculations were made in 3,638 cases between April 2011 and January 2012, and between April 2014 and December 2015. Debts, instead of being deducted, had been added to the assets, so the assets taken into account were too high. In cases that were still pending, this could still be corrected. However, incorrect decisions were issued, and presumably complied with, in more than 2,200 cases. When researchers asked OpenAI\u2019s GPT-3 model, one of the largest and most sophisticated of these models ever created and generally considered to be one of the biggest AI breakthroughs in the past decade, to complete a sentence containing the word \u201cMuslims,\u201d it turned to violent language in more than 60% of cases \u2014 introducing words like bomb, murder, assault, and terrorism. A new report from Georgetown University\u2019s Center for Security and Emerging Technology (CSET) says there is little evidence to suggest there is a massive, urgent AI skills gap. According to the annual State of AI report, academic organizations don\u2019t have enough compute resources to accomplish their projects\u2014but 88% of \u201ctop AI faculty\u201d have received Big Tech funding, the researchers wrote. although AI safety is making headlines, \u201cfewer than 50 researchers\u201d are working on it full-time at the largest AI labs, according to the report \u2014 meaning there are a lot more people whose full-time job it is to build this tech rather than think about potential consequences. In a recent article by the newsletter, Emerging Tech Brew, the journalist talks about how 94% of AI systems studied are more inaccurate when compared to human radiologists. This statistic may seem pretty indicative of the relative nascence of AI, but it in fact demonstrates that many journalists have no clue as to how AI truly works, with statements like these being played for entertainment rather than truth. Autonomous Vehicles Moral Machine Experiment: a worldwide survey pertaining to choices made by Autonomous Vehicles in adverse situation wherein one or more are susceptible to death. Revealed different opinions across the world, with people residing in Asian Countries displaying a slight favouritism to the elderly while some expressed greater appreciation for Females. Stats Social Media According to the International Communication Union, there are more than 3 billion Internet users worldwide. According to a research conducted by Pew Research Centre on 700 American youth in 2018, close to 95% of teenagers have access to a smartphone, and 45% say that they are online on social media platforms on a \u2018almost constant\u2019 basis. The older generation is remarkably less adept at using technology and social media: while 90% of those aged 18-29 use at least one social media account, only 40% of those aged above 65 do. According to research by the Pew Research Centre, 47% of conservatives are likely to see Facebook posts aligned with their own views, as opposed to half that percentage for those who seek the middle ground. Since close to 1.7 billion people of the 7.2 billion people in the world today have active social media accounts, there is a smorgasbord of differing opinions online. Naturalistic research indicates that people who had conversations without the presence of a mobile phone felt that these conversations felt higher levels of \u2018empathetic concern\u2019, as opposed to those who conversed with the presence of a mobile phone. Autonomous Vehicles 9.1 crashes per million miles driven for self-driving cars, compared with 4.1 crashes per million miles for conventional vehicles. \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration Autonomous vehicles cause 0.36 injuries per crash, compared with 0.25 for conventional vehicles. \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration Self-driving cars are not responsible for any accidents \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration Big Tech and Big Data Combined stock market valuation of Apple, Alphabet, Nvidia, Tesla, Microsoft, Amazon and Facebook (i.e FAAMG + Nvidia + Tesla) is $3.4 trillion in 2020. \u2003 Article: CNBC - Tech\u2019s top seven companies added $3.4 trillion in value in 2020 80% of people have TraceTogether, reducing contact tracing from 4 days to 1.5 days \u2003 Article: CNA - Bill restricting police use of TraceTogether data introduced in Parliament, with tougher penalties for misuse \u2003 Source: Smart Nation and Digital Government Group (SNDGG) Singapore Free markets are creating a major free speech problem Companies retroactively policing those who express dissent against China. Many such cases: John Cena calling Taiwan a country, LeBron James & Morey in solidarity with HK, Mercedes Benz quoting Dalai Lama. While individually the cases are not major, the opinions were expressed on english platforms (some banned in China like Twitter), towards english-speaking audiences. PNTR (Permanent Normal Trade Relations which promised US-Chinese economic integration) during Clinton administration was not only for economic growth, but also for the US to abet the liberaization of the human potential of the people with American values in society and trade. (socially and economically since China is communist and micromanages private industries unlike US). However, unexpectedly censorship was smuggled into the US as to be elaborated. Apple TV streaming service disallows any content which criticises or paints China as a villain. While reasonable to market to Chinese consumers, a blanket categorical ban on any such content should ring alarm bells, especially when considering news agencies are pro-profit (slight stretch). Big tech is ultimately pulling the strings of countries as diplomatic agents as for instance since the chinese depend on iCloud and other technologies and does not want to fall behind the west and America does not want to falter in its ambition of liberating Chinese or lose economic primacy. While US and China are playing a precarious game of Twister, Big Tech (In this case FAANG rather than BATX) is free to hold their services hostage and leverage what type of content they will distribute or sponsor, thus influencing mass media, which is a threat to democracy yada yada. Big corporations are more likely to do well in bankruptcy court than small and medium-sized companies, according to a recent Brookings Institution study . Microsoft has aggressively pushed its new business messaging and collaboration tool, Microsoft Teams, which competes with the independent company Slack. On Thursday, Microsoft said the number of users on Teams had grown 37 percent in a week to more than 44 million daily users. There have been at least 900 million meeting and call minutes on Teams every day. Apple\u2019s profit just from the past three months ($21.7 billion) was nearly double the combined annual profits of the five largest U.S. airlines in prepandemic 2019. Amazon\u2019s stock price increases have made Jeff Bezos so rich that he could buy a new model iPhone for 200 million people \u2014 and he would still be a billionaire. Google\u2019s $50 billion in revenue from selling advertisements from April to June was about what Americans \u2014 all of the Americans \u2014 spent on gasoline and gas station purchases last month. The annual revenue of one of Microsoft\u2019s side businesses, LinkedIn, is nearly four times that of Zoom Video Communications, a star of the pandemic, in the past year. Facebook expects to dole out more cash outfitting its computer hubs and offices in 2021 than Exxon spends around the world to dig oil and gas out of the ground in a year. Amazon fell short of investors\u2019 expectations on Thursday. But in the past year, Amazon\u2019s e-commerce revenue still climbed by $109 billion \u2014 an increase in a single year that Walmart needed the past nine years to reach. Cancel Culture More than 90% of the doxed files included the victim's address, 61% included a phone number, and 53% included an email address. \u2003 Source: 2017 NYU - First large-scale doxing study reveals motivations and targets for cyber bullying \u2003 Notes: Over 5500 files of doxxing, implies over 5500 cases of doxxing as sample size 12% of the students acknowledged their engagement in doxing. \u2003 Source: Int J Environ Res Public Health - Doxing: What Adolescents Look for and Their Intentions \u2003 Notes: Survey of 2120 Hong Kong students Surveillance Ten of 24 US Governmental agencies surveyed plan to broaden their use of facial recognition systems by 2023, while eighteen of 24 currently use some form of the technology, according to an August 2021 report by the US Government Accountability Office (GAO). Facial recognition has proved to be less accurate on people with darker skin , women, and younger and older people, according to a 2018 paper presented by MIT and Stanford University researchers. This was due to the fact that dataset used to assess its performance was more than 77 percent male and more than 83 percent white. Leading to a disparity of nearly 46% for some individuals. Your face is not a barcode - Arguments against Facial Recognition Technology (2003) Comprehensive list of arguments against the use of facial recognition and rebuttals of common arguments. Large potential by abuse by governments (e.g. China & Uyghers) or becoming compromised by hackers As opposed to other similar technologies like fingerprinting, facial recognition requires least cooperation from the individual as one can be identified in a crowd from afar. Thus it is quite open to attack vectors. Quite fallible to false positives and inaccuracies from poor lighting conditions e.g. Inefficient among all the false positives, i.e. 1 terrorist among 1000 false positives. Other measures may be preferable for such use-cases, despite scares pushing towards facial recognition post-9/11. Hard to give effective notice, and gain proper consent. Adoption in public spaces such as government buildings or other institutions means it is a fait accompli that those uncomfortable with the technology have to accept. Technology may not necessarily be only accessible and implemented by the government but also by businesses or district authorities, who may wittingly or unwittingly abuse the technology Sets a poor precedents as even if it is well-implemented by wealthier and democratic governments with respect for civil liberties, it may gain adoption among less enlightened governments. For instance, a report by The Conservation where some Chinese citizens were informally interviewed on their thoughts on the government's inexorable push for social surveillance systems believed the west also had similar concepts of social credit which China is justified to follow to catch up with other superpowers. They welcome the infringements of the civil liberties in favor of the public order solidified by such developments (May not necessarily be deluded, just differing culture). If the west must adopt facial recognition to protect the nation or safeguard their democracy, then the artificial 'Tian' watching over Chinese citizens or similar facsimilies that may prop up in other nations will only gain in prevalance and apparent signficance in protecting a nation's civil morals. Facial recognition used to catch felons at a Superbowl in Tampa, Florida 2001 According to recent research by the Center for Democracy and Technology (CDT), \u201c86% of teachers reported that, during the pandemic, schools provided tablets, laptops, or Chromebooks to students at twice the rate (43%) prior to the pandemic, an illustration of schools\u2019 attempts to close disparities in digital access.\u201d More than 80% of surveyed teachers and 77% of surveyed high school students told the CDT that their schools use surveillance software on those devices, and the more reliant students are on those electronics, unable to afford supplementary phones or tablets, the more they are subjected to scrutiny. Security 76.1% of websites use https by default, a more secure protocol to access websites, while the 85.5% top 1 million websites use https by default \u2003 Source: W3Techs \u2003 Notes: Referenced by New York Times Morals and Ethics Estimated 90% - 95% of all online deepfake videos are nonconsensual porn, 90% of the porn feature women \u2003 Article: technlogyreview.com - A horrifying new AI app swaps women into porn videos with a click \u2003 Source: research company Sensity AI \u2003 Notes: IT IS AN ESTIMATION!! Youth Children who used the internet, social media or video games for entertainment \u22654 hours daily were 4x more likely to skip school. \u2003 Source: Rutgers University analysing China Education Panel Survey \u2003 Notes: 10000 1st year middle schoolers, mean age = 13.5 Boys used interactive technology for entertainment significantly more than girls. Boys also performed worse and showed lower engagement levels than girls \u2003 Source: Rutgers University analysing China Education Panel Survey \u2003 Notes: 10000 1st year middle schoolers, mean age = 13.5 Tech Talent Crunch Singaporeans accounted for about 35 per cent of tech jobs, and 35 per cent of net tech jobs created in the past five years in banking and finance. \u2003 Article: NTU Business - What is stopping more Singaporeans from taking up tech jobs? \u2003 Source: Monetary Authority of Singapore (MAS) There appears to be a general lack of students graduating with degrees in Computer Science-related fields, with only 2,800 such graduates every year, compared to the demand of nearly 60,000 needed by 2023, according to Dr Vivian Balakrishnan, Minister-in-charge of the Smart Nation Initiative. The labour shortage by 2030 could cost Singapore nearly 39.2 billion SGD, which is very undesirable for the city state, since it could possibly account for 21 percent of Singapore\u2019s projected economy by then. Only a third of the technological workforce in the financial sector is made of Singaporeans AI In 2019, the AI workforce made up 9% of total US employment. And over the next decade, employment in AI-related occupations is projected to grow twice as fast as employment in all occupations, according to data from the Bureau of Labor Statistics. The US needs to increase its supply of AI PhDs since research suggests there\u2019s a supply-demand gap for top-tier talent; \u201csustain and diversify\u201d pipelines for technical AI roles; and introduce AI education into K-12 curriculums. Prejudice and Discrimination Examples In some of the early chapters of Harper Lee's 1960 novel, To Kill A Mockingbird , we are introduced to the character of Ms Caroline and young schoolteacher who had come from another town. The chapters expand on some of the classroom sessions, showing some of the ways Ms Caroline was unaware of many of the cultural norms the rest had beome very much accustomed to. While a stretch, this can be well-reasoned to the issues that many foreigners experience in different countries, since these countries have differing systems and adjusting to these requires time. Stats Women and early-career academics are more likely to feel like impostors in an academic discipline perceived to require raw talent for success. \u2003 Source: New York University 2021 Study Women in Tech A report by Microsoft found that more than 50% of young females are in fact interested in working in the STEM sector. Only 58% of females Singaporeans, after pursuing a degree in STEM, go on to STEM-related careers. According to the UNESCO Institute for Statistics, less that 30 percent of the world\u2019s researchers are female. Most women drop off the register of professional engineers before the age of 45. Meritocracy & Elitism Harvard, Princeton, Stanford, Yale collectively enroll top 1% of income distribution than from households from bottom 60% In US, 7 top banks and law firms recruit exclusively from elite colleges In 2006 study by American University, 1% of children in bottom 20% of households will join top 5%. Pew Research Centre: 60% of Republicans believe colleges and universities are bad for the US. Burdening the rich is a non-sequitor to reducing inequality as severe income disparities cause the elite to become more competitive to hold onto their status, get a return on their investment in social capital (education, networking etc.), and provide similar resources to their children for succeeding their wealth and status. Elite Overproduction : Coined by Peter Turchin, describing a phase of a complex society involving a bloated elite class, too few elite jobs, declining living standards among general population. The Next Decade Could Be Even Worse Crime and Punishment Examples In the 1994 prison drama, The Shawshank Redemption , the life story of an innocent man thrust into prison is shown, spanning nearly 20 years. As we progress through the story, we see him slowly but surely see the cruel injustices faced in prison, with his management of the corrupt warden's ledger. While an offense on its own, its rather ironic for a man as situationally perpetuated to be innocent as the warden to take over the role of a criminal. Unfortunately that is the case.","title":"Stats and Quotes"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#stats-and-quotes","text":"","title":"Stats and Quotes"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#science","text":"","title":"Science"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#examples","text":"","title":"Examples"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#medicine-and-pharmacology","text":"The National Heart Centre Singapore has proposed that scans of patients\u2019 hearts could be turned into 3D models. This has long-standing benefits as it helps cardiac surgeons make better surgery plans, especially for non-invasive procedures, and can also be used as a training tool for young doctors. Most importantly, the arrival of such technology would make animal testing obsolete; finally, human health need not be predicated on the suffering of animals, which is a step forward for mankind. Medco Health Solutions, one of America\u2019s largest pharmacy benefit managers until it was acquired by Express Solutions in 2011, was leading the way in making the provision of personal genomics services to the masses a reality. It had already achieved successes with personalized treatment of Warfarin, a widely used blood thinner to prevent clots, and Tamoxifen for breast cancer. Essentially, matching the right drugs to the patients has obvious clinical benefits, but also makes good economic sense. By reducing the occurrence of misdiagnosis, long hospitalization periods and the need for follow-up treatment, personalized healthcare can generate significant cost savings. In the United Kingdom, the National Healthcare System is collaborating with academia and industry to analyse the genetic material of persons with rare disorders through the 100,000 Genomes Project. It is hoped that such information will shed light on and save lives from disorders which were previously unknown.","title":"Medicine and Pharmacology"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#ethics","text":"The high profile suicide of Dr Yoshiki Sasai, a disgruntled stem-cell biologist at the Riken Centre for Developmental Biology in Japan, after allegedly fabricating data for clinical trials published in international journals, is just the tip of the iceberg. Institutes place immense pressure on scientists to produce transformative research, at the expense of their well-being and integrity. Many scientists have fabricated research published in academic journals, with even peer-reviewed scientific journals to verify the results of such experiments. This may have far-reaching consequences in the future as fabrications not only cast doubt on the credibility of medical research but leave scientists vulnerable to making critical mistakes. In 2007, British bases in Basra were attacked by terrorist who had used aerial footage from Google Earth to identify target locations. In his bestselling book \u2018Homo Deus: A Brief History of Tomorrow\u2019, Professor Yuval Noah Harari commented that \u201cduring the Agricultural Revolution, humankind silenced animals and plants, and turned the animist grand opera into a dialogue between man and gods. During the Scientific Revolution, humankind silenced the gods too.\u201d In fact, the very premise of Harari\u2019s seemingly outlandish claims is that modern science has made men gods; although the best and the brightest understand that scientific research simply reflects the inadequacy of human knowledge, the accessibility and transformative power of modern science has granted us an immeasurable amount of power which we are wont to abuse.","title":"Ethics"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#genetic-engineering","text":"Aldous Huxley\u2019s Brave New World describes a dystopian world where infants are grown in vats and sorted according to their intelligence levels. Even back in 1937, when the novel was first published, the fear of dehumanization and infringing upon the sanctity of human life was already gaining traction among readers. The Green Revolution, which involved high yielding varieties (HYVs) and genetically modified crops, resulted in an exponential increase in global cereal production by 280% and in global consumption per capita by 25% between 1961 and 2004. Besides addressing hunger, the Green Revolution addressed other aspects of food security, such as nutritional inadequacy, with genetically modified crops. Golden Rice, touted to be a genetic marvel, was efficient in meeting the dietary requirements of those afflicted with Vitamin A deficiency. In societies where widespread famine has struck and there seemed to be no hope for improvement, the Green Revolution was an enormous breakthrough for poverty reduction efforts and indeed shines as a success story.","title":"Genetic Engineering"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#inequality-and-discimination","text":"\"Talking Books\" the godfather of audiobooks was developed in 1930 in US for people who have impaired vision. Some hacked the system to increase reading speed, leading to audio time-stretching technology This shows disabilities driving developments They are key players not an afterthought in innovation","title":"Inequality and Discimination"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#peer-review-processes-in-research","text":"Process Single Blind Double Blind How Authors are unaware of who the Reviewers are, but the Reviewers know who the Authors are Authors and Reviewers are both unaware of who the other is Why is DB Better? The name of the authors is a considerable factor in the peer review process. Anonymity ensures that quality of paper is assessed rather than the name of the authors Examples Peter Higgs, whose seminal paper postulating the existence of the Higgs Boson, was rejected by the Physics Letters Journal, one infamous for employing single blind protocol in peer review processes, although later it was proven to be true and Higgs even won a Nobel Prize for his work back in 2013. NYU Professor Yann LeCun, who is internationally regarded as one of the pioneers of modern AI development, recently posted on many Social Media sites with the proud admission that one of his most recent papers was rejected by the NeurIPS journal, one graded on double blind protocol.","title":"Peer-Review Processes in Research"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#stats","text":"","title":"Stats"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#covid-19-vaccination","text":"high-income economies with around 16% of the world's population have secured over 70% of 5 major vaccine doses for 2021 \u2003 Source: Challenges in ensuring global access to COVID-19 vaccines: production, affordability, allocation, and deployment \u2003 Notes: Research paper on theLancet, found on Fortune. Both are not so well known sources Covax has pledged to send 200 million vaccine shots to some 92 countries. However only 30 million has been received. \u2003 Article: The Straits Times - Problems at biggest vaccine maker in Indialeave world short on Covid-19 shots Serum's CEO, Adar Poonawalla, pledged 400 million doses for low and middle-income countries by the end of 2020. As of Jan 2021, only 70 million shots were manufactured. \u2003 Article: The Straits Times - Problems at biggest vaccine maker in Indialeave world short on Covid-19 shots \u2003 Notes: Mr Poonawalla mentions that the company is uncertain about when it would receive a licence from India and did not have enough warehouse space.","title":"COVID-19 Vaccination"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#inequality-and-discimination_1","text":"58% of Women with STEM Degree have STEM Jobs, 70% of men with STEM Degree have STEM jobs \u2003 Source: Study by Nanyang Technological University \u2003 Notes: Dataset of 738 Singaporeans","title":"Inequality and Discimination"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#medicine-and-pharmacology_1","text":"About half of 117 million American adults are on medication for preventable chronic diseases such as diabetes. According to research conducted by health research firm Quintile IMS, the number of prescriptions filled for Americans increased by an astronomical 85% between 1997 and 2016, from 2.4 billion to 5.4 billion a year, even as the population increased by a mere 21% during this period. In Singapore, 10.5% of the adult population currently receive treatment for diabetes management. The Health Promotion Board estimates that by 2050, 1 in 3 Singaporeans will develop Type 2 diabetes. This worrying trend compelled the then Health Minister Gan Kim Yong to declare a \u2018war\u2019 on diabetes in 2016. Research conducted by the American Society of Plastic Surgeons suggest that Americans underwent close to 17.1 million cosmetic procedures in 2017 alone. Across the Pacific Ocean, the plastic surgery industry is booming, with more than 22 million undergoing cosmetic surgery in 2017. In the cases of polio and smallpox, where in collaboration with the World Health Organization, medical resources were able to reduce the prevalence of such diseases. The Global Polio Eradication Initiative reduced the number of cases of polio by 99% and more than 2 billion children have been immunized. The negative effects of smallpox were also short-lived as it was officially eradicated in 1980. It has been estimated that at least 20 million people would have died of smallpox had it not been eradicated.","title":"Medicine and Pharmacology"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#technology","text":"","title":"Technology"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#definitions","text":"","title":"Definitions"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#cancel-culture","text":"\"Cancel\": To effectively end their career or revoke their cultural cachet, whether through boycotts of their work or disciplinary action from an employer. \"Cancel Culture\": The culture of cancelling people who have done something bad in the eyes of the public, which often involves unfair attempts of online vigilantism, often effectively destroying one's life in the process.","title":"Cancel Culture"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#big-tech","text":"Big Tech / FAGMA : Word/Acronym used to describe the five largest tech companies in the world, namely Facebook, Amazon, Google, Microsoft and Apple Sometimes includes Netflix, Telsa and Nvidia (FAANG) Big companies that have technology of mass public usage, and have a big market share. Apple was the first of these to achieve a two trillion dollar valuation in August 2020 The combined net worth of these five, in addition to five other top tech companies, rounds up to a whopping nine trillion dollars. The current stock market value of the Big Five ($9.3 trillion) is more than the value of the next 27 most valuable U.S. companies put together, including corporate giants like Tesla, Walmart and JPMorgan Chase, according to data from S&P Global Market Intelligence. BATX : Acronym used to describe China's four largest tech companies, that being Baidu, Alibaba, Tencent and Xiaomi. ByteDance is often counted into this list, despite it's name being missing from the actual acronym Beijing has, over the past two years, started to crack down on these companies, especially Alibaba. Data Collection : Collection of data for mass usage, eg MOST cookies, data stored on websites - Data is not distributed to alternative areas","title":"Big Tech"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#ai","text":"Theory of Affordance : a term coined by psychologist James J. Gibson. It states that when intelligent beings look at the world they perceive not simply objects and their relationships but also their possibilities . Eg, when we sit on a chair, the chair \"affords\" the possibility of sitting, whereas the same cannot be said about water, which instead \"affords\" the possibility of drinking or swimming in. The Coded Gaze : Defined by MIT grad student Joy Buolamwini as the bias in machine learning systems that involve lower accuracy for individuals of different skin tones and genders.","title":"AI"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#surveillence-and-security","text":"\"Surveillance\": Either specifically highly targeted or aimless but covert monitoring of individuals, usually through audiovisual data collection \"CCTV\": inherent yet embraced form of surveillance in public spaces, less disconcerting \"Online tracking\": Infringement of rights in the form of sharing of data to other sources (very concerning) \"Stalkerware\": Defined by its ability to hide and stealthily record information from the device. As the name implies, it is used for stalking.","title":"Surveillence and Security"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#quotes","text":"\u201cAll of the biggest technological inventions created by man say little about his intelligence, but speak volumes about his laziness.\u201d ~ (widely attributed to) Mark Kennedy","title":"Quotes"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#cancel-culture_1","text":"\"Canceling is a way to acknowledge that you don\u2019t have to have the power to change structural inequality, You don\u2019t even have to have the power to change all of public sentiment. But as an individual, you can still have power beyond measure.\" ~ Charity Hudley \u201cAll I know is just what I read in the papers, and that\u2019s an alibi for my ignorance.\u201d ~ Will Rogers","title":"Cancel Culture"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#morals-and-ethics","text":"In surveys, people say they want an autonomous vehicle to protect pedestrians, even if it means sacrificing its passengers but also that they wouldn\u2019t buy selfdriving vehicles programmed to act in this way. \u2003 Source: Nature, Bonnefon, J. et al. Science 352, 1573\u20131576 (2016) \"Technology is not neutral\" \u2003 Source: Sam Gregory, program director of the human rights nonprofit Witness \u2003 Notes: On software created with intent of malicious usage \"Science in itself is morally neutral; it becomes good or evil according as it is applied\" \u2003 Source: Aldous Huxley","title":"Morals and Ethics"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#examples_1","text":"","title":"Examples"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#social-media","text":"The Arab Spring of 2011 , where social media was used to rapidly spread awareness of and coordinate the protests. The uprising led to the overthrow of Ben Ali, Mubarak and Gaddafi in Tunisia, Egypt and Libya respectively, and civil war in most others. Notwithstanding the fact that those governments were oppressive and an uprising may perhaps be justified, the fact remains that the use of social media could turn the people against their leaders with unprecedented intensity and speed. In China, Facebook was used during the July 2009 Urumqi Riots by the minority Muslims in Xinjiang which led to the deaths of almost 200 people and almost 2,000 injured, in turn leading to the ban of the social media platform nation-wide. Netizens in China use \u201cMay 35th1989\u201d to refer to the date Tiananmen Incident\u2014\u201cJuly 4th2019\u201d\u2014which itself is a banned phrase as the incident remains taboo. In August 2019, the New York Times reported that China was using LinkedIn to recruit spies in the US, by posing as businessmen or academics inviting American targets to travel to China for an academic conference or presentation, or a business opportunity. From there, the Chinese agents start building a rapport with their targets and slowly convince them to hand over state secrets. Microsoft recently stopped LinkedIn's services in China due to lack of cooperation. The US is also a victim of Russia\u2019s use of Facebook to interfere in the 2016 Presidential Election that saw the election of Donald Trump\u2014at the time perceived by Russia as their preferred candidate. Exacerbating these problems is the proliferation of \u201cfake news\u201d, which many experts suspect not only influenced the outcome of the 2016 US Presidential Election, but the 2016 Brexit vote in the UK as well. In Singapore, the government passed a new piece of legislation in 2019\u2014the Protection from Online Falsehoods and Manipulation Act\u2014proving that social media poses a serious enough challenge to governments hoping to protect their citizens from malicious content. Internet users are particularly vulnerable to confirmation bias as online algorithms detect users\u2019 personal opinions based on their online behaviour, and upload relevant posts on their news feed accordingly. An extreme case happened in Turkey in 2016, when President Recep Tayyip Erdogan managed to survive a military coup by appealing to his supporters via a live stream on FaceTime. In 2011, American congressman Anthony Weiner was forced to resign after a sexually suggestive photo he sent to a woman via Twitter was leaked to the public. Launched in 1997, this website Six Degrees enabled users to upload a profile and make friends with other users. It was the predecessor of the many social networking sites to come, and its creation heralded the advent of a new era \u2013 an era in which social media would emerge as a new medium of communication. Cognizant of the increasing online presence of Singaporeans, the Singapore government created a website called eGov2015 in a bid to ensure that feedback from Singaporeans from all walks of life can be heard, and to facilitate greater co-creation and collaboration between the government and the people. The ALS Ice Bucket Challenge, started in the summer of 2014, raised $11.5 million for its cause. Over 17 million people uploaded their challenge videos to Facebook, and thus the cause was also championed by celebrities such as Taylor Swift, Bill Gates and Mark Zuckerberg. The challenge was started in a bid to raise awareness for the disease known as amyotrophic lateral sclerosis, and the use of social media spreading this challenge resulted in its resounding success.\\ The Trashtag Challenge, where social media users pick up litter, clean their environment and later post a photo of their accomplishments on social media. It has allowed users a better insight of the sheer scale of the plastics pollution problem. It is a good example of how social media can go further than raising awareness to mobilizing individuals to act, producing tangible results.","title":"Social Media"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#big-tech_1","text":"The NSA Scandal : It was revealed in 2013 that America's NSA and their British counterpart had been stealing information secretively from users of Google, Yahoo and many other tech products, with the creators of the products being completely oblivious to this happening. This led to widespread fear regarding government surveillance and stigmated views against Big Tech. Classified as \"surveillance\", since the data was used as a form of targeted monitoring to triangulate individuals based on the others' relative positions in order to identify potential threats. The Facebook-Cambridge Analytica Scandal : In 2016, after Trump's victory at the Election, it was revealed that the company it was working with, the British political firm Cambridge Analytica, had been surreptitiously stealing and harvesting data from Facebook users and in an effort to manipulate and brainwash them, by showing people against Trump's campaign more positive articles about his work in an act of political advertising to influence their decision to work. This massive scandal led to widespread stigma against Big Tech and especially Facebook, which had yet to come into the line of fire. However, Facebook was the most severely hit, with the years that followed being some of its worst. The Rohingya Genocide and Facebook's Involvement : The Rohingya Genocide happened as a result of the overly stigmatized views of people against the Rohingya Muslims, a set of muslims often considered some of the, if not the most discriminated people in the world. During the genocide, many of the military officers went on Facebook posing as popular individuals and purposely incited hate speech, which was one of the main catalysts for the truly inhumane acts that followed. The Christchurch Shootings : In March 2019, a man openly streamed his video on Facebook Live as he went and shot nearly 51 people in 2 mosques in Christchurch, New Zealand, in an effort to spread hate speech towards the Muslim population in New Zealand. The videos surfaced on Facebook, with many audience members surprisingly cheering on for the act to continue, leaving a bleak shot of the white nationalism ever present in Oceania today. While this may not be fully attributed to Facebook, it was nonetheless a display of one of Facebook's weakest moments. The Facebook Whistleblower, Frances Haughen : An ex-employee of Facebook, Frances Haughen had been secretly copying and storing hundreds of internal files in Facebook with regards to research it had done on the impacts of its algorithms to better serve their purpose as a tech company, and shared it with the Wall Street Journal (WSJ), leading to the creation of the \"Facebook Files\" series at the WSJ. Research done by Facebook itself found that angrier posts often got higher engagements , hence in 2018, Facebook's algorithms were reconfigured to give priority to these posts in user feeds. An implication of this would entail that more individuals would be angrier while reading the post and it is definitely possible that it may have led to incitement of more hate speech. This is incredibly problematic, but Facebook's insistence to focus on the profit made by more user retention due to this algorithm led to this being left to the wayside by the developers. During the sudden appearance of the pandemic, a large number of posts with contradictory information was shared on these social media platforms that left many users excessively confused. In fact, a study by the National Centre of Infectious Diseases (NCID) revealed that 6 in 10 Singaporeans received fake information regarding the pandemic. This information overload, or \"infodemic\", as termed by the World Health Organisation (WHO), has led to the spread of excessive amounts of misinformation. During events like the excessive aftermath of George Floyd's death and the Taliban's take-over of Afghanistan, an incredible number of posts led to these events being blown out of proportion. The Capitol Riots and Trump's Ban : After the absolutely horrific events of January 6th 2021, where a mob of individuals barreled their way into the US Capitol Building in support of Trump after his recent loss in the presidential elections, Trump was permanently banned from multiple social media platforms, like Facebook and Twitter. YouTube also banned him, and Apple and Google both refused to offer the Parler, which was commonly associated with Trump and his followers, on their respective mobile app stores. This was followed officially with the impeachment of Trump just a few short days of his term, but it also painted a picture of just how powerful Big Tech Countries, with their act of censorship making them act more like a government than a group of businesses. WhatsApp's Privacy Settings : In the beginning of 2021, millions of people around the globe received a message on their phones that WhatsApp was updating their privacy settings such that certain data would be stored by WhatsApp. Being an application recently acquired by Facebook, this message led to immediate terror and uproar amongst the general populace, with many worldwide immediately deleting the app and switching to Telegram and Signal, the latter of which was instigated by Tesla CEO Elon Musk. While these concerns ended up being overexaggerated, it was still notable that the general populace has become more receptive to privacy concerns and the impacts that these settings could have. Theranos has been a very clear indicator of how much trust we have in technology today. The scandal was one of the many such cases involving market fraud and the misleading of shareholders over the past few years and has been helmed as Silicon Valley\u2019s Greatest Disaster. With an idea as incredible as was the founder\u2019s, Elizabeth Holmes, who possessed a very limited amount of knowledge regarding medical trials and entrepreneurial methods and a small patent developed within a few five days, Theranos seemed doomed from the start, yet it grew from a simple start-up in 2003 to a highly valued company with a valuation of about 9 billion dollars in 2014. In a few short years, the medical company had grown to the point that Holmes was now the richest self-made female billionare in the world, yet all it had was a disappointing line-up of inaccurate products with fabricated results that left many flabbergasted. Holmes had somehow managed to convince stakeholders and even the board of directors of her own company to let her continue the fraud.","title":"Big Tech"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#cancel-culture_2","text":"J.K. Rowling faced intense criticism after voicing transphobic beliefs, got cancelled. In June 2020, she published a transphobic manifesto, sales increased in her home country (Britain)","title":"Cancel Culture"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#prejudice-and-discrimination","text":"Mr Saidullah Karimi, a orthopedic technician and Afghan refugee, built a fully-functioning robot called Athena from recyclable parts to stand as a symbol of what refugees can accomplish and contribute to society if given the chance. In fact, refugees are often denied work by companies in Europe and the US since they are said to use different technology in their homeland, which is far from the truth.","title":"Prejudice and Discrimination"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#ai_1","text":"An AI chatbot developed in South Korea named Luda Lee, initially popular for its cheerful disposition and ability to talk like a human being, quickly became the source of controversy after it made inappropriate comments regarding women's rights, lesbianism and disability, joining a long list of such chatbots infamous for their polarising opinions, often contrary to public opinion, including Microsoft's Tay, Japan's Rinna and China's BabyQ. The company behind it was also sued for disseminating personal information of the users. Research done by the University of Montreal suggests that AI has better clinical decision-making capability than do humans, but generally in a more secondary sense, i.e. in the form of a second opinion. A horrifying application of AI has been tools developed by many that utilise DeepFake technology in the form of false shaming individuals. For example, a software called DeepNude was recently taken down which involved taking in the image of a face, and mapping that onto a Pornographic Film using DeepFake technology. \u2003 Source: https://www.technologyreview.com/2021/09/13/1035449/ai-deepfake-app-face-swaps-women-into-porn/ (MIT Technology Review, 13th September 2021) AI Dungeon, a text generator game utilising AI, was used by a few individuals to generate child pornography. \u2003 Source: Vikram Ramanathan, 21st September 2021 Baidu's Xiaodu is an all-inclusive technology to aid an oncoming \"Silver Tsunami\", providing many crucial services like access to emergency care and emotional support. AI algorithms developed by Big Tech and integrated into commerical and consumer products which drive the daily function of workplaces and people's lifestyles such as Google's search engine may gain more affordance than it should. Gebru Timnit, a researcher at Google's AI labs, grew wary of how quickly such technology were deployed and gaining dominance, and had criticise how black box language models developed by Google and competitors such as Microsoft's GPT-3, may for instance be trained on abusive language of the internet and develop biases yet elbow out promising alternatives, and later cause the future of AI technology to be built on shaky foundations... (List examples from other sections; e.g. coded gaze, stanford facial recognition; google's featured snippets). \u2003 Source: https://www.technologyreview.com/2021/06/14/1026148/ai-big-tech-timnit-gebru-paper-ethics/ (MIT Technology Review on Timnit Gebru's sacking, 14th June 2021) Use of AI in courts: see Science and Technology/Tech/AI/AI in courts. Example: Advisory AI: Civil Resolution Tribunal (CRT) in British Columbia, Canada for disputes relating to subsidised housing. Predictive AI: Correctional Offender Management Profiling for Alternative Sanctions for predicting recidivism, has been known to overestimate recidivism among African American defendants compared to Caucasian Americans (though this may be natural consequence of greater concentrations of black felons in some areas), judges using the tool detain people more than before although it was intended to suade judges towards the opposite outcome. This human control was an issue in the Loomis case, before the Supreme Court of Wisconsin.16 At stake were: (1) whether the use of the result of a risk assessment by an instrument such as COMPAS, where the operation is a business secret, violates the defendant\u2019s right to a fair trial because the secret operation deprives a defendant of the opportunity to test the accuracy and scientific value of the risk assessment, and (2) whether it violates the right to a fair trial to rely on such a risk assessment because it includes gender and race in the assessment of the risk of recurrence. The Wisconsin Supreme Court dismissed Loomis\u2019 objections, but said that the judge should give reasons as to how he or she uses COMPAS. The case was referred to the Supreme Court of the United States, which decided not to hear the case. Pros: Reduced Recidivism, streamlines process and makes it more accessible (cheaper and faster), possibly reduce bias of human jurors (more for felonies since human jurors are supposed to be competent), cases like divorce and employment contract terminations are predictable and the job of a court proceeding is often merely to check all the terms are fulfilled. Cons: In the US, predictive tools are offered commericially and workings are thus business secrets. Ethical Principles (mostly pertaining to previous point): Outlined by The Commission for the Efficiency of Justice (CEPEJ) of the Council of Europe: Respect for fundamental rights. Ensure that design and implementation of AI services and tools are compatible with fundamental rights such as privacy, equal treatment and fair trial. Equal treatment. Avoid discrimination between individuals and groups of individuals. The example of COMPAS above shows that discrimination and unjustified distinction between individuals and groups, is a real risk. The data used by the algorithm may be the cause, and the prejudice may also be embedded in the algorithm itself. Data security. When processing judicial decisions and data, certified sources and data that cannot be altered should be used, with models that are multidisciplinary in design, in a secure technological environment. Transparency. Data processing methods should be made transparent and comprehensible, and external audits should be allowed. The requirement of transparency is now established case law. The user of an algorithm must make public the choices made, and the data and assumptions used, in a complete, timely and appropriate manner so that these choices, data and assumptions are accessible to third parties. Such full, timely and appropriate disclosure should make it possible to assess the choices made and the data, reasoning and assumptions used, so as to ensure effective legal protection against decisions based on those choices, data, reasoning and assumptions, with the possibility of judicial review by the courts. AI under user control. The algorithm may not be used as a prescription, i.e. the computer does not prescribe anything and cannot decide by itself. Users must know and understand what the AI does, and the users must be in control of the choices they make. This means that users must be able to deviate from the outcome of the algorithm without difficulty. What can happen when IT is blindly relied upon is shown by an example from the courts in the United Kingdom.18 There, a relatively simple piece of IT determines the financial capacity of (ex)-spouses in maintenance proceedings. The parties fill in a PDF form, and the IT calculates the resulting capacity. Due to a small mistake, which went unnoticed, incorrect calculations were made in 3,638 cases between April 2011 and January 2012, and between April 2014 and December 2015. Debts, instead of being deducted, had been added to the assets, so the assets taken into account were too high. In cases that were still pending, this could still be corrected. However, incorrect decisions were issued, and presumably complied with, in more than 2,200 cases. When researchers asked OpenAI\u2019s GPT-3 model, one of the largest and most sophisticated of these models ever created and generally considered to be one of the biggest AI breakthroughs in the past decade, to complete a sentence containing the word \u201cMuslims,\u201d it turned to violent language in more than 60% of cases \u2014 introducing words like bomb, murder, assault, and terrorism. A new report from Georgetown University\u2019s Center for Security and Emerging Technology (CSET) says there is little evidence to suggest there is a massive, urgent AI skills gap. According to the annual State of AI report, academic organizations don\u2019t have enough compute resources to accomplish their projects\u2014but 88% of \u201ctop AI faculty\u201d have received Big Tech funding, the researchers wrote. although AI safety is making headlines, \u201cfewer than 50 researchers\u201d are working on it full-time at the largest AI labs, according to the report \u2014 meaning there are a lot more people whose full-time job it is to build this tech rather than think about potential consequences. In a recent article by the newsletter, Emerging Tech Brew, the journalist talks about how 94% of AI systems studied are more inaccurate when compared to human radiologists. This statistic may seem pretty indicative of the relative nascence of AI, but it in fact demonstrates that many journalists have no clue as to how AI truly works, with statements like these being played for entertainment rather than truth.","title":"AI"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#autonomous-vehicles","text":"Moral Machine Experiment: a worldwide survey pertaining to choices made by Autonomous Vehicles in adverse situation wherein one or more are susceptible to death. Revealed different opinions across the world, with people residing in Asian Countries displaying a slight favouritism to the elderly while some expressed greater appreciation for Females.","title":"Autonomous Vehicles"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#stats_1","text":"","title":"Stats"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#social-media_1","text":"According to the International Communication Union, there are more than 3 billion Internet users worldwide. According to a research conducted by Pew Research Centre on 700 American youth in 2018, close to 95% of teenagers have access to a smartphone, and 45% say that they are online on social media platforms on a \u2018almost constant\u2019 basis. The older generation is remarkably less adept at using technology and social media: while 90% of those aged 18-29 use at least one social media account, only 40% of those aged above 65 do. According to research by the Pew Research Centre, 47% of conservatives are likely to see Facebook posts aligned with their own views, as opposed to half that percentage for those who seek the middle ground. Since close to 1.7 billion people of the 7.2 billion people in the world today have active social media accounts, there is a smorgasbord of differing opinions online. Naturalistic research indicates that people who had conversations without the presence of a mobile phone felt that these conversations felt higher levels of \u2018empathetic concern\u2019, as opposed to those who conversed with the presence of a mobile phone.","title":"Social Media"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#autonomous-vehicles_1","text":"9.1 crashes per million miles driven for self-driving cars, compared with 4.1 crashes per million miles for conventional vehicles. \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration Autonomous vehicles cause 0.36 injuries per crash, compared with 0.25 for conventional vehicles. \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration Self-driving cars are not responsible for any accidents \u2003 Source: University of Michigan\u2019s Transportation Research Institute \u2003 Notes: Using crash stats from US National Highway Traffic Safety Administration","title":"Autonomous Vehicles"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#big-tech-and-big-data","text":"Combined stock market valuation of Apple, Alphabet, Nvidia, Tesla, Microsoft, Amazon and Facebook (i.e FAAMG + Nvidia + Tesla) is $3.4 trillion in 2020. \u2003 Article: CNBC - Tech\u2019s top seven companies added $3.4 trillion in value in 2020 80% of people have TraceTogether, reducing contact tracing from 4 days to 1.5 days \u2003 Article: CNA - Bill restricting police use of TraceTogether data introduced in Parliament, with tougher penalties for misuse \u2003 Source: Smart Nation and Digital Government Group (SNDGG) Singapore Free markets are creating a major free speech problem Companies retroactively policing those who express dissent against China. Many such cases: John Cena calling Taiwan a country, LeBron James & Morey in solidarity with HK, Mercedes Benz quoting Dalai Lama. While individually the cases are not major, the opinions were expressed on english platforms (some banned in China like Twitter), towards english-speaking audiences. PNTR (Permanent Normal Trade Relations which promised US-Chinese economic integration) during Clinton administration was not only for economic growth, but also for the US to abet the liberaization of the human potential of the people with American values in society and trade. (socially and economically since China is communist and micromanages private industries unlike US). However, unexpectedly censorship was smuggled into the US as to be elaborated. Apple TV streaming service disallows any content which criticises or paints China as a villain. While reasonable to market to Chinese consumers, a blanket categorical ban on any such content should ring alarm bells, especially when considering news agencies are pro-profit (slight stretch). Big tech is ultimately pulling the strings of countries as diplomatic agents as for instance since the chinese depend on iCloud and other technologies and does not want to fall behind the west and America does not want to falter in its ambition of liberating Chinese or lose economic primacy. While US and China are playing a precarious game of Twister, Big Tech (In this case FAANG rather than BATX) is free to hold their services hostage and leverage what type of content they will distribute or sponsor, thus influencing mass media, which is a threat to democracy yada yada. Big corporations are more likely to do well in bankruptcy court than small and medium-sized companies, according to a recent Brookings Institution study . Microsoft has aggressively pushed its new business messaging and collaboration tool, Microsoft Teams, which competes with the independent company Slack. On Thursday, Microsoft said the number of users on Teams had grown 37 percent in a week to more than 44 million daily users. There have been at least 900 million meeting and call minutes on Teams every day. Apple\u2019s profit just from the past three months ($21.7 billion) was nearly double the combined annual profits of the five largest U.S. airlines in prepandemic 2019. Amazon\u2019s stock price increases have made Jeff Bezos so rich that he could buy a new model iPhone for 200 million people \u2014 and he would still be a billionaire. Google\u2019s $50 billion in revenue from selling advertisements from April to June was about what Americans \u2014 all of the Americans \u2014 spent on gasoline and gas station purchases last month. The annual revenue of one of Microsoft\u2019s side businesses, LinkedIn, is nearly four times that of Zoom Video Communications, a star of the pandemic, in the past year. Facebook expects to dole out more cash outfitting its computer hubs and offices in 2021 than Exxon spends around the world to dig oil and gas out of the ground in a year. Amazon fell short of investors\u2019 expectations on Thursday. But in the past year, Amazon\u2019s e-commerce revenue still climbed by $109 billion \u2014 an increase in a single year that Walmart needed the past nine years to reach.","title":"Big Tech and Big Data"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#cancel-culture_3","text":"More than 90% of the doxed files included the victim's address, 61% included a phone number, and 53% included an email address. \u2003 Source: 2017 NYU - First large-scale doxing study reveals motivations and targets for cyber bullying \u2003 Notes: Over 5500 files of doxxing, implies over 5500 cases of doxxing as sample size 12% of the students acknowledged their engagement in doxing. \u2003 Source: Int J Environ Res Public Health - Doxing: What Adolescents Look for and Their Intentions \u2003 Notes: Survey of 2120 Hong Kong students","title":"Cancel Culture"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#surveillance","text":"Ten of 24 US Governmental agencies surveyed plan to broaden their use of facial recognition systems by 2023, while eighteen of 24 currently use some form of the technology, according to an August 2021 report by the US Government Accountability Office (GAO). Facial recognition has proved to be less accurate on people with darker skin , women, and younger and older people, according to a 2018 paper presented by MIT and Stanford University researchers. This was due to the fact that dataset used to assess its performance was more than 77 percent male and more than 83 percent white. Leading to a disparity of nearly 46% for some individuals. Your face is not a barcode - Arguments against Facial Recognition Technology (2003) Comprehensive list of arguments against the use of facial recognition and rebuttals of common arguments. Large potential by abuse by governments (e.g. China & Uyghers) or becoming compromised by hackers As opposed to other similar technologies like fingerprinting, facial recognition requires least cooperation from the individual as one can be identified in a crowd from afar. Thus it is quite open to attack vectors. Quite fallible to false positives and inaccuracies from poor lighting conditions e.g. Inefficient among all the false positives, i.e. 1 terrorist among 1000 false positives. Other measures may be preferable for such use-cases, despite scares pushing towards facial recognition post-9/11. Hard to give effective notice, and gain proper consent. Adoption in public spaces such as government buildings or other institutions means it is a fait accompli that those uncomfortable with the technology have to accept. Technology may not necessarily be only accessible and implemented by the government but also by businesses or district authorities, who may wittingly or unwittingly abuse the technology Sets a poor precedents as even if it is well-implemented by wealthier and democratic governments with respect for civil liberties, it may gain adoption among less enlightened governments. For instance, a report by The Conservation where some Chinese citizens were informally interviewed on their thoughts on the government's inexorable push for social surveillance systems believed the west also had similar concepts of social credit which China is justified to follow to catch up with other superpowers. They welcome the infringements of the civil liberties in favor of the public order solidified by such developments (May not necessarily be deluded, just differing culture). If the west must adopt facial recognition to protect the nation or safeguard their democracy, then the artificial 'Tian' watching over Chinese citizens or similar facsimilies that may prop up in other nations will only gain in prevalance and apparent signficance in protecting a nation's civil morals. Facial recognition used to catch felons at a Superbowl in Tampa, Florida 2001 According to recent research by the Center for Democracy and Technology (CDT), \u201c86% of teachers reported that, during the pandemic, schools provided tablets, laptops, or Chromebooks to students at twice the rate (43%) prior to the pandemic, an illustration of schools\u2019 attempts to close disparities in digital access.\u201d More than 80% of surveyed teachers and 77% of surveyed high school students told the CDT that their schools use surveillance software on those devices, and the more reliant students are on those electronics, unable to afford supplementary phones or tablets, the more they are subjected to scrutiny.","title":"Surveillance"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#security","text":"76.1% of websites use https by default, a more secure protocol to access websites, while the 85.5% top 1 million websites use https by default \u2003 Source: W3Techs \u2003 Notes: Referenced by New York Times","title":"Security"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#morals-and-ethics_1","text":"Estimated 90% - 95% of all online deepfake videos are nonconsensual porn, 90% of the porn feature women \u2003 Article: technlogyreview.com - A horrifying new AI app swaps women into porn videos with a click \u2003 Source: research company Sensity AI \u2003 Notes: IT IS AN ESTIMATION!!","title":"Morals and Ethics"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#youth","text":"Children who used the internet, social media or video games for entertainment \u22654 hours daily were 4x more likely to skip school. \u2003 Source: Rutgers University analysing China Education Panel Survey \u2003 Notes: 10000 1st year middle schoolers, mean age = 13.5 Boys used interactive technology for entertainment significantly more than girls. Boys also performed worse and showed lower engagement levels than girls \u2003 Source: Rutgers University analysing China Education Panel Survey \u2003 Notes: 10000 1st year middle schoolers, mean age = 13.5","title":"Youth"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#tech-talent-crunch","text":"Singaporeans accounted for about 35 per cent of tech jobs, and 35 per cent of net tech jobs created in the past five years in banking and finance. \u2003 Article: NTU Business - What is stopping more Singaporeans from taking up tech jobs? \u2003 Source: Monetary Authority of Singapore (MAS) There appears to be a general lack of students graduating with degrees in Computer Science-related fields, with only 2,800 such graduates every year, compared to the demand of nearly 60,000 needed by 2023, according to Dr Vivian Balakrishnan, Minister-in-charge of the Smart Nation Initiative. The labour shortage by 2030 could cost Singapore nearly 39.2 billion SGD, which is very undesirable for the city state, since it could possibly account for 21 percent of Singapore\u2019s projected economy by then. Only a third of the technological workforce in the financial sector is made of Singaporeans","title":"Tech Talent Crunch"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#ai_2","text":"In 2019, the AI workforce made up 9% of total US employment. And over the next decade, employment in AI-related occupations is projected to grow twice as fast as employment in all occupations, according to data from the Bureau of Labor Statistics. The US needs to increase its supply of AI PhDs since research suggests there\u2019s a supply-demand gap for top-tier talent; \u201csustain and diversify\u201d pipelines for technical AI roles; and introduce AI education into K-12 curriculums.","title":"AI"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#prejudice-and-discrimination_1","text":"","title":"Prejudice and Discrimination"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#examples_2","text":"In some of the early chapters of Harper Lee's 1960 novel, To Kill A Mockingbird , we are introduced to the character of Ms Caroline and young schoolteacher who had come from another town. The chapters expand on some of the classroom sessions, showing some of the ways Ms Caroline was unaware of many of the cultural norms the rest had beome very much accustomed to. While a stretch, this can be well-reasoned to the issues that many foreigners experience in different countries, since these countries have differing systems and adjusting to these requires time.","title":"Examples"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#stats_2","text":"Women and early-career academics are more likely to feel like impostors in an academic discipline perceived to require raw talent for success. \u2003 Source: New York University 2021 Study","title":"Stats"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#women-in-tech","text":"A report by Microsoft found that more than 50% of young females are in fact interested in working in the STEM sector. Only 58% of females Singaporeans, after pursuing a degree in STEM, go on to STEM-related careers. According to the UNESCO Institute for Statistics, less that 30 percent of the world\u2019s researchers are female. Most women drop off the register of professional engineers before the age of 45.","title":"Women in Tech"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#meritocracy-elitism","text":"Harvard, Princeton, Stanford, Yale collectively enroll top 1% of income distribution than from households from bottom 60% In US, 7 top banks and law firms recruit exclusively from elite colleges In 2006 study by American University, 1% of children in bottom 20% of households will join top 5%. Pew Research Centre: 60% of Republicans believe colleges and universities are bad for the US. Burdening the rich is a non-sequitor to reducing inequality as severe income disparities cause the elite to become more competitive to hold onto their status, get a return on their investment in social capital (education, networking etc.), and provide similar resources to their children for succeeding their wealth and status. Elite Overproduction : Coined by Peter Turchin, describing a phase of a complex society involving a bloated elite class, too few elite jobs, declining living standards among general population. The Next Decade Could Be Even Worse","title":"Meritocracy &amp; Elitism"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#crime-and-punishment","text":"","title":"Crime and Punishment"},{"location":"EL5131/Stats%20and%20Quotes%20Notes/#examples_3","text":"In the 1994 prison drama, The Shawshank Redemption , the life story of an innocent man thrust into prison is shown, spanning nearly 20 years. As we progress through the story, we see him slowly but surely see the cruel injustices faced in prison, with his management of the corrupt warden's ledger. While an offense on its own, its rather ironic for a man as situationally perpetuated to be innocent as the warden to take over the role of a criminal. Unfortunately that is the case.","title":"Examples"},{"location":"EL5131/Climate%20Change/Global%20Efforts/","text":"Global Efforts Malicious Big Oil : Investigative reports by multiple activist organizations revealed Big Oil companies such as Exxon Mobil perpetuated falsehoods in climate science, and were long aware of the extent to which they were harming the environment since 1957. Report stoked public outrage resulting in protests and through #ExxonKnew Inside Exxon\u2019s playbook https://www.nytimes.com/2016/05/24/science/public-campaign-against-exxon-has-roots-in-a-2012-meeting.html?_r=2 Not very good ones Carbon Offsets Summary : Criticised for being a smokescreen that enables polluters, and doesn't help much with achieving net zero. 2 Types of offsets: Avoided emissions and Negative emissions. Examples : US Airline Jetblue funds solar/wind farms, forest protection, and landfill gas capture, because it can't just stop burning jet fuel Evaluating Offsets : Additionality: Was anyone going to cut the tree in the first place? Permanance: Is the forest going to be preserved forever? Double-Counting: Landowner makes protection promizes to multiple companies Leakage: If I can't cut this forest, cut that one instead Good ones Innovations Ocean shipping produces 2% of global emissions but hard to reduce while maintaining productivity. But startup FleetZero propose making use of large batteries to electrify ships, given how electric vehicles have broken into the mainstream consumer market. Effectiveness of these efforts Underdeveloped countries : Behind on global environmental efforts although they often they face the brunt of the damage caused by climate change, especially island nations, and those near the tropics. Carribean: Anomalous 30 Tropical storms in 2020; Pacific Islands: 3 Cyclones in 2020-2021. Uganda: Landslides (causing soil erosion, burying settlements, farms) and flooding more frequent. Internation institute for Environment and Development (IIED) showed 46 of the world's least developed countries don't have finanical means to \"climate proof\" themselves. They need at least $40bn a year, but between 2014-18, just $5.9bn was delivered. Under UN climate convention, EU & 23 developed countries pledges $100bn a year to fund climate projects in developed nations. But Organization for Economic Cooperation and Development (OECD) although $80bn was made available in 2018, only 21% was used for adaptive Why are they behind in fighting climate change? : Immediate need to put food on table Weak policy design, implementation and enforcement. Not enough foreign financial aid.","title":"Global Efforts"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#global-efforts","text":"","title":"Global Efforts"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#malicious","text":"Big Oil : Investigative reports by multiple activist organizations revealed Big Oil companies such as Exxon Mobil perpetuated falsehoods in climate science, and were long aware of the extent to which they were harming the environment since 1957. Report stoked public outrage resulting in protests and through #ExxonKnew Inside Exxon\u2019s playbook https://www.nytimes.com/2016/05/24/science/public-campaign-against-exxon-has-roots-in-a-2012-meeting.html?_r=2","title":"Malicious"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#not-very-good-ones","text":"","title":"Not very good ones"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#carbon-offsets","text":"Summary : Criticised for being a smokescreen that enables polluters, and doesn't help much with achieving net zero. 2 Types of offsets: Avoided emissions and Negative emissions. Examples : US Airline Jetblue funds solar/wind farms, forest protection, and landfill gas capture, because it can't just stop burning jet fuel Evaluating Offsets : Additionality: Was anyone going to cut the tree in the first place? Permanance: Is the forest going to be preserved forever? Double-Counting: Landowner makes protection promizes to multiple companies Leakage: If I can't cut this forest, cut that one instead","title":"Carbon Offsets"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#good-ones","text":"","title":"Good ones"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#innovations","text":"Ocean shipping produces 2% of global emissions but hard to reduce while maintaining productivity. But startup FleetZero propose making use of large batteries to electrify ships, given how electric vehicles have broken into the mainstream consumer market.","title":"Innovations"},{"location":"EL5131/Climate%20Change/Global%20Efforts/#effectiveness-of-these-efforts","text":"Underdeveloped countries : Behind on global environmental efforts although they often they face the brunt of the damage caused by climate change, especially island nations, and those near the tropics. Carribean: Anomalous 30 Tropical storms in 2020; Pacific Islands: 3 Cyclones in 2020-2021. Uganda: Landslides (causing soil erosion, burying settlements, farms) and flooding more frequent. Internation institute for Environment and Development (IIED) showed 46 of the world's least developed countries don't have finanical means to \"climate proof\" themselves. They need at least $40bn a year, but between 2014-18, just $5.9bn was delivered. Under UN climate convention, EU & 23 developed countries pledges $100bn a year to fund climate projects in developed nations. But Organization for Economic Cooperation and Development (OECD) although $80bn was made available in 2018, only 21% was used for adaptive Why are they behind in fighting climate change? : Immediate need to put food on table Weak policy design, implementation and enforcement. Not enough foreign financial aid.","title":"Effectiveness of these efforts"},{"location":"EL5131/Climate%20Change/Net%20Zero%20is%20Not%20Zero/","text":"Net Zero Is Not Zero Recent carbon-neutrality pledges may seem ambitious, but merely serve to promote a new set of false climate solutions under a different guise. Such pledges may persuade many people, but the climate isn\u2019t buying it. RIO DE JANEIRO/BERLIN \u2013 It may seem as if the world is finally taking the climate crisis seriously, judging by the number of pledges to reach \u201cnet-zero emissions.\u201d Among the major emitters, both the United States and the European Union have promised to achieve this goal by 2050, while China intends to become carbon-neutral before 2060. Even oil giants Shell and BP plan to reach net-zero emissions by mid-century. Large tech corporations appear more ambitious still. Amazon has committed to reaching net-zero carbon dioxide emissions by 2040. Microsoft has pledged to be \u201ccarbon negative\u201d by 2030, and by 2050 it aims to have removed from the air all the CO2 the firm has emitted since it was founded in 1975. Google claims to have been carbon neutral since 2007, and aims to be \u201ccarbon free\u201d by 2030. In fact, net-zero pledges have come from all parts of the economy, including the meat and dairy industry , aviation, mining , finance, and retail. But these seemingly ambitious goals in fact amount to yet another round of greenwashing and dangerous distractions that will delay and prevent the adoption of real climate solutions. That is because net zero is not really zero . For starters, 2050 is almost three decades away. Making long-term net-zero pledges enables governments and businesses to avoid drastic emissions cuts now. From a climate-justice perspective in particular, mid-century is way too late. Wealthy, industrialized countries in the Global North, due to their historical emissions and current levels of wealth, have a responsibility to decarbonize much faster. Compounding the problem, many net-zero plans are not backed up by corresponding short-term and interim emissions-reductions targets, such as for 2025. Instead, the majority of countries\u2019 nationally determined contributions under the 2015 Paris climate agreement , which were recently updated or reviewed, are based on a 2030 timeframe. This disregards the five-year review cycle at the heart of the Paris accord. Worse still, the inclusion of \u201cnet\u201d in climate pledges confirms that emissions will not actually decline to zero. Instead, they will supposedly be offset \u2013 to an unclear and disputable extent \u2013 by removal of CO2 from the atmosphere. Many such net-zero schemes rely excessively on natural ecosystems to remove and store atmospheric CO2. This has sparked the current hype surrounding so-called nature-based solutions. While carefully restoring natural ecosystems is crucial in addressing both the climate and the biodiversity crises, it must not serve to prolong the lifetime of polluting industries. But nature-based solutions also include proposals that would transform agriculture into a large-scale emissions-mitigation opportunity associated with the soil carbon market. Net-zero plans often also rely on speculative techno-fixes to remove CO2 from the atmosphere. Climate-geoengineering technologies such as bioenergy with carbon capture and storage (BECCS) or direct air capture (DAC) are highly risky and unproven \u2013 in particular at climate-relevant scales \u2013 and could have potentially devastating consequences for people and ecosystems. Either way, \u201csolutions\u201d like BECCS and DAC risk locking in several more decades of continued fossil-fuel production and combustion. The conversation instead needs to return to the real climate solutions that currently do not feature at high-level intergovernmental conferences. The debate should center on a comprehensive and long-overdue transformation of our exploitative and destructive economic systems. Bringing down global greenhouse-gas (GHG) emissions to real zero requires addressing the manifold global and historical injustices that caused the climate crisis and continue to shape it. Specifically, the rights, lives, and livelihoods of indigenous peoples and local communities must be at the core of any climate solution. That means listening to these groups, and taking their practices and proposals seriously. Strengthening and securing their land rights is one of the most effective ways to protect ecosystems, biodiversity, and the climate. In addition, we need to leave fossil fuels in the ground right now. There must be no further development of these resources, and existing fossil-fuel infrastructure must be phased out as soon as possible, based on a just transition for workers and communities that depend on them. Moving away from industrial agriculture is another high priority. Over-intensive, destructive production has exhausted the Earth\u2019s soils and ecosystems and is generating massive amounts of GHG emissions, while feeding only a fraction of the world\u2019s population. It is a major driver of deforestation, and the resulting destruction of ecological barriers and buffers probably contributed to the outbreak of the COVID-19 pandemic. By contrast, agroecology offers new possibilities for socio-ecological transformation and can contribute to tackling climate change in a safe way. This approach can also help to guarantee food and nutrition security and sovereignty, and conserve biodiversity. The Global North\u2019s overconsumption and the profit-driven exploitation of the world\u2019s resources need to stop. Instead, we must align economic activities with the goals of global social and climate justice, thereby putting well-being and care at the center of our efforts to protect our shared environment. Recent net-zero pledges may seem ambitious, but they only promote a new set of false solutions under a guise of 50 shades of green. Governments and businesses must abandon their greenwashing strategies once and for all. At this crucial moment, we need real political will to create real change.","title":"Net Zero Is Not Zero"},{"location":"EL5131/Climate%20Change/Net%20Zero%20is%20Not%20Zero/#net-zero-is-not-zero","text":"Recent carbon-neutrality pledges may seem ambitious, but merely serve to promote a new set of false climate solutions under a different guise. Such pledges may persuade many people, but the climate isn\u2019t buying it. RIO DE JANEIRO/BERLIN \u2013 It may seem as if the world is finally taking the climate crisis seriously, judging by the number of pledges to reach \u201cnet-zero emissions.\u201d Among the major emitters, both the United States and the European Union have promised to achieve this goal by 2050, while China intends to become carbon-neutral before 2060. Even oil giants Shell and BP plan to reach net-zero emissions by mid-century. Large tech corporations appear more ambitious still. Amazon has committed to reaching net-zero carbon dioxide emissions by 2040. Microsoft has pledged to be \u201ccarbon negative\u201d by 2030, and by 2050 it aims to have removed from the air all the CO2 the firm has emitted since it was founded in 1975. Google claims to have been carbon neutral since 2007, and aims to be \u201ccarbon free\u201d by 2030. In fact, net-zero pledges have come from all parts of the economy, including the meat and dairy industry , aviation, mining , finance, and retail. But these seemingly ambitious goals in fact amount to yet another round of greenwashing and dangerous distractions that will delay and prevent the adoption of real climate solutions. That is because net zero is not really zero . For starters, 2050 is almost three decades away. Making long-term net-zero pledges enables governments and businesses to avoid drastic emissions cuts now. From a climate-justice perspective in particular, mid-century is way too late. Wealthy, industrialized countries in the Global North, due to their historical emissions and current levels of wealth, have a responsibility to decarbonize much faster. Compounding the problem, many net-zero plans are not backed up by corresponding short-term and interim emissions-reductions targets, such as for 2025. Instead, the majority of countries\u2019 nationally determined contributions under the 2015 Paris climate agreement , which were recently updated or reviewed, are based on a 2030 timeframe. This disregards the five-year review cycle at the heart of the Paris accord. Worse still, the inclusion of \u201cnet\u201d in climate pledges confirms that emissions will not actually decline to zero. Instead, they will supposedly be offset \u2013 to an unclear and disputable extent \u2013 by removal of CO2 from the atmosphere. Many such net-zero schemes rely excessively on natural ecosystems to remove and store atmospheric CO2. This has sparked the current hype surrounding so-called nature-based solutions. While carefully restoring natural ecosystems is crucial in addressing both the climate and the biodiversity crises, it must not serve to prolong the lifetime of polluting industries. But nature-based solutions also include proposals that would transform agriculture into a large-scale emissions-mitigation opportunity associated with the soil carbon market. Net-zero plans often also rely on speculative techno-fixes to remove CO2 from the atmosphere. Climate-geoengineering technologies such as bioenergy with carbon capture and storage (BECCS) or direct air capture (DAC) are highly risky and unproven \u2013 in particular at climate-relevant scales \u2013 and could have potentially devastating consequences for people and ecosystems. Either way, \u201csolutions\u201d like BECCS and DAC risk locking in several more decades of continued fossil-fuel production and combustion. The conversation instead needs to return to the real climate solutions that currently do not feature at high-level intergovernmental conferences. The debate should center on a comprehensive and long-overdue transformation of our exploitative and destructive economic systems. Bringing down global greenhouse-gas (GHG) emissions to real zero requires addressing the manifold global and historical injustices that caused the climate crisis and continue to shape it. Specifically, the rights, lives, and livelihoods of indigenous peoples and local communities must be at the core of any climate solution. That means listening to these groups, and taking their practices and proposals seriously. Strengthening and securing their land rights is one of the most effective ways to protect ecosystems, biodiversity, and the climate. In addition, we need to leave fossil fuels in the ground right now. There must be no further development of these resources, and existing fossil-fuel infrastructure must be phased out as soon as possible, based on a just transition for workers and communities that depend on them. Moving away from industrial agriculture is another high priority. Over-intensive, destructive production has exhausted the Earth\u2019s soils and ecosystems and is generating massive amounts of GHG emissions, while feeding only a fraction of the world\u2019s population. It is a major driver of deforestation, and the resulting destruction of ecological barriers and buffers probably contributed to the outbreak of the COVID-19 pandemic. By contrast, agroecology offers new possibilities for socio-ecological transformation and can contribute to tackling climate change in a safe way. This approach can also help to guarantee food and nutrition security and sovereignty, and conserve biodiversity. The Global North\u2019s overconsumption and the profit-driven exploitation of the world\u2019s resources need to stop. Instead, we must align economic activities with the goals of global social and climate justice, thereby putting well-being and care at the center of our efforts to protect our shared environment. Recent net-zero pledges may seem ambitious, but they only promote a new set of false solutions under a guise of 50 shades of green. Governments and businesses must abandon their greenwashing strategies once and for all. At this crucial moment, we need real political will to create real change.","title":"Net Zero Is Not Zero"},{"location":"EL5131/Climate%20Change/Singapore/","text":"Singapore Climate Change effects in Singapore 1980-2020: annual mean temperature rose from 26.9C to 28C 1980-2019: Annual rainfall increased at rate of 67mm per decade 2001: Typhoon Vamei swept north of Singapore, caused major flooding, tropical cyclones near equator could become more frequent Climate change risks in Singapore Sea level : Much of our nation lies only 15m above the mean sea level. 30% is less than 5m above it. Water : Drought test reliability of water supply, heavy rainfall overwhelms drainage and causes flash floods Biodiversity and greenery : self-explantory Public health : vector-borne diseases are observed during warmer periods of the year. Also makes heat stress more prevalent among the old and sick Urban Heat Island Effect : Replacement of natural land cover with infrastructure which produce/retain heat. Singapore's efforts Pledges 2009: Reduce emissions by 16% by 2020 2015: Reduce Emission Intensity (EI) (Amount of GHGs emitted per dollar GDP) by 36% from 2005 levels by 2030. Peak emissions by 2030 Budget 2022 - Achieve net-zero by mid-century, in line with COP-26 in Nov 2021.","title":"Singapore"},{"location":"EL5131/Climate%20Change/Singapore/#singapore","text":"","title":"Singapore"},{"location":"EL5131/Climate%20Change/Singapore/#climate-change-effects-in-singapore","text":"1980-2020: annual mean temperature rose from 26.9C to 28C 1980-2019: Annual rainfall increased at rate of 67mm per decade 2001: Typhoon Vamei swept north of Singapore, caused major flooding, tropical cyclones near equator could become more frequent","title":"Climate Change effects in Singapore"},{"location":"EL5131/Climate%20Change/Singapore/#climate-change-risks-in-singapore","text":"Sea level : Much of our nation lies only 15m above the mean sea level. 30% is less than 5m above it. Water : Drought test reliability of water supply, heavy rainfall overwhelms drainage and causes flash floods Biodiversity and greenery : self-explantory Public health : vector-borne diseases are observed during warmer periods of the year. Also makes heat stress more prevalent among the old and sick Urban Heat Island Effect : Replacement of natural land cover with infrastructure which produce/retain heat.","title":"Climate change risks in Singapore"},{"location":"EL5131/Climate%20Change/Singapore/#singapores-efforts","text":"","title":"Singapore's efforts"},{"location":"EL5131/Climate%20Change/Singapore/#pledges","text":"2009: Reduce emissions by 16% by 2020 2015: Reduce Emission Intensity (EI) (Amount of GHGs emitted per dollar GDP) by 36% from 2005 levels by 2030. Peak emissions by 2030","title":"Pledges"},{"location":"EL5131/Climate%20Change/Singapore/#budget-2022","text":"","title":"Budget 2022"},{"location":"EL5131/Climate%20Change/Singapore/#-achieve-net-zero-by-mid-century-in-line-with-cop-26-in-nov-2021","text":"","title":"- Achieve net-zero by mid-century, in line with COP-26 in Nov 2021."},{"location":"EL5131/Climate%20Change/Worldwide%20Impact/","text":"Worldwide Impact Underdeveloped countries Behind on global environmental efforts although they often they face the brunt of the damage caused by climate change, especially island nations, and those near the tropics. Carribean: Anomalous 30 Tropical storms in 2020; Pacific Islands: 3 Cyclones in 2020-2021. Uganda: Landslides (causing soil erosion, burying settlements, farms) and flooding more frequent. Internation institute for Environment and Development (IIED) showed 46 of the world's least developed countries don't have finanical means to \"climate proof\" themselves. They need at least $40bn a year, but between 2014-18, just $5.9bn was delivered. Under UN climate convention, EU & 23 developed countries pledges $100bn a year to fund climate projects in developed nations. But Organization for Economic Cooperation and Development (OECD) although $80bn was made available in 2018, only 21% was used for adaptive Why are they behind in fighting climate change? : Immediate need to put food on table Weak policy design, implementation and enforcement. Not enough foreign financial aid.","title":"Worldwide Impact"},{"location":"EL5131/Climate%20Change/Worldwide%20Impact/#worldwide-impact","text":"","title":"Worldwide Impact"},{"location":"EL5131/Climate%20Change/Worldwide%20Impact/#underdeveloped-countries","text":"Behind on global environmental efforts although they often they face the brunt of the damage caused by climate change, especially island nations, and those near the tropics. Carribean: Anomalous 30 Tropical storms in 2020; Pacific Islands: 3 Cyclones in 2020-2021. Uganda: Landslides (causing soil erosion, burying settlements, farms) and flooding more frequent. Internation institute for Environment and Development (IIED) showed 46 of the world's least developed countries don't have finanical means to \"climate proof\" themselves. They need at least $40bn a year, but between 2014-18, just $5.9bn was delivered. Under UN climate convention, EU & 23 developed countries pledges $100bn a year to fund climate projects in developed nations. But Organization for Economic Cooperation and Development (OECD) although $80bn was made available in 2018, only 21% was used for adaptive Why are they behind in fighting climate change? : Immediate need to put food on table Weak policy design, implementation and enforcement. Not enough foreign financial aid.","title":"Underdeveloped countries"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/","text":"Singapore Climate Change effects in Singapore 1980-2020: annual mean temperature rose from 26.9C to 28C 1980-2019: Annual rainfall increased at rate of 67mm per decade 2001: Typhoon Vamei swept north of Singapore, caused major flooding, tropical cyclones near equator could become more frequent Climate change risks in Singapore Sea level : Much of our nation lies only 15m above the mean sea level. 30% is less than 5m above it. Water : Drought test reliability of water supply, heavy rainfall overwhelms drainage and causes flash floods Biodiversity and greenery : self-explantory Public health : vector-borne diseases are observed during warmer periods of the year. Also makes heat stress more prevalent among the old and sick Urban Heat Island Effect : Replacement of natural land cover with infrastructure which produce/retain heat. Singapore's efforts Pledges 2009: Reduce emissions by 16% by 2020 2015: Reduce Emission Intensity (EI) (Amount of GHGs emitted per dollar GDP) by 36% from 2005 levels by 2030. Peak emissions by 2030 Budget 2022 - Achieve net-zero by mid-century, in line with COP-26 in Nov 2021.","title":"Singapore"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#singapore","text":"","title":"Singapore"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#climate-change-effects-in-singapore","text":"1980-2020: annual mean temperature rose from 26.9C to 28C 1980-2019: Annual rainfall increased at rate of 67mm per decade 2001: Typhoon Vamei swept north of Singapore, caused major flooding, tropical cyclones near equator could become more frequent","title":"Climate Change effects in Singapore"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#climate-change-risks-in-singapore","text":"Sea level : Much of our nation lies only 15m above the mean sea level. 30% is less than 5m above it. Water : Drought test reliability of water supply, heavy rainfall overwhelms drainage and causes flash floods Biodiversity and greenery : self-explantory Public health : vector-borne diseases are observed during warmer periods of the year. Also makes heat stress more prevalent among the old and sick Urban Heat Island Effect : Replacement of natural land cover with infrastructure which produce/retain heat.","title":"Climate change risks in Singapore"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#singapores-efforts","text":"","title":"Singapore's efforts"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#pledges","text":"2009: Reduce emissions by 16% by 2020 2015: Reduce Emission Intensity (EI) (Amount of GHGs emitted per dollar GDP) by 36% from 2005 levels by 2030. Peak emissions by 2030","title":"Pledges"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#budget-2022","text":"","title":"Budget 2022"},{"location":"EL5131/Climate%20Change/Singapore/Singapore/#-achieve-net-zero-by-mid-century-in-line-with-cop-26-in-nov-2021","text":"","title":"- Achieve net-zero by mid-century, in line with COP-26 in Nov 2021."},{"location":"EL5131/FourIR/Articles/article0/","text":"New robots \u2014 smarter and faster \u2014 are taking over warehouses Most picking jobs will be done by bots A DECADE AGO Amazon started to introduce robots into its \u201cfulfilment centres\u201d, as online retailers call their giant distribution warehouses. Instead of having people wandering up and down rows of shelves picking goods to complete orders, the machines would lift and then carry the shelves to the pickers. That saved time and money. Amazon now has more than 350,000 robots of various sorts deployed worldwide. But it is not enough to secure its future. Advances in warehouse robotics, coupled with increasing labour costs and difficulty in finding workers , has created a watershed moment in the logistics industry. With covid-19 lockdowns causing supply-chain disruptions and a boom in home deliveries that looks likely to endure, fulfilment centres have been working at full tilt. Despite the bots, many firms have to bring in temporary workers to cope during busy periods. Competition for staff is fierce. In the run-up to the holiday shopping season in December, Amazon brought in some 150,000 extra workers in America alone, offering sign-on bonuses of up to $3,000. The long-term implications of such a high reliance on increasingly hard-to-find labour in distribution is clear, according to a new study by McKinsey, a consultancy: \u201cAutomation in warehousing is no longer just nice to have but an imperative for sustainable growth.\u201d This means more robots are needed , including newer, more efficient versions to replace those already at work and advanced machines to take over most of the remaining jobs done by humans. As a result, McKinsey forecasts the warehouse-automation market will grow at a compound annual rate of 23% to be worth more than $50bn by 2030. The new robots are coming. One of them is the prototype 600 Series bot. This machine \u201cchanges everything\u201d according to Tim Steiner, chief executive of Ocado Group, which began in 2002 as an online British grocer and has evolved over the years into a leading provider of warehouse robotics. The 600 Series is a strange-looking beast, much like a box on wheels made out of skeletal parts. That is because more than half its components are 3D-printed. As 3D-printing builds things up layer by layer it allows the shapes to be optimised, thus using the least amount of material. As a result, the 600 Series is five times lighter than the company\u2019s present generation of bots, which makes it more agile and less demanding on battery power. March of the machines Ocado\u2019s bots work in what is known as the \u201cHive\u201d, a giant metallic grid at the centre of its fulfilment centres. Some of these Hives are bigger than a football pitch. Each cell on the grid contains products stored in plastic crates, stacked 21 deep. As orders arrive, a bot is dispatched to extract a crate and transport it to a picking station, where a human worker takes all the time they need, scans each one and puts them into a bag, much as happens at a supermarket checkout. It could take an hour or so walking around a warehouse to collect each item manually for a large order. But as hundreds of bots operate on the grid simultaneously, they are much faster. The bots are choreographed by an AI-driven computer system, which communicates with each machine over a wireless network. The system allows Ocado\u2019s current bot, the 500 Series, to gather all the goods required for a 50-item order in less than five minutes. The new 600 Series will match or better its predecessor\u2019s performance and use less energy. It also \u201cunlocks a cascade of benefits\u201d, says Mr Steiner, as it allows Hives to be made smaller and lighter. This means they can be installed in weeks rather than months and at a lower cost. That will make \u201cmicro\u201d fulfilment centres viable. Most fulfilment centres are housed in large buildings on out-of-town trading estates, but smaller units could be sited in urban areas closer to customers. This would speed up deliveries, in some cases to within hours. Amazon is also developing more-efficient robots. It\u2019s original machines were known as Kivas, after Kiva Systems, the Massachusetts-based company that manufactured them. The Kiva is a squat device which can slip under a stack of head-height shelves in which goods are stored. The robot then lifts and carries the shelves to a picking station. In 2012 Amazon bought Kiva Systems for $775m and later changed its name to Amazon Robotics. Amazon Robotics has since developed a family of bots, including a smaller version of a Kiva called Pegasus. These will allow it to pack more goods into its fulfilment centres and also use bots in smaller inner-city distribution sites. To prepare for a more automated future, Amazon Robotics recently opened a new robot manufacturing plant in Westborough, Massachusetts to boost its output. In 2014, when it became clear that future Kivas would be made exclusively for Amazon, Romain Moulin and Renaud Heitz, a pair of engineers working for a medical firm, decided to set up Exotec, a French rival, to produce a different sort of robotic warehouse. The firm has developed a three-dimensional system, which uses bots called Skypods. Looking a bit like Kivas, they also roam the warehouse floor. But instead of moving shelves, Skypods climb them. Once the robot reaches the necessary level, it extracts a crate, climbs down and delivers it to a picking station. Skypods, says Mr Moulin, maximise the space in a warehouse because they can ascend shelving stacked 12 metres high. Being modular, the system can be expanded easily. As well as returning crates to the shelves, Skypods also take them to refilling points. A number of retailers have started using Skypods, including Carrefour, a giant French supermarket group, GAP, an American clothing firm, and Uniqlo, a Japanese group. Because such robots move quickly and could cause injury\u2014Skypods zoom along at four metres-per-second (9mph)\u2014they tend to operate in closed areas. If Amazon\u2019s staff need to enter the robot area they don a special safety vest. This contains electronics which signal to any nearby bots that a human is present. The bot will then stop or take an alternative route. Some robots, however, are designed to work alongside people in warehouses. They often ferry things between people taking goods off shelves and pallets to people putting them into bags and boxes for shipping. Such systems can avoid the cost of installing fixed infrastructure, which allows warehouses to be reconfigured quickly\u2014a useful advantage for logistics centres that work for multiple retailers and have to deal with constantly changing product lines. When robots work among people, however, they have to be fitted with additional safety systems, such as cameras, radar and other sensors, to avoid bumping into staff. Hence they tend to move slowly and are cautious, which can result in bots frequently coming to a standstill and slowing operations. However, machines that are more aware of their surroundings are on the way. For instance, NEC, a Japanese electronics group, has started using \u201crisk-sensitive stochastic control technology\u201d, which is software similar to that used in finance to avoid high-risk investments. In this case, though, it allows a robot to weigh up risks when taking any action, such as selecting the safest and fastest route through a warehouse. In trials, NEC says it doubles the average speed of a robot without compromising safety. New tricks The toughest job to automate in a warehouse is picking and packing, hence the demand for extra pairs of hands during busy periods. This task is far from easy for robots because fulfilment centres stock tens of thousands of different items, in many shapes, sizes and weights. Nevertheless, Amazon, Ocado, Exotec and others are beginning to automate the task by placing robotic arms at some picking stations. These arms tend to use cameras and read bar codes to identify goods, and suction pads and other mechanisms to pick them up. Machine learning, a form of AI, is employed to teach the robots how to handle specific items, such as for example not putting potatoes on top of eggs. Ocado is also developing an arm which could bypass a picking station and take items directly from crates in the Hive. Fetch Robotics, a Silicon Valley producer of logistics robots that was acquired last year by Zebra Technologies, a computing firm, has developed a mobile picking arm which can travel around a fulfilment centre. Boston Dynamics, another Massachusetts robot maker, has come up with a heavyweight mobile version called Stretch, which can unpack lorries and put boxes on pallets. On January 26th DHL, a logistics giant, placed the first order for Stretch robots. It will deploy them in its North American warehouses over the next three years. That timetable gives a clue that progress will not be rapid. It will take ten to 15 years before robots begin to be adept at picking and packing goods, reckons Zehao Li, the author of a new report on warehouse robotics for IDTechEx, a firm of British analysts. Some companies think their bots will be able to pick 80% or so of their stock over the coming years, although much depends on the range of goods carried by different operations. Objects with irregular shapes, like bananas and loose vegetables, can be hard for a robot to grasp if it has primarily been designed to pick up products in neat packages. The bot might also be restricted in what weight it can lift, so would struggle with a flat-screen television or a heavy cask of beer. Further into the future, systems could emerge to overcome many of these limitations, such as multi-arm robots. So what jobs will remain? On the warehouse floor, at least, that mainly leaves technicians maintaining and fixing robots, says Mr Li. He thinks there are also likely to be a handful of supervisors watching over the bots and lending a hand if there remains anything that their mechanical brethren still can\u2019t handle. But it is not just inside the warehouse where jobs will go, but outside, too, once driverless delivery vehicles are allowed. At that point many products will travel through the supply chain and arrive at peoples\u2019 homes untouched by human hand. However, other jobs will emerge. For a start, someone has to build all these new robots. Amazon Robotics\u2019s new factory will create more than 200 new manufacturing jobs, although that dwindles into insignificance compared to the more than one million jobs which the pioneer of e-commerce has created since the first robots arrived in its fulfilment centres. A lot of those jobs are bound to go, although many are monotonous and strenuous, which is why they are hard to fill. Technological change, though, inevitably creates new roles for people. In the 1960s there used to be thousands of telephone switchboard operators, a job which has almost disappeared since exchanges became automated. But the number of other jobs in telecoms has soared. As logistics gets more efficient through greater automation, and online businesses grow, the overall number of jobs in e-commerce should still increase. But there will be many different sorts of jobs, just as there are many different sorts of robot.","title":"New robots \u2014 smarter and faster \u2014 are taking over warehouses"},{"location":"EL5131/FourIR/Articles/article0/#new-robots-smarter-and-faster-are-taking-over-warehouses","text":"Most picking jobs will be done by bots A DECADE AGO Amazon started to introduce robots into its \u201cfulfilment centres\u201d, as online retailers call their giant distribution warehouses. Instead of having people wandering up and down rows of shelves picking goods to complete orders, the machines would lift and then carry the shelves to the pickers. That saved time and money. Amazon now has more than 350,000 robots of various sorts deployed worldwide. But it is not enough to secure its future. Advances in warehouse robotics, coupled with increasing labour costs and difficulty in finding workers , has created a watershed moment in the logistics industry. With covid-19 lockdowns causing supply-chain disruptions and a boom in home deliveries that looks likely to endure, fulfilment centres have been working at full tilt. Despite the bots, many firms have to bring in temporary workers to cope during busy periods. Competition for staff is fierce. In the run-up to the holiday shopping season in December, Amazon brought in some 150,000 extra workers in America alone, offering sign-on bonuses of up to $3,000. The long-term implications of such a high reliance on increasingly hard-to-find labour in distribution is clear, according to a new study by McKinsey, a consultancy: \u201cAutomation in warehousing is no longer just nice to have but an imperative for sustainable growth.\u201d This means more robots are needed , including newer, more efficient versions to replace those already at work and advanced machines to take over most of the remaining jobs done by humans. As a result, McKinsey forecasts the warehouse-automation market will grow at a compound annual rate of 23% to be worth more than $50bn by 2030. The new robots are coming. One of them is the prototype 600 Series bot. This machine \u201cchanges everything\u201d according to Tim Steiner, chief executive of Ocado Group, which began in 2002 as an online British grocer and has evolved over the years into a leading provider of warehouse robotics. The 600 Series is a strange-looking beast, much like a box on wheels made out of skeletal parts. That is because more than half its components are 3D-printed. As 3D-printing builds things up layer by layer it allows the shapes to be optimised, thus using the least amount of material. As a result, the 600 Series is five times lighter than the company\u2019s present generation of bots, which makes it more agile and less demanding on battery power. March of the machines Ocado\u2019s bots work in what is known as the \u201cHive\u201d, a giant metallic grid at the centre of its fulfilment centres. Some of these Hives are bigger than a football pitch. Each cell on the grid contains products stored in plastic crates, stacked 21 deep. As orders arrive, a bot is dispatched to extract a crate and transport it to a picking station, where a human worker takes all the time they need, scans each one and puts them into a bag, much as happens at a supermarket checkout. It could take an hour or so walking around a warehouse to collect each item manually for a large order. But as hundreds of bots operate on the grid simultaneously, they are much faster. The bots are choreographed by an AI-driven computer system, which communicates with each machine over a wireless network. The system allows Ocado\u2019s current bot, the 500 Series, to gather all the goods required for a 50-item order in less than five minutes. The new 600 Series will match or better its predecessor\u2019s performance and use less energy. It also \u201cunlocks a cascade of benefits\u201d, says Mr Steiner, as it allows Hives to be made smaller and lighter. This means they can be installed in weeks rather than months and at a lower cost. That will make \u201cmicro\u201d fulfilment centres viable. Most fulfilment centres are housed in large buildings on out-of-town trading estates, but smaller units could be sited in urban areas closer to customers. This would speed up deliveries, in some cases to within hours. Amazon is also developing more-efficient robots. It\u2019s original machines were known as Kivas, after Kiva Systems, the Massachusetts-based company that manufactured them. The Kiva is a squat device which can slip under a stack of head-height shelves in which goods are stored. The robot then lifts and carries the shelves to a picking station. In 2012 Amazon bought Kiva Systems for $775m and later changed its name to Amazon Robotics. Amazon Robotics has since developed a family of bots, including a smaller version of a Kiva called Pegasus. These will allow it to pack more goods into its fulfilment centres and also use bots in smaller inner-city distribution sites. To prepare for a more automated future, Amazon Robotics recently opened a new robot manufacturing plant in Westborough, Massachusetts to boost its output. In 2014, when it became clear that future Kivas would be made exclusively for Amazon, Romain Moulin and Renaud Heitz, a pair of engineers working for a medical firm, decided to set up Exotec, a French rival, to produce a different sort of robotic warehouse. The firm has developed a three-dimensional system, which uses bots called Skypods. Looking a bit like Kivas, they also roam the warehouse floor. But instead of moving shelves, Skypods climb them. Once the robot reaches the necessary level, it extracts a crate, climbs down and delivers it to a picking station. Skypods, says Mr Moulin, maximise the space in a warehouse because they can ascend shelving stacked 12 metres high. Being modular, the system can be expanded easily. As well as returning crates to the shelves, Skypods also take them to refilling points. A number of retailers have started using Skypods, including Carrefour, a giant French supermarket group, GAP, an American clothing firm, and Uniqlo, a Japanese group. Because such robots move quickly and could cause injury\u2014Skypods zoom along at four metres-per-second (9mph)\u2014they tend to operate in closed areas. If Amazon\u2019s staff need to enter the robot area they don a special safety vest. This contains electronics which signal to any nearby bots that a human is present. The bot will then stop or take an alternative route. Some robots, however, are designed to work alongside people in warehouses. They often ferry things between people taking goods off shelves and pallets to people putting them into bags and boxes for shipping. Such systems can avoid the cost of installing fixed infrastructure, which allows warehouses to be reconfigured quickly\u2014a useful advantage for logistics centres that work for multiple retailers and have to deal with constantly changing product lines. When robots work among people, however, they have to be fitted with additional safety systems, such as cameras, radar and other sensors, to avoid bumping into staff. Hence they tend to move slowly and are cautious, which can result in bots frequently coming to a standstill and slowing operations. However, machines that are more aware of their surroundings are on the way. For instance, NEC, a Japanese electronics group, has started using \u201crisk-sensitive stochastic control technology\u201d, which is software similar to that used in finance to avoid high-risk investments. In this case, though, it allows a robot to weigh up risks when taking any action, such as selecting the safest and fastest route through a warehouse. In trials, NEC says it doubles the average speed of a robot without compromising safety. New tricks The toughest job to automate in a warehouse is picking and packing, hence the demand for extra pairs of hands during busy periods. This task is far from easy for robots because fulfilment centres stock tens of thousands of different items, in many shapes, sizes and weights. Nevertheless, Amazon, Ocado, Exotec and others are beginning to automate the task by placing robotic arms at some picking stations. These arms tend to use cameras and read bar codes to identify goods, and suction pads and other mechanisms to pick them up. Machine learning, a form of AI, is employed to teach the robots how to handle specific items, such as for example not putting potatoes on top of eggs. Ocado is also developing an arm which could bypass a picking station and take items directly from crates in the Hive. Fetch Robotics, a Silicon Valley producer of logistics robots that was acquired last year by Zebra Technologies, a computing firm, has developed a mobile picking arm which can travel around a fulfilment centre. Boston Dynamics, another Massachusetts robot maker, has come up with a heavyweight mobile version called Stretch, which can unpack lorries and put boxes on pallets. On January 26th DHL, a logistics giant, placed the first order for Stretch robots. It will deploy them in its North American warehouses over the next three years. That timetable gives a clue that progress will not be rapid. It will take ten to 15 years before robots begin to be adept at picking and packing goods, reckons Zehao Li, the author of a new report on warehouse robotics for IDTechEx, a firm of British analysts. Some companies think their bots will be able to pick 80% or so of their stock over the coming years, although much depends on the range of goods carried by different operations. Objects with irregular shapes, like bananas and loose vegetables, can be hard for a robot to grasp if it has primarily been designed to pick up products in neat packages. The bot might also be restricted in what weight it can lift, so would struggle with a flat-screen television or a heavy cask of beer. Further into the future, systems could emerge to overcome many of these limitations, such as multi-arm robots. So what jobs will remain? On the warehouse floor, at least, that mainly leaves technicians maintaining and fixing robots, says Mr Li. He thinks there are also likely to be a handful of supervisors watching over the bots and lending a hand if there remains anything that their mechanical brethren still can\u2019t handle. But it is not just inside the warehouse where jobs will go, but outside, too, once driverless delivery vehicles are allowed. At that point many products will travel through the supply chain and arrive at peoples\u2019 homes untouched by human hand. However, other jobs will emerge. For a start, someone has to build all these new robots. Amazon Robotics\u2019s new factory will create more than 200 new manufacturing jobs, although that dwindles into insignificance compared to the more than one million jobs which the pioneer of e-commerce has created since the first robots arrived in its fulfilment centres. A lot of those jobs are bound to go, although many are monotonous and strenuous, which is why they are hard to fill. Technological change, though, inevitably creates new roles for people. In the 1960s there used to be thousands of telephone switchboard operators, a job which has almost disappeared since exchanges became automated. But the number of other jobs in telecoms has soared. As logistics gets more efficient through greater automation, and online businesses grow, the overall number of jobs in e-commerce should still increase. But there will be many different sorts of jobs, just as there are many different sorts of robot.","title":"New robots \u2014 smarter and faster \u2014 are taking over warehouses"},{"location":"EL5131/FourIR/Articles/article1/","text":"The Coming Robot Dystopia All Too Inhuman By Illah Reza Nourbakhsh The term \u201crobotics revolution\u201d evokes images of the future: a not-too-distant future, perhaps, but an era surely distinct from the present. In fact, that revolution is already well under way. Today, military robots appear on battlefields, drones fill the skies, driverless cars take to the roads, and \u201ctelepresence robots\u201d allow people to manifest themselves halfway around the world from their actual location. But the exciting, even seductive appeal of these technological advances has overshadowed deep, sometimes uncomfortable questions about what increasing human-robot interaction will mean for society. Robotic technologies that collect, interpret, and respond to massive amounts of real-world data on behalf of governments, corporations, and ordinary people will unquestionably advance human life. But they also have the potential to produce dystopian outcomes. We are hardly on the brink of the nightmarish futures conjured by Hollywood movies such as The Matrix or The Terminator , in which intelligent machines attempt to enslave or exterminate humans. But those dark fantasies contain a seed of truth: the robotic future will involve dramatic tradeoffs, some so significant that they could lead to a collective identity crisis over what it means to be human. This is a familiar warning when it comes to technological innovations of all kinds. But there is a crucial distinction between what\u2019s happening now and the last great breakthrough in robotic technology, when manufacturing automatons began to appear on factory floors during the late twentieth century. Back then, clear boundaries separated industrial robots from humans: protective fences isolated robot workspaces, ensuring minimal contact between man and machine, and humans and robots performed wholly distinct tasks without interacting. Such barriers have been breached, not only in the workplace but also in the wider society: robots now share the formerly human-only commons, and humans will increasingly interact socially with a diverse ecosystem of robots. The trouble is that the rich traditions of moral thought that guide human relationships have no equivalent when it comes to robot-to-human interactions. And of course, robots themselves have no innate drive to avoid ethical transgressions regarding, say, privacy or the protection of human life. How robots interact with people depends to a great deal on how much their creators know or care about such issues, and robot creators tend to be engineers, programmers, and designers with little training in ethics, human rights, privacy, or security. In the United States, hardly any of the academic engineering programs that grant degrees in robotics require the in-depth study of such fields. LUKE MACGREGOR / COURTESY REUTERS A robot is pictured in front of the Houses of Parliament and Westminster Abbey as part of the Campaign to Stop Killer Robots in London, April 2013. One might hope that political and legal institutions would fill that gap, by steering and constraining the development of robots with the goal of reducing their potential for harm. Ideally, the rapid expansion of robots\u2019 roles in society would be matched by equally impressive advances in regulation and in tort and liability law, so that societies could deal with the issues of accountability and responsibility that will inevitably crop up in the coming years. But the pace of change in robotics is far outstripping the ability of regulators and lawmakers to keep up, especially as large corporations pour massive investments into secretive robotics projects that are nearly invisible to government regulators. We are hardly on the brink of the nightmarish futures conjured by The Matrix or The Terminator. But those dark fantasies contain a seed of truth: the robotic future will involve dramatic tradeoffs. There is every reason to believe that this gap between robot capability and robot regulation will widen every year, posing all kinds of quandaries for law and government. Imagine an adaptive robot that lives with and learns from its human owner. Its behavior over time will be a function of its original programming mixed with the influence of its environment and \u201cupbringing.\u201d It would be difficult for existing liability laws to apportion responsibility if such a machine caused injury, since its actions would be determined not merely by computer code but also by a deep neural-like network that would have learned from various sources. Who would be to blame? The robot? Its owner? Its creator? We face a future in which robots will test the boundaries of our ethical and legal frameworks with increasing audacity. There will be no easy solutions to this challenge\u2014but there are some steps we can take to prepare for it. Research institutes, universities, and the authorities that regulate them must help ensure that people trained to design and build intelligent machines also receive a rigorous education in ethics. And those already on the frontlines of innovation need to concentrate on investing robots with true agency. Human efforts to determine accountability almost always depend on our ability to discover and analyze intention. If we are going to live in a world with machines who act more and more like people and who make ever more \u201cpersonal\u201d choices, then we should insist that robots also be able to communicate with us about what they know, how they know it, and what they want. A DOUBLE-EDGED SWORD For a good illustration of the kinds of quandaries that robots will pose by mixing clear social benefits with frustrating ethical dilemmas, consider the wheelchair. Today, more than 65 million people are confined to wheelchairs, contending with many more obstacles than their walking peers and sitting in a world designed for standing. But thanks to robotics, the next two decades will likely see the end of the wheelchair. Researchers at Carnegie Mellon; the University of California, Berkeley; and a number of other medical robotics laboratories are currently developing exoskeletal robotic legs that can sense objects and maintain balance. With these new tools, elderly people who are too frail to walk will find new footing, knowing that a slip that could result in a dangerous fracture will be far less likely. For visually impaired wheelchair users, exoskeletal robotic legs combined with computerized cameras and sensors will create a human-robot team: the person will select a high-level strategy\u2014say, going to a coffee shop\u2014and the legs will take care of the low-level operations of step-by-step navigation and motion. Such outcomes would represent unqualified gains for humanity. But as robotic prosthetics enter the mainstream, the able-bodied will surely want to take advantage of them, too. These prosthetics will house sensors and cloud-connected software that will exceed the human body\u2019s ability to sense, store, and process information. Such combinations are the first step in what futurists such as Hans Moravec and Ray Kurzweil have dubbed \u201ctranshumanism\u201d: a post-evolutionary transformation that will replace humans with a hybrid of man and machine. To date, hybrid performance has mostly fallen short of conventional human prowess, but it is merely a matter of time before human-robot couplings greatly outperform purely biological systems. In the United States, hardly any of the academic engineering programs that grant degrees in robotics require the in-depth study of ethics, human rights, privacy, or security. These superhuman capabilities will not be limited to physical action: computers are increasingly capable of receiving and interpreting brain signals transmitted through electrodes implanted in the head (or arranged around the head) and have even demonstrated rudimentary forms of brain-based machine control. Today, researchers are primarily interested in designing one-way systems, which can read brain signals and then send them to devices such as prosthetic limbs and cars. But no serious obstacles prevent computer interfaces from sending such signals right back, arming a human brain with a silicon turbocharge. The ability to perform complex mathematical calculations, produce top-quality language translation, and even deliver virtuosic musical performances might one day depend not solely on innate skill and practice but also on having access to the best brain-computer hybrid architecture. Such advantages, however, would run headlong into a set of ethical problems: just as a fine line separates genetic engineering from eugenics, so, too, is there no clear distinction between robotics that would lift a human\u2019s capabilities to their organic limit and those that would vault a person beyond all known boundaries. Such technologies have the potential to vastly magnify the already-significant gaps in opportunity and achievement that exist between people of different economic means. In the robotic future, today\u2019s intense debates about social and economic inequality will seem almost quaint. EVERY STEP YOU TAKE Democracy and capitalism rely on a common underlying assumption: if informed individuals acting rationally can express their free will, their individual choices will combine to yield the best outcome for society as a whole. Both systems thus depend on two conditions: people must have access to information and must have the power to make choices. The age of \u201cbig data\u201d promises greater access to information of all kinds. But robotic technologies that collect and interpret unprecedented amounts of data about human behavior actually threaten both access to information and freedom of choice. A fundamental shift has begun to take place in the relationship between automation technologies and human behavior. Conventional interactions between consumers and firms are based on direct economic exchanges: consumers pay for goods and services, and firms provide them. In the digital economy, however, consumers benefit more and more from seemingly free service, while firms profit not by directly charging consumers but by collecting and then monetizing information about consumers\u2019 behavior, often without their knowledge or acquiescence. This kind of basic data mining has become commonplace: think, for example, of how Google analyzes users\u2019 search histories and e-mail messages in order to determine what products they might be interested in buying and then uses that information to sell targeted advertising space to other firms. JOHN GRESS / COURTESY REUTERS Zac Vawter, a 31-year-old software engineer, uses the world's first neural-controlled Bionic leg in Chicago, November 2012. As more automation technologies begin to appear in the physical world, such processes will become even more invasive. In the coming years, digital advertisements will incorporate pupil-tracking technology\u2014currently in development at Carnegie Mellon and elsewhere\u2014that can monitor the gazes of passersby from meters away. Fitted with sophisticated cameras and software that can estimate a passerby\u2019s age and gender and observe facial cues to recognize moods and emotions, interactive billboards will not merely display static advertisements to viewers but also conduct ongoing tests of human responses to particular messages and stimuli, noting the emotional responses and purchasing behaviors of every subcategory of consumer and compiling massive, aggregated histories of the effect of each advertisement. This very concept was depicted in the 2002 science-fiction film Minority Report during a scene in which the protagonist (played by Tom Cruise) walks through a shopping center where holographic signs and avatars bombard him with marketing messages, calling out his name and offering him products and services specifically tailored to him. Far from suggesting a shopper\u2019s paradise, the scene is deeply unsettling, because it captures the way that intelligent machines might someday push humans\u2019 buttons so well that we will become the automatons, under the sway (and even control) of well-informed, highly social robots that have learned how to influence our behavior. When an intelligent machine causes injury, who is to blame? The robot? Its owner? Its creator? A less fantastic, shorter-term concern about the effects of robotics and machine learning on human agency and well-being revolves around labor. In The Second Machine Age , the economist Erik Brynjolfsson and the information technology expert Andrew McAfee demonstrate that robotic technology is increasingly more efficient than human labor, offering a significant return on investment when performing both routine manual jobs and simple mental tasks. Unlike human workers, whose collective performance doesn\u2019t change much over time, robot employees keep getting more efficient. With each advance in robot capability, it becomes harder to justify employing humans, even in jobs that require specialized skills or knowledge. No fundamental barrier exists to stop the onward march of robots into the labor market: almost every job, blue collar and white collar, will be at risk in an age of exponential progress in computing and robotics. The result might be higher unemployment, which, in turn, could contribute to rising economic inequality, as the wealth created by new technologies benefits fewer and fewer people. ONE SINGULAR SENSATION In discussions and debates among technologists, economists, and philosophers, such visions of the future sit alongside a number of less grim prognostications about what the world will look like once artificial intelligence and machine learning have produced the \u201ctechnological singularity\u201d: computer systems that can themselves invent new technologies that surpass those created by their original human creators. The details of such predictions vary depending on the forecaster. Some, such as Moravec, foresee a post-evolutionary successor to Homo sapiens that will usher in a new leisure age of comfort and prosperity. Others envision robotic vessels able to \u201cupload\u201d human consciousness. And Kurzweil has suggested that the technological singularity will offer people a kind of software-based immortality. These long-term views, however, can distract from the more prosaic near-term consequences of the robotics revolution\u2014not the great dislocations caused by a superhuman machine consciousness but rather the small train wrecks that will result from the spread of mediocre robot intelligence. Today, nearly all our social interactions take place with other humans, but we are on the cusp of an era in which machines will become our usual interlocutors. Our driverless cars will join in our fights with one another over parking spots: when an argument leads to a fender-bender, we will insist to our robot mechanics that they have not repaired our robot cars properly. We will negotiate with robot hostesses for corner tables at restaurants where the food is prepared by robot chefs. Every day, we will encounter robots, from hovering drones to delivery machines to taxis, that will operate seamlessly with and without human remote control; daily life will involve constantly interacting with machines without knowing just how much another person might be involved in the machine\u2019s response. There will be no room in such infinitely adjustable human-robot systems for us to treat robots one way and humans another; each style of interaction will infect the other, and the result will be an erosion of our sense of identity. But the result need not be a robot dystopia. A clear set of decisions about robot design and regulation stand between today\u2019s world of human agency and tomorrow\u2019s world of robot autonomy. Inventors must begin to combine technological ingenuity with sociological awareness, and governments need to design institutions and processes that will help integrate new, artificial agents into society. Today, all civil engineers are required to study ethics because an incorrectly designed bridge can cause great public harm. Roboticists face this same kind of responsibility today, because their creations are no longer mere academic pursuits. Computer science departments, which typically sponsor robotics research, must follow the lead of civil engineering departments and require that every degree candidate receive sufficient training in ethics and some exposure to sociology. But preparing tomorrow\u2019s robot creators will help only so much; the clock is ticking, and today\u2019s roboticists must begin to think more clearly about how to build intelligent machines able to integrate themselves into societies. An important first step would be to make clear distinctions between robotic appliances and robotic agents. Robots that follow fixed directions and make no autonomous decisions should wear their limited cognitive abilities on their sleeves. This means they should not have faces, and they should not speak or communicate like people or express human emotions: a robotic vacuum cleaner shouldn\u2019t tell its owner that it misses him when he\u2019s at work. As for robots designed to formulate goals, make decisions, and convince people of their agency, they need to grow up. If roboticists want such machines to have anthropomorphic qualities, then their robots must also accept direct accountability: people must be able to question these machines about their knowledge, their goals, their desires, and their intentions. Knowledge and transparency, the most valuable goods promised by the dawn of the information age in the last century, will take on even greater importance in the age of automation. Educators and regulators must help robot inventors acquire knowledge, and the inventors, in turn, must pledge to create more transparent artificial beings.","title":"The Coming Robot Dystopia"},{"location":"EL5131/FourIR/Articles/article1/#the-coming-robot-dystopia","text":"All Too Inhuman By Illah Reza Nourbakhsh The term \u201crobotics revolution\u201d evokes images of the future: a not-too-distant future, perhaps, but an era surely distinct from the present. In fact, that revolution is already well under way. Today, military robots appear on battlefields, drones fill the skies, driverless cars take to the roads, and \u201ctelepresence robots\u201d allow people to manifest themselves halfway around the world from their actual location. But the exciting, even seductive appeal of these technological advances has overshadowed deep, sometimes uncomfortable questions about what increasing human-robot interaction will mean for society. Robotic technologies that collect, interpret, and respond to massive amounts of real-world data on behalf of governments, corporations, and ordinary people will unquestionably advance human life. But they also have the potential to produce dystopian outcomes. We are hardly on the brink of the nightmarish futures conjured by Hollywood movies such as The Matrix or The Terminator , in which intelligent machines attempt to enslave or exterminate humans. But those dark fantasies contain a seed of truth: the robotic future will involve dramatic tradeoffs, some so significant that they could lead to a collective identity crisis over what it means to be human. This is a familiar warning when it comes to technological innovations of all kinds. But there is a crucial distinction between what\u2019s happening now and the last great breakthrough in robotic technology, when manufacturing automatons began to appear on factory floors during the late twentieth century. Back then, clear boundaries separated industrial robots from humans: protective fences isolated robot workspaces, ensuring minimal contact between man and machine, and humans and robots performed wholly distinct tasks without interacting. Such barriers have been breached, not only in the workplace but also in the wider society: robots now share the formerly human-only commons, and humans will increasingly interact socially with a diverse ecosystem of robots. The trouble is that the rich traditions of moral thought that guide human relationships have no equivalent when it comes to robot-to-human interactions. And of course, robots themselves have no innate drive to avoid ethical transgressions regarding, say, privacy or the protection of human life. How robots interact with people depends to a great deal on how much their creators know or care about such issues, and robot creators tend to be engineers, programmers, and designers with little training in ethics, human rights, privacy, or security. In the United States, hardly any of the academic engineering programs that grant degrees in robotics require the in-depth study of such fields. LUKE MACGREGOR / COURTESY REUTERS A robot is pictured in front of the Houses of Parliament and Westminster Abbey as part of the Campaign to Stop Killer Robots in London, April 2013. One might hope that political and legal institutions would fill that gap, by steering and constraining the development of robots with the goal of reducing their potential for harm. Ideally, the rapid expansion of robots\u2019 roles in society would be matched by equally impressive advances in regulation and in tort and liability law, so that societies could deal with the issues of accountability and responsibility that will inevitably crop up in the coming years. But the pace of change in robotics is far outstripping the ability of regulators and lawmakers to keep up, especially as large corporations pour massive investments into secretive robotics projects that are nearly invisible to government regulators. We are hardly on the brink of the nightmarish futures conjured by The Matrix or The Terminator. But those dark fantasies contain a seed of truth: the robotic future will involve dramatic tradeoffs. There is every reason to believe that this gap between robot capability and robot regulation will widen every year, posing all kinds of quandaries for law and government. Imagine an adaptive robot that lives with and learns from its human owner. Its behavior over time will be a function of its original programming mixed with the influence of its environment and \u201cupbringing.\u201d It would be difficult for existing liability laws to apportion responsibility if such a machine caused injury, since its actions would be determined not merely by computer code but also by a deep neural-like network that would have learned from various sources. Who would be to blame? The robot? Its owner? Its creator? We face a future in which robots will test the boundaries of our ethical and legal frameworks with increasing audacity. There will be no easy solutions to this challenge\u2014but there are some steps we can take to prepare for it. Research institutes, universities, and the authorities that regulate them must help ensure that people trained to design and build intelligent machines also receive a rigorous education in ethics. And those already on the frontlines of innovation need to concentrate on investing robots with true agency. Human efforts to determine accountability almost always depend on our ability to discover and analyze intention. If we are going to live in a world with machines who act more and more like people and who make ever more \u201cpersonal\u201d choices, then we should insist that robots also be able to communicate with us about what they know, how they know it, and what they want. A DOUBLE-EDGED SWORD For a good illustration of the kinds of quandaries that robots will pose by mixing clear social benefits with frustrating ethical dilemmas, consider the wheelchair. Today, more than 65 million people are confined to wheelchairs, contending with many more obstacles than their walking peers and sitting in a world designed for standing. But thanks to robotics, the next two decades will likely see the end of the wheelchair. Researchers at Carnegie Mellon; the University of California, Berkeley; and a number of other medical robotics laboratories are currently developing exoskeletal robotic legs that can sense objects and maintain balance. With these new tools, elderly people who are too frail to walk will find new footing, knowing that a slip that could result in a dangerous fracture will be far less likely. For visually impaired wheelchair users, exoskeletal robotic legs combined with computerized cameras and sensors will create a human-robot team: the person will select a high-level strategy\u2014say, going to a coffee shop\u2014and the legs will take care of the low-level operations of step-by-step navigation and motion. Such outcomes would represent unqualified gains for humanity. But as robotic prosthetics enter the mainstream, the able-bodied will surely want to take advantage of them, too. These prosthetics will house sensors and cloud-connected software that will exceed the human body\u2019s ability to sense, store, and process information. Such combinations are the first step in what futurists such as Hans Moravec and Ray Kurzweil have dubbed \u201ctranshumanism\u201d: a post-evolutionary transformation that will replace humans with a hybrid of man and machine. To date, hybrid performance has mostly fallen short of conventional human prowess, but it is merely a matter of time before human-robot couplings greatly outperform purely biological systems. In the United States, hardly any of the academic engineering programs that grant degrees in robotics require the in-depth study of ethics, human rights, privacy, or security. These superhuman capabilities will not be limited to physical action: computers are increasingly capable of receiving and interpreting brain signals transmitted through electrodes implanted in the head (or arranged around the head) and have even demonstrated rudimentary forms of brain-based machine control. Today, researchers are primarily interested in designing one-way systems, which can read brain signals and then send them to devices such as prosthetic limbs and cars. But no serious obstacles prevent computer interfaces from sending such signals right back, arming a human brain with a silicon turbocharge. The ability to perform complex mathematical calculations, produce top-quality language translation, and even deliver virtuosic musical performances might one day depend not solely on innate skill and practice but also on having access to the best brain-computer hybrid architecture. Such advantages, however, would run headlong into a set of ethical problems: just as a fine line separates genetic engineering from eugenics, so, too, is there no clear distinction between robotics that would lift a human\u2019s capabilities to their organic limit and those that would vault a person beyond all known boundaries. Such technologies have the potential to vastly magnify the already-significant gaps in opportunity and achievement that exist between people of different economic means. In the robotic future, today\u2019s intense debates about social and economic inequality will seem almost quaint. EVERY STEP YOU TAKE Democracy and capitalism rely on a common underlying assumption: if informed individuals acting rationally can express their free will, their individual choices will combine to yield the best outcome for society as a whole. Both systems thus depend on two conditions: people must have access to information and must have the power to make choices. The age of \u201cbig data\u201d promises greater access to information of all kinds. But robotic technologies that collect and interpret unprecedented amounts of data about human behavior actually threaten both access to information and freedom of choice. A fundamental shift has begun to take place in the relationship between automation technologies and human behavior. Conventional interactions between consumers and firms are based on direct economic exchanges: consumers pay for goods and services, and firms provide them. In the digital economy, however, consumers benefit more and more from seemingly free service, while firms profit not by directly charging consumers but by collecting and then monetizing information about consumers\u2019 behavior, often without their knowledge or acquiescence. This kind of basic data mining has become commonplace: think, for example, of how Google analyzes users\u2019 search histories and e-mail messages in order to determine what products they might be interested in buying and then uses that information to sell targeted advertising space to other firms. JOHN GRESS / COURTESY REUTERS Zac Vawter, a 31-year-old software engineer, uses the world's first neural-controlled Bionic leg in Chicago, November 2012. As more automation technologies begin to appear in the physical world, such processes will become even more invasive. In the coming years, digital advertisements will incorporate pupil-tracking technology\u2014currently in development at Carnegie Mellon and elsewhere\u2014that can monitor the gazes of passersby from meters away. Fitted with sophisticated cameras and software that can estimate a passerby\u2019s age and gender and observe facial cues to recognize moods and emotions, interactive billboards will not merely display static advertisements to viewers but also conduct ongoing tests of human responses to particular messages and stimuli, noting the emotional responses and purchasing behaviors of every subcategory of consumer and compiling massive, aggregated histories of the effect of each advertisement. This very concept was depicted in the 2002 science-fiction film Minority Report during a scene in which the protagonist (played by Tom Cruise) walks through a shopping center where holographic signs and avatars bombard him with marketing messages, calling out his name and offering him products and services specifically tailored to him. Far from suggesting a shopper\u2019s paradise, the scene is deeply unsettling, because it captures the way that intelligent machines might someday push humans\u2019 buttons so well that we will become the automatons, under the sway (and even control) of well-informed, highly social robots that have learned how to influence our behavior. When an intelligent machine causes injury, who is to blame? The robot? Its owner? Its creator? A less fantastic, shorter-term concern about the effects of robotics and machine learning on human agency and well-being revolves around labor. In The Second Machine Age , the economist Erik Brynjolfsson and the information technology expert Andrew McAfee demonstrate that robotic technology is increasingly more efficient than human labor, offering a significant return on investment when performing both routine manual jobs and simple mental tasks. Unlike human workers, whose collective performance doesn\u2019t change much over time, robot employees keep getting more efficient. With each advance in robot capability, it becomes harder to justify employing humans, even in jobs that require specialized skills or knowledge. No fundamental barrier exists to stop the onward march of robots into the labor market: almost every job, blue collar and white collar, will be at risk in an age of exponential progress in computing and robotics. The result might be higher unemployment, which, in turn, could contribute to rising economic inequality, as the wealth created by new technologies benefits fewer and fewer people. ONE SINGULAR SENSATION In discussions and debates among technologists, economists, and philosophers, such visions of the future sit alongside a number of less grim prognostications about what the world will look like once artificial intelligence and machine learning have produced the \u201ctechnological singularity\u201d: computer systems that can themselves invent new technologies that surpass those created by their original human creators. The details of such predictions vary depending on the forecaster. Some, such as Moravec, foresee a post-evolutionary successor to Homo sapiens that will usher in a new leisure age of comfort and prosperity. Others envision robotic vessels able to \u201cupload\u201d human consciousness. And Kurzweil has suggested that the technological singularity will offer people a kind of software-based immortality. These long-term views, however, can distract from the more prosaic near-term consequences of the robotics revolution\u2014not the great dislocations caused by a superhuman machine consciousness but rather the small train wrecks that will result from the spread of mediocre robot intelligence. Today, nearly all our social interactions take place with other humans, but we are on the cusp of an era in which machines will become our usual interlocutors. Our driverless cars will join in our fights with one another over parking spots: when an argument leads to a fender-bender, we will insist to our robot mechanics that they have not repaired our robot cars properly. We will negotiate with robot hostesses for corner tables at restaurants where the food is prepared by robot chefs. Every day, we will encounter robots, from hovering drones to delivery machines to taxis, that will operate seamlessly with and without human remote control; daily life will involve constantly interacting with machines without knowing just how much another person might be involved in the machine\u2019s response. There will be no room in such infinitely adjustable human-robot systems for us to treat robots one way and humans another; each style of interaction will infect the other, and the result will be an erosion of our sense of identity. But the result need not be a robot dystopia. A clear set of decisions about robot design and regulation stand between today\u2019s world of human agency and tomorrow\u2019s world of robot autonomy. Inventors must begin to combine technological ingenuity with sociological awareness, and governments need to design institutions and processes that will help integrate new, artificial agents into society. Today, all civil engineers are required to study ethics because an incorrectly designed bridge can cause great public harm. Roboticists face this same kind of responsibility today, because their creations are no longer mere academic pursuits. Computer science departments, which typically sponsor robotics research, must follow the lead of civil engineering departments and require that every degree candidate receive sufficient training in ethics and some exposure to sociology. But preparing tomorrow\u2019s robot creators will help only so much; the clock is ticking, and today\u2019s roboticists must begin to think more clearly about how to build intelligent machines able to integrate themselves into societies. An important first step would be to make clear distinctions between robotic appliances and robotic agents. Robots that follow fixed directions and make no autonomous decisions should wear their limited cognitive abilities on their sleeves. This means they should not have faces, and they should not speak or communicate like people or express human emotions: a robotic vacuum cleaner shouldn\u2019t tell its owner that it misses him when he\u2019s at work. As for robots designed to formulate goals, make decisions, and convince people of their agency, they need to grow up. If roboticists want such machines to have anthropomorphic qualities, then their robots must also accept direct accountability: people must be able to question these machines about their knowledge, their goals, their desires, and their intentions. Knowledge and transparency, the most valuable goods promised by the dawn of the information age in the last century, will take on even greater importance in the age of automation. Educators and regulators must help robot inventors acquire knowledge, and the inventors, in turn, must pledge to create more transparent artificial beings.","title":"The Coming Robot Dystopia"},{"location":"EL5131/Technology/Big%20Tech/index.md/","text":"Big Tech a compilation of important notes Definitions Big Tech / FAGMA : Word/Acronym used to describe the five largest tech companies in the world, namely Facebook, Amazon, Google, Microsoft and Apple Sometimes includes Tesla and Nvidia (FAANG) Apple was the first of these to achieve a two trillion dollar valuation in August 2020 The combined net worth of these five, in addition to five other top tech companies, rounds up to a whopping nine trillion dollars. BATX : Acronym used to describe China's four largest tech companies, that being Baidu, Alibaba, Tencent and Xiaomi. ByteDance is often counted into this list, despite it's name being missing from the actual acronym Beijing has, over the past two years, started to crack down on these companies, especially Alibaba. Scandals and Problems The NSA Scandal : It was revealed in 2013 that America's NSA and their British counterpart had been stealing information secretively from users of Google, Yahoo and many other tech products, with the creators of the products being completely oblivious to this happening. This led to widespread fear regarding government surveillance and stigmated views against Big Tech. Classified as \"surveillance\", since the data was used as a form of targeted monitoring to triangulate individuals based on the others' relative positions in order to identify potential threats. The Cambridge Analytica Scandal : In 2016, after Trump's victory at the Election, it was revealed that the company it was working with, Cambridge Analytica, had been surreptitiously stealing data from Facebook users and in an effort to manipulate and brainwash them, by showing people against Trump's campaign more positive articles about his work to influence their decision to work. This massive scandal led to widespread stigma against Big Tech and especially Facebook, which had yet to come into the line of fire. However, Facebook was the most severely hit, with the years that followed being some of its worst. The Rohingya Genocide and Facebook's Involvement : The Rohingya Genocide happened as a result of the overly stigmatized views of people against the Rohingya Muslims, a set of muslims often considered some of the, if not the most discriminated people in the world. During the genocide, many of the military officers went on Facebook posing as popular individuals and purposely incited hate speech, which was one of the main catalysts for the truly inhumane acts that followed. The Christchurch Shootings : In March 2019, a man openly streamed his video on Facebook Live as he went and shot nearly 51 people in 2 mosques in Christchurch, New Zealand, in an effort to spread hate speech towards the Muslim population in New Zealand. The videos surfaced on Facebook, with many audience members surprisingly cheering on for the act to continue, leaving a bleak shot of the white nationalism ever present in Oceania today. While this may not be fully attributed to Facebook, it was nonetheless a display of one of Facebook's weakest moments. The Facebook Whistleblower, Frances Haughen : An ex-employee of Facebook, Frances Haughen had been secretly copying and storing hundreds of internal files in Facebook with regards to research it had done on the impacts of its algorithms to better serve their purpose as a tech company, and shared it with the Wall Street Journal (WSJ), leading to the creation of the \"Facebook Files\" series at the WSJ. Research done by Facebook itself found that angrier posts often got higher engagements , hence in 2018, Facebook's algorithms were reconfigured to give priority to these posts in user feeds. An implication of this would entail that more individuals would be angrier while reading the post and it is definitely possible that it may have led to incitement of more hate speech. This is incredibly problematic, but Facebook's insistence to focus on the profit made by more user retention due to this algorithm led to this being left to the wayside by the developers. During the sudden appearance of the pandemic, a large number of posts with contradictory information was shared on these social media platforms that left many users excessively confused. In fact, a study by the National Centre of Infectious Diseases (NCID) revealed that 6 in 10 Singaporeans received fake information regarding the pandemic. This information overload, or \"infodemic\", as termed by the World Health Organisation (WHO), has led to the spread of excessive amounts of misinformation. During events like the excessive aftermath of George Floyd's death and the Taliban's take-over of Afghanistan, an incredible number of posts led to these events being blown out of proportion. The Capitol Riots and Trump's Ban : After the absolutely horrific events of January 6th 2021, where a mob of individuals barreled their way into the US Capitol Building in support of Trump after his recent loss in the presidential elections, Trump was permanently banned from multiple social media platforms, like Facebook and Twitter. YouTube also banned him, and Apple and Google both refused to offer the Parler, which was commonly associated with Trump and his followers, on their respective mobile app stores. This was followed officially with the impeachment of Trump just a few short days of his term, but it also painted a picture of just how powerful Big Tech Countries, with their act of censorship making them act more like a government than a group of businesses. WhatsApp's Privacy Settings : In the beginning of 2021, millions of people around the globe received a message on their phones that WhatsApp was updating their privacy settings such that certain data would be stored by WhatsApp. Being an application recently acquired by Facebook, this message led to immediate terror and uproar amongst the general populace, with many worldwide immediately deleting the app and switching to Telegram and Signal, the latter of which was instigated by Tesla CEO Elon Musk. While these concerns ended up being overexaggerated, it was still notable that the general populace has become more receptive to privacy concerns and the impacts that these settings could have.","title":"Home"},{"location":"EL5131/Technology/Big%20Tech/index.md/#big-tech","text":"a compilation of important notes","title":"Big Tech"},{"location":"EL5131/Technology/Big%20Tech/index.md/#definitions","text":"Big Tech / FAGMA : Word/Acronym used to describe the five largest tech companies in the world, namely Facebook, Amazon, Google, Microsoft and Apple Sometimes includes Tesla and Nvidia (FAANG) Apple was the first of these to achieve a two trillion dollar valuation in August 2020 The combined net worth of these five, in addition to five other top tech companies, rounds up to a whopping nine trillion dollars. BATX : Acronym used to describe China's four largest tech companies, that being Baidu, Alibaba, Tencent and Xiaomi. ByteDance is often counted into this list, despite it's name being missing from the actual acronym Beijing has, over the past two years, started to crack down on these companies, especially Alibaba.","title":"Definitions"},{"location":"EL5131/Technology/Big%20Tech/index.md/#scandals-and-problems","text":"The NSA Scandal : It was revealed in 2013 that America's NSA and their British counterpart had been stealing information secretively from users of Google, Yahoo and many other tech products, with the creators of the products being completely oblivious to this happening. This led to widespread fear regarding government surveillance and stigmated views against Big Tech. Classified as \"surveillance\", since the data was used as a form of targeted monitoring to triangulate individuals based on the others' relative positions in order to identify potential threats. The Cambridge Analytica Scandal : In 2016, after Trump's victory at the Election, it was revealed that the company it was working with, Cambridge Analytica, had been surreptitiously stealing data from Facebook users and in an effort to manipulate and brainwash them, by showing people against Trump's campaign more positive articles about his work to influence their decision to work. This massive scandal led to widespread stigma against Big Tech and especially Facebook, which had yet to come into the line of fire. However, Facebook was the most severely hit, with the years that followed being some of its worst. The Rohingya Genocide and Facebook's Involvement : The Rohingya Genocide happened as a result of the overly stigmatized views of people against the Rohingya Muslims, a set of muslims often considered some of the, if not the most discriminated people in the world. During the genocide, many of the military officers went on Facebook posing as popular individuals and purposely incited hate speech, which was one of the main catalysts for the truly inhumane acts that followed. The Christchurch Shootings : In March 2019, a man openly streamed his video on Facebook Live as he went and shot nearly 51 people in 2 mosques in Christchurch, New Zealand, in an effort to spread hate speech towards the Muslim population in New Zealand. The videos surfaced on Facebook, with many audience members surprisingly cheering on for the act to continue, leaving a bleak shot of the white nationalism ever present in Oceania today. While this may not be fully attributed to Facebook, it was nonetheless a display of one of Facebook's weakest moments. The Facebook Whistleblower, Frances Haughen : An ex-employee of Facebook, Frances Haughen had been secretly copying and storing hundreds of internal files in Facebook with regards to research it had done on the impacts of its algorithms to better serve their purpose as a tech company, and shared it with the Wall Street Journal (WSJ), leading to the creation of the \"Facebook Files\" series at the WSJ. Research done by Facebook itself found that angrier posts often got higher engagements , hence in 2018, Facebook's algorithms were reconfigured to give priority to these posts in user feeds. An implication of this would entail that more individuals would be angrier while reading the post and it is definitely possible that it may have led to incitement of more hate speech. This is incredibly problematic, but Facebook's insistence to focus on the profit made by more user retention due to this algorithm led to this being left to the wayside by the developers. During the sudden appearance of the pandemic, a large number of posts with contradictory information was shared on these social media platforms that left many users excessively confused. In fact, a study by the National Centre of Infectious Diseases (NCID) revealed that 6 in 10 Singaporeans received fake information regarding the pandemic. This information overload, or \"infodemic\", as termed by the World Health Organisation (WHO), has led to the spread of excessive amounts of misinformation. During events like the excessive aftermath of George Floyd's death and the Taliban's take-over of Afghanistan, an incredible number of posts led to these events being blown out of proportion. The Capitol Riots and Trump's Ban : After the absolutely horrific events of January 6th 2021, where a mob of individuals barreled their way into the US Capitol Building in support of Trump after his recent loss in the presidential elections, Trump was permanently banned from multiple social media platforms, like Facebook and Twitter. YouTube also banned him, and Apple and Google both refused to offer the Parler, which was commonly associated with Trump and his followers, on their respective mobile app stores. This was followed officially with the impeachment of Trump just a few short days of his term, but it also painted a picture of just how powerful Big Tech Countries, with their act of censorship making them act more like a government than a group of businesses. WhatsApp's Privacy Settings : In the beginning of 2021, millions of people around the globe received a message on their phones that WhatsApp was updating their privacy settings such that certain data would be stored by WhatsApp. Being an application recently acquired by Facebook, this message led to immediate terror and uproar amongst the general populace, with many worldwide immediately deleting the app and switching to Telegram and Signal, the latter of which was instigated by Tesla CEO Elon Musk. While these concerns ended up being overexaggerated, it was still notable that the general populace has become more receptive to privacy concerns and the impacts that these settings could have.","title":"Scandals and Problems"},{"location":"EL5131/Technology/Big%20Tech/Articles/article0/","text":"Big tech and censorship Silicon Valley should not be given control over free speech The Economist, Jan 16th 2021 Find the article here: https://www.economist.com/leaders/2021/01/16/big-tech-and-censorship The first reaction of many people was one of relief. On January 6th, with 14 days remaining of his term, the social-media president was suspended from Twitter after years of pumping abuse, lies and nonsense into the public sphere. Soon after, many of his cronies and supporters were shut down online by Silicon Valley, too. The end of their cacophony was blissful. But the peace belies a limiting of free speech that is chilling for America\u2014and all democracies. The bans that followed the storming of the Capitol were chaotic. On January 7th Facebook issued an \u201cindefinite\u201d suspension of Donald Trump. Twitter followed with a permanent ban a day later. Snapchat and YouTube barred him. An array of other accounts were suspended. Google and Apple booted Parler, a small social network popular with the far-right, from their app stores and Amazon kicked Parler off its cloud service, forcing it offline entirely. Surely this was acceptable in the face of a mob on the rampage? Legally, private companies can do as they choose. However, some decisions lacked consistency or proportionality. Although Twitter cited a \u201crisk of further incitement of violence\u201d by Mr Trump, the tweets it pointed to did not cross the common legal threshold defining an abuse of the constitutional right to free speech. Meanwhile Ayatollah Ali Khamenei is still on Twitter and death threats are easy to find online. The companies ought to have focused on individual posts for incitement. Instead they have banned people, including the president, pushing fringe voices further from the mainstream. In some cases action was needed, as with Parler\u2019s poorly policed and violent exchanges, but overall there was no clear test for when speech should be banned. The internet\u2019s infrastructure, including cloud-computing services, which should be neutral, risks being drawn into divisive partisan battles. The other problem is who made the decisions. The tech industry\u2019s concentration means that a few unelected and unaccountable executives are in control. Perhaps their intent really is to protect democracy, but they may also have other, less elevated motives. Some Democrats cheered, but they should evaluate any new speech regime based on its broader application. Otherwise an act that silenced their enemies last week could become a precedent for silencing them in future. The regrets were telling. Angela Merkel, Germany\u2019s leader, said that private firms should not determine speech rules. Alexei Navalny, a Russian dissident, decried an \u201cunacceptable act of censorship\u201d. Even Jack Dorsey, Twitter\u2019s CEO, called it a \u201cdangerous precedent\u201d. There is a better way to deal with speech online. Making the industry more competitive would help by diluting the clout of individual firms and by stimulating new business models that do not rely on virality. But for as long as the industry is an oligopoly, another approach is needed. The first step is to define a test of what should be censored. In America that should be based on the constitutional protection of speech. If companies want to go further by attaching warnings or limiting legal content they need to be transparent and predictable. Difficult judgments should fall to independent non-statutory boards that give people the right of appeal. Over 80% of Twitter and Facebook users live outside America. In most countries tech firms should adhere to local laws on speech\u2014Germany\u2019s rules on hate speech, say. In autocracies, like Belarus, they should default to the standards they observe in America. Again, judgments about which standards apply in which country could be guided by media boards. This may harm American firms in more places: this week Uganda banned Facebook and Twitter ahead of a contentious election. America needs to resolve its constitutional crisis through a political process, not censorship. And the world must seek a better way of dealing with speech online than allowing tech oligopolies to take control of fundamental liberties.","title":"Big tech and censorship"},{"location":"EL5131/Technology/Big%20Tech/Articles/article0/#big-tech-and-censorship","text":"Silicon Valley should not be given control over free speech The Economist, Jan 16th 2021 Find the article here: https://www.economist.com/leaders/2021/01/16/big-tech-and-censorship The first reaction of many people was one of relief. On January 6th, with 14 days remaining of his term, the social-media president was suspended from Twitter after years of pumping abuse, lies and nonsense into the public sphere. Soon after, many of his cronies and supporters were shut down online by Silicon Valley, too. The end of their cacophony was blissful. But the peace belies a limiting of free speech that is chilling for America\u2014and all democracies. The bans that followed the storming of the Capitol were chaotic. On January 7th Facebook issued an \u201cindefinite\u201d suspension of Donald Trump. Twitter followed with a permanent ban a day later. Snapchat and YouTube barred him. An array of other accounts were suspended. Google and Apple booted Parler, a small social network popular with the far-right, from their app stores and Amazon kicked Parler off its cloud service, forcing it offline entirely. Surely this was acceptable in the face of a mob on the rampage? Legally, private companies can do as they choose. However, some decisions lacked consistency or proportionality. Although Twitter cited a \u201crisk of further incitement of violence\u201d by Mr Trump, the tweets it pointed to did not cross the common legal threshold defining an abuse of the constitutional right to free speech. Meanwhile Ayatollah Ali Khamenei is still on Twitter and death threats are easy to find online. The companies ought to have focused on individual posts for incitement. Instead they have banned people, including the president, pushing fringe voices further from the mainstream. In some cases action was needed, as with Parler\u2019s poorly policed and violent exchanges, but overall there was no clear test for when speech should be banned. The internet\u2019s infrastructure, including cloud-computing services, which should be neutral, risks being drawn into divisive partisan battles. The other problem is who made the decisions. The tech industry\u2019s concentration means that a few unelected and unaccountable executives are in control. Perhaps their intent really is to protect democracy, but they may also have other, less elevated motives. Some Democrats cheered, but they should evaluate any new speech regime based on its broader application. Otherwise an act that silenced their enemies last week could become a precedent for silencing them in future. The regrets were telling. Angela Merkel, Germany\u2019s leader, said that private firms should not determine speech rules. Alexei Navalny, a Russian dissident, decried an \u201cunacceptable act of censorship\u201d. Even Jack Dorsey, Twitter\u2019s CEO, called it a \u201cdangerous precedent\u201d. There is a better way to deal with speech online. Making the industry more competitive would help by diluting the clout of individual firms and by stimulating new business models that do not rely on virality. But for as long as the industry is an oligopoly, another approach is needed. The first step is to define a test of what should be censored. In America that should be based on the constitutional protection of speech. If companies want to go further by attaching warnings or limiting legal content they need to be transparent and predictable. Difficult judgments should fall to independent non-statutory boards that give people the right of appeal. Over 80% of Twitter and Facebook users live outside America. In most countries tech firms should adhere to local laws on speech\u2014Germany\u2019s rules on hate speech, say. In autocracies, like Belarus, they should default to the standards they observe in America. Again, judgments about which standards apply in which country could be guided by media boards. This may harm American firms in more places: this week Uganda banned Facebook and Twitter ahead of a contentious election. America needs to resolve its constitutional crisis through a political process, not censorship. And the world must seek a better way of dealing with speech online than allowing tech oligopolies to take control of fundamental liberties.","title":"Big tech and censorship"},{"location":"EL5131/Technology/Big%20Tech/Articles/article1/","text":"How Facebook got addicted to spreading misinformation The company\u2019s AI algorithms gave it an insatiable habit for lies and hate speech. Now the man who built them can't fix the problem. MIT Technology Review, Karen Hao, March 11 2021 Find the original article here: https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/ Joaquin Qui\u00f1onero Candela , a director of AI at Facebook, was apologizing to his audience. It was March 23, 2018, just days after the revelation that Cambridge Analytica, a consultancy that worked on Donald Trump\u2019s 2016 presidential election campaign, had surreptitiously siphoned the personal data of tens of millions of Americans from their Facebook accounts in an attempt to influence how they voted. It was the biggest privacy breach in Facebook\u2019s history, and Qui\u00f1onero had been previously scheduled to speak at a conference on, among other things, \u201cthe intersection of AI, ethics, and privacy\u201d at the company. He considered canceling, but after debating it with his communications director, he\u2019d kept his allotted time. As he stepped up to face the room, he began with an admission. \u201cI\u2019ve just had the hardest five days in my tenure at Facebook,\u201d he remembers saying. \u201cIf there\u2019s criticism, I\u2019ll accept it.\u201d The Cambridge Analytica scandal would kick off Facebook\u2019s largest publicity crisis ever. It compounded fears that the algorithms that determine what people see on the platform were amplifying fake news and hate speech, and that Russian hackers had weaponized them to try to sway the election in Trump\u2019s favor. Millions began deleting the app ; employees left in protest; the company\u2019s market capitalization plunged by more than $100 billion after its July earnings call. In the ensuing months, Mark Zuckerberg began his own apologizing. He apologized for not taking \u201c a broad enough view \u201d of Facebook\u2019s responsibilities, and for his mistakes as a CEO. Internally, Sheryl Sandberg, the chief operating officer, kicked off a two-year civil rights audit to recommend ways the company could prevent the use of its platform to undermine democracy. Finally, Mike Schroepfer, Facebook\u2019s chief technology officer, asked Qui\u00f1onero to start a team with a directive that was a little vague: to examine the societal impact of the company\u2019s algorithms. The group named itself the Society and AI Lab (SAIL); last year it combined with another team working on issues of data privacy to form Responsible AI. Qui\u00f1onero was a natural pick for the job. He, as much as anybody, was the one responsible for Facebook\u2019s position as an AI powerhouse. In his six years at Facebook, he\u2019d created some of the first algorithms for targeting users with content precisely tailored to their interests, and then he\u2019d diffused those algorithms across the company. Now his mandate would be to make them less harmful. Facebook has consistently pointed to the efforts by Qui\u00f1onero and others as it seeks to repair its reputation. It regularly trots out various leaders to speak to the media about the ongoing reforms. In May of 2019, it granted a series of interviews with Schroepfer to the New York Times, which rewarded the company with a humanizing profile of a sensitive, well-intentioned executive striving to overcome the technical challenges of filtering out misinformation and hate speech from a stream of content that amounted to billions of pieces a day. These challenges are so hard that it makes Schroepfer emotional, wrote the Times: \u201cSometimes that brings him to tears.\u201d In the spring of 2020, it was apparently my turn. Ari Entin, Facebook\u2019s AI communications director, asked in an email if I wanted to take a deeper look at the company\u2019s AI work. After talking to several of its AI leaders, I decided to focus on Qui\u00f1onero. Entin happily obliged. As not only the leader of the Responsible AI team but also the man who had made Facebook into an AI-driven company, Qui\u00f1onero was a solid choice to use as a poster boy. He seemed a natural choice of subject to me, too. In the years since he\u2019d formed his team following the Cambridge Analytica scandal, concerns about the spread of lies and hate speech on Facebook had only grown. In late 2018 the company admitted that this activity had helped fuel a genocidal anti-Muslim campaign in Myanmar for several years. In 2020 Facebook started belatedly taking action against Holocaust deniers, anti-vaxxers, and the conspiracy movement QAnon. All these dangerous falsehoods were metastasizing thanks to the AI capabilities Qui\u00f1onero had helped build. The algorithms that underpin Facebook\u2019s business weren\u2019t created to filter out what was false or inflammatory; they were designed to make people share and engage with as much content as possible by showing them things they were most likely to be outraged or titillated by. Fixing this problem, to me, seemed like core Responsible AI territory. I began video-calling Qui\u00f1onero regularly. I also spoke to Facebook executives, current and former employees, industry peers, and external experts. Many spoke on condition of anonymity because they\u2019d signed nondisclosure agreements or feared retaliation. I wanted to know: What was Qui\u00f1onero\u2019s team doing to rein in the hate and lies on its platform? Joaquin Qui\u00f1onero Candela outside his home in the Bay Area, where he lives with his wife and three kids. But Entin and Qui\u00f1onero had a different agenda. Each time I tried to bring up these topics, my requests to speak about them were dropped or redirected. They only wanted to discuss the Responsible AI team\u2019s plan to tackle one specific kind of problem: AI bias, in which algorithms discriminate against particular user groups. An example would be an ad-targeting algorithm that shows certain job or housing opportunities to white people but not to minorities. By the time thousands of rioters stormed the US Capitol in January, organized in part on Facebook and fueled by the lies about a stolen election that had fanned out across the platform, it was clear from my conversations that the Responsible AI team had failed to make headway against misinformation and hate speech because it had never made those problems its main focus. More important, I realized, if it tried to, it would be set up for failure. The reason is simple. Everything the company does and chooses not to do flows from a single motivation: Zuckerberg\u2019s relentless desire for growth. Qui\u00f1onero\u2019s AI expertise supercharged that growth. His team got pigeonholed into targeting AI bias, as I learned in my reporting, because preventing such bias helps the company avoid proposed regulation that might, if passed, hamper that growth. Facebook leadership has also repeatedly weakened or halted many initiatives meant to clean up misinformation on the platform because doing so would undermine that growth. In other words, the Responsible AI team\u2019s work\u2014whatever its merits on the specific problem of tackling AI bias\u2014is essentially irrelevant to fixing the bigger problems of misinformation, extremism, and political polarization. And it\u2019s all of us who pay the price. \u201cWhen you\u2019re in the business of maximizing engagement, you\u2019re not interested in truth. You\u2019re not interested in harm, divisiveness, conspiracy. In fact, those are your friends,\u201d says Hany Farid, a professor at the University of California, Berkeley who collaborates with Facebook to understand image- and video-based misinformation on the platform. \u201cThey always do just enough to be able to put the press release out. But with a few exceptions, I don\u2019t think it\u2019s actually translated into better policies. They\u2019re never really dealing with the fundamental problems.\u201d In March of 2012, Qui\u00f1onero visited a friend in the Bay Area. At the time, he was a manager in Microsoft Research\u2019s UK office, leading a team using machine learning to get more visitors to click on ads displayed by the company\u2019s search engine, Bing. His expertise was rare, and the team was less than a year old. Machine learning, a subset of AI, had yet to prove itself as a solution to large-scale industry problems. Few tech giants had invested in the technology. Qui\u00f1onero\u2019s friend wanted to show off his new employer, one of the hottest startups in Silicon Valley: Facebook, then eight years old and already with close to a billion monthly active users (i.e., those who have logged in at least once in the past 30 days). As Qui\u00f1onero walked around its Menlo Park headquarters, he watched a lone engineer make a major update to the website, something that would have involved significant red tape at Microsoft. It was a memorable introduction to Zuckerberg\u2019s \u201cMove fast and break things\u201d ethos. Qui\u00f1onero was awestruck by the possibilities. Within a week, he had been through interviews and signed an offer to join the company. His arrival couldn\u2019t have been better timed. Facebook\u2019s ads service was in the middle of a rapid expansion as the company was preparing for its May IPO. The goal was to increase revenue and take on Google, which had the lion\u2019s share of the online advertising market. Machine learning, which could predict which ads would resonate best with which users and thus make them more effective, could be the perfect tool. Shortly after starting, Qui\u00f1onero was promoted to managing a team similar to the one he\u2019d led at Microsoft. Qui\u00f1onero started raising chickens in late 2019 as a way to unwind from the intensity of his job. Unlike traditional algorithms, which are hard-coded by engineers, machine-learning algorithms \u201ctrain\u201d on input data to learn the correlations within it. The trained algorithm, known as a machine-learning model, can then automate future decisions. An algorithm trained on ad click data, for example, might learn that women click on ads for yoga leggings more often than men. The resultant model will then serve more of those ads to women. Today at an AI-based company like Facebook, engineers generate countless models with slight variations to see which one performs best on a given problem.< Facebook\u2019s massive amounts of user data gave Qui\u00f1onero a big advantage. His team could develop models that learned to infer the existence not only of broad categories like \u201cwomen\u201d and \u201cmen,\u201d but of very fine-grained categories like \u201cwomen between 25 and 34 who liked Facebook pages related to yoga,\u201d and targeted ads to them. The finer-grained the targeting, the better the chance of a click, which would give advertisers more bang for their buck. Within a year his team had developed these models, as well as the tools for designing and deploying new ones faster. Before, it had taken Qui\u00f1onero\u2019s engineers six to eight weeks to build, train, and test a new model. Now it took only one. News of the success spread quickly. The team that worked on determining which posts individual Facebook users would see on their personal news feeds wanted to apply the same techniques. Just as algorithms could be trained to predict who would click what ad, they could also be trained to predict who would like or share what post, and then give those posts more prominence. If the model determined that a person really liked dogs, for instance, friends\u2019 posts about dogs would appear higher up on that user\u2019s news feed. Qui\u00f1onero\u2019s success with the news feed\u2014coupled with impressive new AI research being conducted outside the company\u2014caught the attention of Zuckerberg and Schroepfer. Facebook now had just over 1 billion users, making it more than eight times larger than any other social network, but they wanted to know how to continue that growth. The executives decided to invest heavily in AI, internet connectivity, and virtual reality. They created two AI teams. One was FAIR, a fundamental research lab that would advance the technology\u2019s state-of-the-art capabilities. The other, Applied Machine Learning (AML), would integrate those capabilities into Facebook\u2019s products and services. In December 2013, after months of courting and persuasion, the executives recruited Yann LeCun, one of the biggest names in the field, to lead FAIR. Three months later, Qui\u00f1onero was promoted again, this time to lead AML. (It was later renamed FAIAR, pronounced \u201cfire.\u201d) \u201cThat\u2019s how you know what\u2019s on his mind. I was always, for a couple of years, a few steps from Mark's desk.\u201d Joaquin Qui\u00f1onero Candela In his new role, Qui\u00f1onero built a new model-development platform for anyone at Facebook to access. Called FBLearner Flow , it allowed engineers with little AI experience to train and deploy machine-learning models within days. By mid-2016, it was in use by more than a quarter of Facebook\u2019s engineering team and had already been used to train over a million models, including models for image recognition, ad targeting, and content moderation. Zuckerberg\u2019s obsession with getting the whole world to use Facebook had found a powerful new weapon. Teams had previously used design tactics, like experimenting with the content and frequency of notifications, to try to hook users more effectively. Their goal, among other things, was to increase a metric called L6/7, the fraction of people who logged in to Facebook six of the previous seven days. L6/7 is just one of myriad ways in which Facebook has measured \u201cengagement\u201d\u2014the propensity of people to use its platform in any way, whether it\u2019s by posting things, commenting on them, liking or sharing them, or just looking at them. Now every user interaction once analyzed by engineers was being analyzed by algorithms. Those algorithms were creating much faster, more personalized feedback loops for tweaking and tailoring each user\u2019s news feed to keep nudging up engagement numbers. Zuckerberg, who sat in the center of Building 20, the main office at the Menlo Park headquarters, placed the new FAIR and AML teams beside him. Many of the original AI hires were so close that his desk and theirs were practically touching. It was \u201cthe inner sanctum,\u201d says a former leader in the AI org (the branch of Facebook that contains all its AI teams), who recalls the CEO shuffling people in and out of his vicinity as they gained or lost his favor. \u201cThat\u2019s how you know what\u2019s on his mind,\u201d says Qui\u00f1onero. \u201cI was always, for a couple of years, a few steps from Mark's desk.\u201d With new machine-learning models coming online daily, the company created a new system to track their impact and maximize user engagement. The process is still the same today. Teams train up a new machine-learning model on FBLearner, whether to change the ranking order of posts or to better catch content that violates Facebook\u2019s community standards (its rules on what is and isn\u2019t allowed on the platform). Then they test the new model on a small subset of Facebook\u2019s users to measure how it changes engagement metrics, such as the number of likes, comments, and shares, says Krishna Gade, who served as the engineering manager for news feed from 2016 to 2018. If a model reduces engagement too much, it\u2019s discarded. Otherwise, it\u2019s deployed and continually monitored. On Twitter, Gade explained that his engineers would get notifications every few days when metrics such as likes or comments were down. Then they\u2019d decipher what had caused the problem and whether any models needed retraining. But this approach soon caused issues. The models that maximize engagement also favor controversy, misinformation, and extremism: put simply, people just like outrageous stuff. Sometimes this inflames existing political tensions. The most devastating example to date is the case of Myanmar, where viral fake news and hate speech about the Rohingya Muslim minority escalated the country\u2019s religious conflict into a full-blown genocide. Facebook admitted in 2018 , after years of downplaying its role, that it had not done enough \u201cto help prevent our platform from being used to foment division and incite offline violence.\u201d While Facebook may have been oblivious to these consequences in the beginning, it was studying them by 2016. In an internal presentation from that year, reviewed by the Wall Street Journal , a company researcher, Monica Lee, found that Facebook was not only hosting a large number of extremist groups but also promoting them to its users: \u201c64% of all extremist group joins are due to our recommendation tools,\u201d the presentation said, predominantly thanks to the models behind the \u201cGroups You Should Join\u201d and \u201cDiscover\u201d features. \u201cThe question for leadership was: Should we be optimizing for engagement if you find that somebody is in a vulnerable state of mind?\u201d A former AI researcher who joined in 2018 In 2017, Chris Cox, Facebook\u2019s longtime chief product officer, formed a new task force to understand whether maximizing user engagement on Facebook was contributing to political polarization. It found that there was indeed a correlation, and that reducing polarization would mean taking a hit on engagement. In a mid-2018 document reviewed by the Journal, the task force proposed several potential fixes, such as tweaking the recommendation algorithms to suggest a more diverse range of groups for people to join. But it acknowledged that some of the ideas were \u201cantigrowth.\u201d Most of the proposals didn\u2019t move forward, and the task force disbanded. Since then, other employees have corroborated these findings. A former Facebook AI researcher who joined in 2018 says he and his team conducted \u201cstudy after study\u201d confirming the same basic idea: models that maximize engagement increase polarization. They could easily track how strongly users agreed or disagreed on different issues, what content they liked to engage with, and how their stances changed as a result. Regardless of the issue, the models learned to feed users increasingly extreme viewpoints. \u201cOver time they measurably become more polarized,\u201d he says. The researcher\u2019s team also found that users with a tendency to post or engage with melancholy content\u2014a possible sign of depression\u2014could easily spiral into consuming increasingly negative material that risked further worsening their mental health. The team proposed tweaking the content-ranking models for these users to stop maximizing engagement alone, so they would be shown less of the depressing stuff. \u201cThe question for leadership was: Should we be optimizing for engagement if you find that somebody is in a vulnerable state of mind?\u201d he remembers. (A Facebook spokesperson said she could not find documentation for this proposal.) But anything that reduced engagement, even for reasons such as not exacerbating someone\u2019s depression, led to a lot of hemming and hawing among leadership. With their performance reviews and salaries tied to the successful completion of projects, employees quickly learned to drop those that received pushback and continue working on those dictated from the top down. One such project heavily pushed by company leaders involved predicting whether a user might be at risk for something several people had already done: livestreaming their own suicide on Facebook Live. The task involved building a model to analyze the comments that other users were posting on a video after it had gone live, and bringing at-risk users to the attention of trained Facebook community reviewers who could call local emergency responders to perform a wellness check. It didn\u2019t require any changes to content-ranking models, had negligible impact on engagement, and effectively fended off negative press. It was also nearly impossible, says the researcher: \u201cIt\u2019s more of a PR stunt. The efficacy of trying to determine if somebody is going to kill themselves in the next 30 seconds, based on the first 10 seconds of video analysis\u2014you\u2019re not going to be very effective.\u201d Facebook disputes this characterization, saying the team that worked on this effort has since successfully predicted which users were at risk and increased the number of wellness checks performed. But the company does not release data on the accuracy of its predictions or how many wellness checks turned out to be real emergencies. That former employee, meanwhile, no longer lets his daughter use Facebook. Qui\u00f1onero should have been perfectly placed to tackle these problems when he created the SAIL (later Responsible AI) team in April 2018. His time as the director of Applied Machine Learning had made him intimately familiar with the company\u2019s algorithms, especially the ones used for recommending posts, ads, and other content to users. It also seemed that Facebook was ready to take these problems seriously. Whereas previous efforts to work on them had been scattered across the company, Qui\u00f1onero was now being granted a centralized team with leeway in his mandate to work on whatever he saw fit at the intersection of AI and society. At the time, Qui\u00f1onero was engaging in his own reeducation about how to be a responsible technologist. The field of AI research was paying growing attention to problems of AI bias and accountability in the wake of high-profile studies showing that, for example, an algorithm was scoring Black defendants as more likely to be rearrested than white defendants who\u2019d been arrested for the same or a more serious offense. Qui\u00f1onero began studying the scientific literature on algorithmic fairness, reading books on ethical engineering and the history of technology, and speaking with civil rights experts and moral philosophers. Over the many hours I spent with him, I could tell he took this seriously. He had joined Facebook amid the Arab Spring, a series of revolutions against oppressive Middle Eastern regimes. Experts had lauded social media for spreading the information that fueled the uprisings and giving people tools to organize. Born in Spain but raised in Morocco, where he\u2019d seen the suppression of free speech firsthand, Qui\u00f1onero felt an intense connection to Facebook\u2019s potential as a force for good. Six years later, Cambridge Analytica had threatened to overturn this promise. The controversy forced him to confront his faith in the company and examine what staying would mean for his integrity. \u201cI think what happens to most people who work at Facebook\u2014and definitely has been my story\u2014is that there's no boundary between Facebook and me,\u201d he says. \u201cIt's extremely personal.\u201d But he chose to stay, and to head SAIL, because he believed he could do more for the world by helping turn the company around than by leaving it behind. \u201cI think if you're at a company like Facebook, especially over the last few years, you really realize the impact that your products have on people's lives\u2014on what they think, how they communicate, how they interact with each other,\u201d says Qui\u00f1onero\u2019s longtime friend Zoubin Ghahramani, who helps lead the Google Brain team. \u201cI know Joaquin cares deeply about all aspects of this. As somebody who strives to achieve better and improve things, he sees the important role that he can have in shaping both the thinking and the policies around responsible AI.\u201d At first, SAIL had only five people, who came from different parts of the company but were all interested in the societal impact of algorithms. One founding member, Isabel Kloumann, a research scientist who\u2019d come from the company\u2019s core data science team, brought with her an initial version of a tool to measure the bias in AI models. The team also brainstormed many other ideas for projects. The former leader in the AI org, who was present for some of the early meetings of SAIL, recalls one proposal for combating polarization. It involved using sentiment analysis, a form of machine learning that interprets opinion in bits of text, to better identify comments that expressed extreme points of view. These comments wouldn\u2019t be deleted, but they would be hidden by default with an option to reveal them, thus limiting the number of people who saw them. And there were discussions about what role SAIL could play within Facebook and how it should evolve over time. The sentiment was that the team would first produce responsible-AI guidelines to tell the product teams what they should or should not do. But the hope was that it would ultimately serve as the company\u2019s central hub for evaluating AI projects and stopping those that didn\u2019t follow the guidelines. Former employees described, however, how hard it could be to get buy-in or financial support when the work didn\u2019t directly improve Facebook\u2019s growth. By its nature, the team was not thinking about growth, and in some cases it was proposing ideas antithetical to growth. As a result, it received few resources and languished. Many of its ideas stayed largely academic. On August 29, 2018, that suddenly changed. In the ramp-up to the US midterm elections, President Donald Trump and other Republican leaders ratcheted up accusations that Facebook, Twitter, and Google had anti-conservative bias. They claimed that Facebook\u2019s moderators in particular, in applying the community standards, were suppressing conservative voices more than liberal ones. This charge would later be debunked , but the hashtag #StopTheBias , fueled by a Trump tweet, was rapidly spreading on social media. For Trump, it was the latest effort to sow distrust in the country\u2019s mainstream information distribution channels. For Zuckerberg, it threatened to alienate Facebook\u2019s conservative US users and make the company more vulnerable to regulation from a Republican-led government. In other words, it threatened the company\u2019s growth. Facebook did not grant me an interview with Zuckerberg, but previous reporting has shown how he increasingly pandered to Trump and the Republican leadership. After Trump was elected, Joel Kaplan, Facebook\u2019s VP of global public policy and its highest-ranking Republican, advised Zuckerberg to tread carefully in the new political environment. On September 20, 2018, three weeks after Trump\u2019s #StopTheBias tweet, Zuckerberg held a meeting with Qui\u00f1onero for the first time since SAIL\u2019s creation. He wanted to know everything Qui\u00f1onero had learned about AI bias and how to quash it in Facebook\u2019s content-moderation models. By the end of the meeting, one thing was clear: AI bias was now Qui\u00f1onero\u2019s top priority. \u201cThe leadership has been very, very pushy about making sure we scale this aggressively,\u201d says Rachad Alao, the engineering director of Responsible AI who joined in April 2019. It was a win for everybody in the room. Zuckerberg got a way to ward off charges of anti-conservative bias. And Qui\u00f1onero now had more money and a bigger team to make the overall Facebook experience better for users. They could build upon Kloumann\u2019s existing tool in order to measure and correct the alleged anti-conservative bias in content-moderation models, as well as to correct other types of bias in the vast majority of models across the platform. This could help prevent the platform from unintentionally discriminating against certain users. By then, Facebook already had thousands of models running concurrently, and almost none had been measured for bias. That would get it into legal trouble a few months later with the US Department of Housing and Urban Development (HUD), which alleged that the company\u2019s algorithms were inferring \u201cprotected\u201d attributes like race from users\u2019 data and showing them ads for housing based on those attributes\u2014an illegal form of discrimination. (The lawsuit is still pending.) Schroepfer also predicted that Congress would soon pass laws to regulate algorithmic discrimination , so Facebook needed to make headway on these efforts anyway. (Facebook disputes the idea that it pursued its work on AI bias to protect growth or in anticipation of regulation. \u201cWe built the Responsible AI team because it was the right thing to do,\u201d a spokesperson said.) But narrowing SAIL\u2019s focus to algorithmic fairness would sideline all Facebook\u2019s other long-standing algorithmic problems. Its content-recommendation models would continue pushing posts, news, and groups to users in an effort to maximize engagement, rewarding extremist content and contributing to increasingly fractured political discourse. Zuckerberg even admitted this. Two months after the meeting with Qui\u00f1onero, in a public note outlining Facebook\u2019s plans for content moderation, he illustrated the harmful effects of the company\u2019s engagement strategy with a simplified chart. It showed that the more likely a post is to violate Facebook\u2019s community standards, the more user engagement it receives, because the algorithms that maximize engagement reward inflammatory content. A chart titled \"natural engagement pattern\" that shows allowed content on the X axis, engagement on the Y axis, and an exponential increase in engagement as content nears the policy line for prohibited content. But then he showed another chart with the inverse relationship. Rather than rewarding content that came close to violating the community standards, Zuckerberg wrote, Facebook could choose to start \u201cpenalizing\u201d it, giving it \u201cless distribution and engagement\u201d rather than more. How would this be done? With more AI. Facebook would develop better content-moderation models to detect this \u201cborderline content\u201d so it could be retroactively pushed lower in the news feed to snuff out its virality, he said. A chart titled \"adjusted to discourage borderline content\" that shows the same chart but the curve inverted to reach no engagement when it reaches the policy line. The problem is that for all Zuckerberg\u2019s promises, this strategy is tenuous at best. Misinformation and hate speech constantly evolve. New falsehoods spring up; new people and groups become targets. To catch things before they go viral, content-moderation models must be able to identify new unwanted content with high accuracy. But machine-learning models do not work that way. An algorithm that has learned to recognize Holocaust denial can\u2019t immediately spot, say, Rohingya genocide denial. It must be trained on thousands, often even millions, of examples of a new type of content before learning to filter it out. Even then, users can quickly learn to outwit the model by doing things like changing the wording of a post or replacing incendiary phrases with euphemisms, making their message illegible to the AI while still obvious to a human. This is why new conspiracy theories can rapidly spiral out of control, and partly why, even after such content is banned, forms of it can persist on the platform. In his New York Times profile, Schroepfer named these limitations of the company\u2019s content-moderation strategy. \u201cEvery time Mr. Schroepfer and his more than 150 engineering specialists create A.I. solutions that flag and squelch noxious material, new and dubious posts that the A.I. systems have never seen before pop up\u2014and are thus not caught,\u201d wrote the Times. \u201cIt\u2019s never going to go to zero,\u201d Schroepfer told the publication. Meanwhile, the algorithms that recommend this content still work to maximize engagement. This means every toxic post that escapes the content-moderation filters will continue to be pushed higher up the news feed and promoted to reach a larger audience. Indeed, a study from New York University recently found that among partisan publishers\u2019 Facebook pages, those that regularly posted political misinformation received the most engagement in the lead-up to the 2020 US presidential election and the Capitol riots. \u201cThat just kind of got me,\u201d says a former employee who worked on integrity issues from 2018 to 2019. \u201cWe fully acknowledged [this], and yet we\u2019re still increasing engagement.\u201d But Qui\u00f1onero\u2019s SAIL team wasn\u2019t working on this problem. Because of Kaplan\u2019s and Zuckerberg\u2019s worries about alienating conservatives, the team stayed focused on bias. And even after it merged into the bigger Responsible AI team, it was never mandated to work on content-recommendation systems that might limit the spread of misinformation. Nor has any other team, as I confirmed after Entin and another spokesperson gave me a full list of all Facebook\u2019s other initiatives on integrity issues\u2014the company\u2019s umbrella term for problems including misinformation, hate speech, and polarization. A Facebook spokesperson said, \u201cThe work isn\u2019t done by one specific team because that\u2019s not how the company operates.\u201d It is instead distributed among the teams that have the specific expertise to tackle how content ranking affects misinformation for their part of the platform, she said. But Schroepfer told me precisely the opposite in an earlier interview. I had asked him why he had created a centralized Responsible AI team instead of directing existing teams to make progress on the issue. He said it was \u201cbest practice\u201d at the company. \u201c[If] it's an important area, we need to move fast on it, it's not well-defined, [we create] a dedicated team and get the right leadership,\u201d he said. \u201cAs an area grows and matures, you'll see the product teams take on more work, but the central team is still needed because you need to stay up with state-of-the-art work.\u201d When I described the Responsible AI team\u2019s work to other experts on AI ethics and human rights, they noted the incongruity between the problems it was tackling and those, like misinformation, for which Facebook is most notorious. \u201cThis seems to be so oddly removed from Facebook as a product\u2014the things Facebook builds and the questions about impact on the world that Facebook faces,\u201d said Rumman Chowdhury, whose startup, Parity , advises firms on the responsible use of AI, and was acquired by Twitter after our interview. I had shown Chowdhury the Qui\u00f1onero team\u2019s documentation detailing its work. \u201cI find it surprising that we\u2019re going to talk about inclusivity, fairness, equity, and not talk about the very real issues happening today,\u201d she said. \u201cIt seems like the \u2018responsible AI\u2019 framing is completely subjective to what a company decides it wants to care about. It\u2019s like, \u2018We\u2019ll make up the terms and then we\u2019ll follow them,\u2019\u201d says Ellery Roberts Biddle, the editorial director of Ranking Digital Rights, a nonprofit that studies the impact of tech companies on human rights. \u201cI don\u2019t even understand what they mean when they talk about fairness. Do they think it\u2019s fair to recommend that people join extremist groups, like the ones that stormed the Capitol? If everyone gets the recommendation, does that mean it was fair?\u201d \u201cWe\u2019re at a place where there\u2019s one genocide [Myanmar] that the UN has, with a lot of evidence, been able to specifically point to Facebook and to the way that the platform promotes content,\u201d Biddle adds. \u201cHow much higher can the stakes get?\u201d Over the last two years, Qui\u00f1onero\u2019s team has built out Kloumann\u2019s original tool, called Fairness Flow. It allows engineers to measure the accuracy of machine-learning models for different user groups. They can compare a face-detection model\u2019s accuracy across different ages, genders, and skin tones, or a speech-recognition algorithm\u2019s accuracy across different languages, dialects, and accents. Fairness Flow also comes with a set of guidelines to help engineers understand what it means to train a \u201cfair\u201d model. One of the thornier problems with making algorithms fair is that there are different definitions of fairness , which can be mutually incompatible. Fairness Flow lists four definitions that engineers can use according to which suits their purpose best, such as whether a speech-recognition model recognizes all accents with equal accuracy or with a minimum threshold of accuracy. But testing algorithms for fairness is still largely optional at Facebook. None of the teams that work directly on Facebook\u2019s news feed, ad service, or other products are required to do it. Pay incentives are still tied to engagement and growth metrics. And while there are guidelines about which fairness definition to use in any given situation, they aren\u2019t enforced. This last problem came to the fore when the company had to deal with allegations of anti-conservative bias. In 2014, Kaplan was promoted from US policy head to global vice president for policy, and he began playing a more heavy-handed role in content moderation and decisions about how to rank posts in users\u2019 news feeds. After Republicans started voicing claims of anti-conservative bias in 2016, his team began manually reviewing the impact of misinformation-detection models on users to ensure\u2014among other things\u2014that they didn\u2019t disproportionately penalize conservatives. All Facebook users have some 200 \u201ctraits\u201d attached to their profile. These include various dimensions submitted by users or estimated by machine-learning models, such as race, political and religious leanings, socioeconomic class, and level of education. Kaplan\u2019s team began using the traits to assemble custom user segments that reflected largely conservative interests: users who engaged with conservative content, groups, and pages, for example. Then they\u2019d run special analyses to see how content-moderation decisions would affect posts from those segments, according to a former researcher whose work was subject to those reviews. The Fairness Flow documentation, which the Responsible AI team wrote later, includes a case study on how to use the tool in such a situation. When deciding whether a misinformation model is fair with respect to political ideology, the team wrote, \u201cfairness\u201d does not mean the model should affect conservative and liberal users equally. If conservatives are posting a greater fraction of misinformation, as judged by public consensus, then the model should flag a greater fraction of conservative content. If liberals are posting more misinformation, it should flag their content more often too. But members of Kaplan\u2019s team followed exactly the opposite approach: they took \u201cfairness\u201d to mean that these models should not affect conservatives more than liberals. When a model did so, they would stop its deployment and demand a change. Once, they blocked a medical-misinformation detector that had noticeably reduced the reach of anti-vaccine campaigns, the former researcher told me. They told the researchers that the model could not be deployed until the team fixed this discrepancy. But that effectively made the model meaningless. \u201cThere\u2019s no point, then,\u201d the researcher says. A model modified in that way \u201cwould have literally no impact on the actual problem\u201d of misinformation. \u201cI don\u2019t even understand what they mean when they talk about fairness. Do they think it\u2019s fair to recommend that people join extremist groups, like the ones that stormed the Capitol? If everyone gets the recommendation, does that mean it was fair?\u201d Ellery Roberts Biddle, editorial director of Ranking Digital Rights This happened countless other times\u2014and not just for content moderation. In 2020, the Washington Post reported that Kaplan\u2019s team had undermined efforts to mitigate election interference and polarization within Facebook, saying they could contribute to anti-conservative bias. In 2018, it used the same argument to shelve a project to edit Facebook\u2019s recommendation models even though researchers believed it would reduce divisiveness on the platform, according to the Wall Street Journal . His claims about political bias also weakened a proposal to edit the ranking models for the news feed that Facebook\u2019s data scientists believed would strengthen the platform against the manipulation tactics Russia had used during the 2016 US election. And ahead of the 2020 election, Facebook policy executives used this excuse, according to the New York Times , to veto or weaken several proposals that would have reduced the spread of hateful and damaging content. Facebook disputed the Wall Street Journal\u2019s reporting in a follow-up blog post, and challenged the New York Times\u2019s characterization in an interview with the publication. A spokesperson for Kaplan\u2019s team also denied to me that this was a pattern of behavior, saying the cases reported by the Post, the Journal, and the Times were \u201call individual instances that we believe are then mischaracterized.\u201d He declined to comment about the retraining of misinformation models on the record. Many of these incidents happened before Fairness Flow was adopted. But they show how Facebook\u2019s pursuit of fairness in the service of growth had already come at a steep cost to progress on the platform\u2019s other challenges. And if engineers used the definition of fairness that Kaplan\u2019s team had adopted, Fairness Flow could simply systematize behavior that rewarded misinformation instead of helping to combat it. Often \u201cthe whole fairness thing\u201d came into play only as a convenient way to maintain the status quo, the former researcher says: \u201cIt seems to fly in the face of the things that Mark was saying publicly in terms of being fair and equitable.\u201d The last time I spoke with Qui\u00f1onero was a month after the US Capitol riots. I wanted to know how the storming of Congress had affected his thinking and the direction of his work. In the video call, it was as it always was: Qui\u00f1onero dialing in from his home office in one window and Entin, his PR handler, in another. I asked Qui\u00f1onero what role he felt Facebook had played in the riots and whether it changed the task he saw for Responsible AI. After a long pause, he sidestepped the question, launching into a description of recent work he\u2019d done to promote greater diversity and inclusion among the AI teams. I asked him the question again. His Facebook Portal camera, which uses computer-vision algorithms to track the speaker, began to slowly zoom in on his face as he grew still. \u201cI don\u2019t know that I have an easy answer to that question, Karen,\u201d he said. \u201cIt\u2019s an extremely difficult question to ask me.\u201d Entin, who\u2019d been rapidly pacing with a stoic poker face, grabbed a red stress ball. I asked Qui\u00f1onero why his team hadn\u2019t previously looked at ways to edit Facebook\u2019s content-ranking models to tamp down misinformation and extremism. He told me it was the job of other teams (though none, as I confirmed, have been mandated to work on that task). \u201cIt\u2019s not feasible for the Responsible AI team to study all those things ourselves,\u201d he said. When I asked whether he would consider having his team tackle those issues in the future, he vaguely admitted, \u201cI would agree with you that that is going to be the scope of these types of conversations.\u201d Near the end of our hour-long interview, he began to emphasize that AI was often unfairly painted as \u201cthe culprit.\u201d Regardless of whether Facebook used AI or not, he said, people would still spew lies and hate speech, and that content would still spread across the platform. I pressed him one more time. Certainly he couldn\u2019t believe that algorithms had done absolutely nothing to change the nature of these issues, I said. \u201cI don\u2019t know,\u201d he said with a halting stutter. Then he repeated, with more conviction: \u201cThat\u2019s my honest answer. Honest to God. I don\u2019t know.\u201d","title":"How Facebook got addicted to spreading misinformation"},{"location":"EL5131/Technology/Big%20Tech/Articles/article1/#how-facebook-got-addicted-to-spreading-misinformation","text":"The company\u2019s AI algorithms gave it an insatiable habit for lies and hate speech. Now the man who built them can't fix the problem. MIT Technology Review, Karen Hao, March 11 2021 Find the original article here: https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/ Joaquin Qui\u00f1onero Candela , a director of AI at Facebook, was apologizing to his audience. It was March 23, 2018, just days after the revelation that Cambridge Analytica, a consultancy that worked on Donald Trump\u2019s 2016 presidential election campaign, had surreptitiously siphoned the personal data of tens of millions of Americans from their Facebook accounts in an attempt to influence how they voted. It was the biggest privacy breach in Facebook\u2019s history, and Qui\u00f1onero had been previously scheduled to speak at a conference on, among other things, \u201cthe intersection of AI, ethics, and privacy\u201d at the company. He considered canceling, but after debating it with his communications director, he\u2019d kept his allotted time. As he stepped up to face the room, he began with an admission. \u201cI\u2019ve just had the hardest five days in my tenure at Facebook,\u201d he remembers saying. \u201cIf there\u2019s criticism, I\u2019ll accept it.\u201d The Cambridge Analytica scandal would kick off Facebook\u2019s largest publicity crisis ever. It compounded fears that the algorithms that determine what people see on the platform were amplifying fake news and hate speech, and that Russian hackers had weaponized them to try to sway the election in Trump\u2019s favor. Millions began deleting the app ; employees left in protest; the company\u2019s market capitalization plunged by more than $100 billion after its July earnings call. In the ensuing months, Mark Zuckerberg began his own apologizing. He apologized for not taking \u201c a broad enough view \u201d of Facebook\u2019s responsibilities, and for his mistakes as a CEO. Internally, Sheryl Sandberg, the chief operating officer, kicked off a two-year civil rights audit to recommend ways the company could prevent the use of its platform to undermine democracy. Finally, Mike Schroepfer, Facebook\u2019s chief technology officer, asked Qui\u00f1onero to start a team with a directive that was a little vague: to examine the societal impact of the company\u2019s algorithms. The group named itself the Society and AI Lab (SAIL); last year it combined with another team working on issues of data privacy to form Responsible AI. Qui\u00f1onero was a natural pick for the job. He, as much as anybody, was the one responsible for Facebook\u2019s position as an AI powerhouse. In his six years at Facebook, he\u2019d created some of the first algorithms for targeting users with content precisely tailored to their interests, and then he\u2019d diffused those algorithms across the company. Now his mandate would be to make them less harmful. Facebook has consistently pointed to the efforts by Qui\u00f1onero and others as it seeks to repair its reputation. It regularly trots out various leaders to speak to the media about the ongoing reforms. In May of 2019, it granted a series of interviews with Schroepfer to the New York Times, which rewarded the company with a humanizing profile of a sensitive, well-intentioned executive striving to overcome the technical challenges of filtering out misinformation and hate speech from a stream of content that amounted to billions of pieces a day. These challenges are so hard that it makes Schroepfer emotional, wrote the Times: \u201cSometimes that brings him to tears.\u201d In the spring of 2020, it was apparently my turn. Ari Entin, Facebook\u2019s AI communications director, asked in an email if I wanted to take a deeper look at the company\u2019s AI work. After talking to several of its AI leaders, I decided to focus on Qui\u00f1onero. Entin happily obliged. As not only the leader of the Responsible AI team but also the man who had made Facebook into an AI-driven company, Qui\u00f1onero was a solid choice to use as a poster boy. He seemed a natural choice of subject to me, too. In the years since he\u2019d formed his team following the Cambridge Analytica scandal, concerns about the spread of lies and hate speech on Facebook had only grown. In late 2018 the company admitted that this activity had helped fuel a genocidal anti-Muslim campaign in Myanmar for several years. In 2020 Facebook started belatedly taking action against Holocaust deniers, anti-vaxxers, and the conspiracy movement QAnon. All these dangerous falsehoods were metastasizing thanks to the AI capabilities Qui\u00f1onero had helped build. The algorithms that underpin Facebook\u2019s business weren\u2019t created to filter out what was false or inflammatory; they were designed to make people share and engage with as much content as possible by showing them things they were most likely to be outraged or titillated by. Fixing this problem, to me, seemed like core Responsible AI territory. I began video-calling Qui\u00f1onero regularly. I also spoke to Facebook executives, current and former employees, industry peers, and external experts. Many spoke on condition of anonymity because they\u2019d signed nondisclosure agreements or feared retaliation. I wanted to know: What was Qui\u00f1onero\u2019s team doing to rein in the hate and lies on its platform? Joaquin Qui\u00f1onero Candela outside his home in the Bay Area, where he lives with his wife and three kids. But Entin and Qui\u00f1onero had a different agenda. Each time I tried to bring up these topics, my requests to speak about them were dropped or redirected. They only wanted to discuss the Responsible AI team\u2019s plan to tackle one specific kind of problem: AI bias, in which algorithms discriminate against particular user groups. An example would be an ad-targeting algorithm that shows certain job or housing opportunities to white people but not to minorities. By the time thousands of rioters stormed the US Capitol in January, organized in part on Facebook and fueled by the lies about a stolen election that had fanned out across the platform, it was clear from my conversations that the Responsible AI team had failed to make headway against misinformation and hate speech because it had never made those problems its main focus. More important, I realized, if it tried to, it would be set up for failure. The reason is simple. Everything the company does and chooses not to do flows from a single motivation: Zuckerberg\u2019s relentless desire for growth. Qui\u00f1onero\u2019s AI expertise supercharged that growth. His team got pigeonholed into targeting AI bias, as I learned in my reporting, because preventing such bias helps the company avoid proposed regulation that might, if passed, hamper that growth. Facebook leadership has also repeatedly weakened or halted many initiatives meant to clean up misinformation on the platform because doing so would undermine that growth. In other words, the Responsible AI team\u2019s work\u2014whatever its merits on the specific problem of tackling AI bias\u2014is essentially irrelevant to fixing the bigger problems of misinformation, extremism, and political polarization. And it\u2019s all of us who pay the price. \u201cWhen you\u2019re in the business of maximizing engagement, you\u2019re not interested in truth. You\u2019re not interested in harm, divisiveness, conspiracy. In fact, those are your friends,\u201d says Hany Farid, a professor at the University of California, Berkeley who collaborates with Facebook to understand image- and video-based misinformation on the platform. \u201cThey always do just enough to be able to put the press release out. But with a few exceptions, I don\u2019t think it\u2019s actually translated into better policies. They\u2019re never really dealing with the fundamental problems.\u201d In March of 2012, Qui\u00f1onero visited a friend in the Bay Area. At the time, he was a manager in Microsoft Research\u2019s UK office, leading a team using machine learning to get more visitors to click on ads displayed by the company\u2019s search engine, Bing. His expertise was rare, and the team was less than a year old. Machine learning, a subset of AI, had yet to prove itself as a solution to large-scale industry problems. Few tech giants had invested in the technology. Qui\u00f1onero\u2019s friend wanted to show off his new employer, one of the hottest startups in Silicon Valley: Facebook, then eight years old and already with close to a billion monthly active users (i.e., those who have logged in at least once in the past 30 days). As Qui\u00f1onero walked around its Menlo Park headquarters, he watched a lone engineer make a major update to the website, something that would have involved significant red tape at Microsoft. It was a memorable introduction to Zuckerberg\u2019s \u201cMove fast and break things\u201d ethos. Qui\u00f1onero was awestruck by the possibilities. Within a week, he had been through interviews and signed an offer to join the company. His arrival couldn\u2019t have been better timed. Facebook\u2019s ads service was in the middle of a rapid expansion as the company was preparing for its May IPO. The goal was to increase revenue and take on Google, which had the lion\u2019s share of the online advertising market. Machine learning, which could predict which ads would resonate best with which users and thus make them more effective, could be the perfect tool. Shortly after starting, Qui\u00f1onero was promoted to managing a team similar to the one he\u2019d led at Microsoft. Qui\u00f1onero started raising chickens in late 2019 as a way to unwind from the intensity of his job. Unlike traditional algorithms, which are hard-coded by engineers, machine-learning algorithms \u201ctrain\u201d on input data to learn the correlations within it. The trained algorithm, known as a machine-learning model, can then automate future decisions. An algorithm trained on ad click data, for example, might learn that women click on ads for yoga leggings more often than men. The resultant model will then serve more of those ads to women. Today at an AI-based company like Facebook, engineers generate countless models with slight variations to see which one performs best on a given problem.< Facebook\u2019s massive amounts of user data gave Qui\u00f1onero a big advantage. His team could develop models that learned to infer the existence not only of broad categories like \u201cwomen\u201d and \u201cmen,\u201d but of very fine-grained categories like \u201cwomen between 25 and 34 who liked Facebook pages related to yoga,\u201d and targeted ads to them. The finer-grained the targeting, the better the chance of a click, which would give advertisers more bang for their buck. Within a year his team had developed these models, as well as the tools for designing and deploying new ones faster. Before, it had taken Qui\u00f1onero\u2019s engineers six to eight weeks to build, train, and test a new model. Now it took only one. News of the success spread quickly. The team that worked on determining which posts individual Facebook users would see on their personal news feeds wanted to apply the same techniques. Just as algorithms could be trained to predict who would click what ad, they could also be trained to predict who would like or share what post, and then give those posts more prominence. If the model determined that a person really liked dogs, for instance, friends\u2019 posts about dogs would appear higher up on that user\u2019s news feed. Qui\u00f1onero\u2019s success with the news feed\u2014coupled with impressive new AI research being conducted outside the company\u2014caught the attention of Zuckerberg and Schroepfer. Facebook now had just over 1 billion users, making it more than eight times larger than any other social network, but they wanted to know how to continue that growth. The executives decided to invest heavily in AI, internet connectivity, and virtual reality. They created two AI teams. One was FAIR, a fundamental research lab that would advance the technology\u2019s state-of-the-art capabilities. The other, Applied Machine Learning (AML), would integrate those capabilities into Facebook\u2019s products and services. In December 2013, after months of courting and persuasion, the executives recruited Yann LeCun, one of the biggest names in the field, to lead FAIR. Three months later, Qui\u00f1onero was promoted again, this time to lead AML. (It was later renamed FAIAR, pronounced \u201cfire.\u201d) \u201cThat\u2019s how you know what\u2019s on his mind. I was always, for a couple of years, a few steps from Mark's desk.\u201d Joaquin Qui\u00f1onero Candela In his new role, Qui\u00f1onero built a new model-development platform for anyone at Facebook to access. Called FBLearner Flow , it allowed engineers with little AI experience to train and deploy machine-learning models within days. By mid-2016, it was in use by more than a quarter of Facebook\u2019s engineering team and had already been used to train over a million models, including models for image recognition, ad targeting, and content moderation. Zuckerberg\u2019s obsession with getting the whole world to use Facebook had found a powerful new weapon. Teams had previously used design tactics, like experimenting with the content and frequency of notifications, to try to hook users more effectively. Their goal, among other things, was to increase a metric called L6/7, the fraction of people who logged in to Facebook six of the previous seven days. L6/7 is just one of myriad ways in which Facebook has measured \u201cengagement\u201d\u2014the propensity of people to use its platform in any way, whether it\u2019s by posting things, commenting on them, liking or sharing them, or just looking at them. Now every user interaction once analyzed by engineers was being analyzed by algorithms. Those algorithms were creating much faster, more personalized feedback loops for tweaking and tailoring each user\u2019s news feed to keep nudging up engagement numbers. Zuckerberg, who sat in the center of Building 20, the main office at the Menlo Park headquarters, placed the new FAIR and AML teams beside him. Many of the original AI hires were so close that his desk and theirs were practically touching. It was \u201cthe inner sanctum,\u201d says a former leader in the AI org (the branch of Facebook that contains all its AI teams), who recalls the CEO shuffling people in and out of his vicinity as they gained or lost his favor. \u201cThat\u2019s how you know what\u2019s on his mind,\u201d says Qui\u00f1onero. \u201cI was always, for a couple of years, a few steps from Mark's desk.\u201d With new machine-learning models coming online daily, the company created a new system to track their impact and maximize user engagement. The process is still the same today. Teams train up a new machine-learning model on FBLearner, whether to change the ranking order of posts or to better catch content that violates Facebook\u2019s community standards (its rules on what is and isn\u2019t allowed on the platform). Then they test the new model on a small subset of Facebook\u2019s users to measure how it changes engagement metrics, such as the number of likes, comments, and shares, says Krishna Gade, who served as the engineering manager for news feed from 2016 to 2018. If a model reduces engagement too much, it\u2019s discarded. Otherwise, it\u2019s deployed and continually monitored. On Twitter, Gade explained that his engineers would get notifications every few days when metrics such as likes or comments were down. Then they\u2019d decipher what had caused the problem and whether any models needed retraining. But this approach soon caused issues. The models that maximize engagement also favor controversy, misinformation, and extremism: put simply, people just like outrageous stuff. Sometimes this inflames existing political tensions. The most devastating example to date is the case of Myanmar, where viral fake news and hate speech about the Rohingya Muslim minority escalated the country\u2019s religious conflict into a full-blown genocide. Facebook admitted in 2018 , after years of downplaying its role, that it had not done enough \u201cto help prevent our platform from being used to foment division and incite offline violence.\u201d While Facebook may have been oblivious to these consequences in the beginning, it was studying them by 2016. In an internal presentation from that year, reviewed by the Wall Street Journal , a company researcher, Monica Lee, found that Facebook was not only hosting a large number of extremist groups but also promoting them to its users: \u201c64% of all extremist group joins are due to our recommendation tools,\u201d the presentation said, predominantly thanks to the models behind the \u201cGroups You Should Join\u201d and \u201cDiscover\u201d features. \u201cThe question for leadership was: Should we be optimizing for engagement if you find that somebody is in a vulnerable state of mind?\u201d A former AI researcher who joined in 2018 In 2017, Chris Cox, Facebook\u2019s longtime chief product officer, formed a new task force to understand whether maximizing user engagement on Facebook was contributing to political polarization. It found that there was indeed a correlation, and that reducing polarization would mean taking a hit on engagement. In a mid-2018 document reviewed by the Journal, the task force proposed several potential fixes, such as tweaking the recommendation algorithms to suggest a more diverse range of groups for people to join. But it acknowledged that some of the ideas were \u201cantigrowth.\u201d Most of the proposals didn\u2019t move forward, and the task force disbanded. Since then, other employees have corroborated these findings. A former Facebook AI researcher who joined in 2018 says he and his team conducted \u201cstudy after study\u201d confirming the same basic idea: models that maximize engagement increase polarization. They could easily track how strongly users agreed or disagreed on different issues, what content they liked to engage with, and how their stances changed as a result. Regardless of the issue, the models learned to feed users increasingly extreme viewpoints. \u201cOver time they measurably become more polarized,\u201d he says. The researcher\u2019s team also found that users with a tendency to post or engage with melancholy content\u2014a possible sign of depression\u2014could easily spiral into consuming increasingly negative material that risked further worsening their mental health. The team proposed tweaking the content-ranking models for these users to stop maximizing engagement alone, so they would be shown less of the depressing stuff. \u201cThe question for leadership was: Should we be optimizing for engagement if you find that somebody is in a vulnerable state of mind?\u201d he remembers. (A Facebook spokesperson said she could not find documentation for this proposal.) But anything that reduced engagement, even for reasons such as not exacerbating someone\u2019s depression, led to a lot of hemming and hawing among leadership. With their performance reviews and salaries tied to the successful completion of projects, employees quickly learned to drop those that received pushback and continue working on those dictated from the top down. One such project heavily pushed by company leaders involved predicting whether a user might be at risk for something several people had already done: livestreaming their own suicide on Facebook Live. The task involved building a model to analyze the comments that other users were posting on a video after it had gone live, and bringing at-risk users to the attention of trained Facebook community reviewers who could call local emergency responders to perform a wellness check. It didn\u2019t require any changes to content-ranking models, had negligible impact on engagement, and effectively fended off negative press. It was also nearly impossible, says the researcher: \u201cIt\u2019s more of a PR stunt. The efficacy of trying to determine if somebody is going to kill themselves in the next 30 seconds, based on the first 10 seconds of video analysis\u2014you\u2019re not going to be very effective.\u201d Facebook disputes this characterization, saying the team that worked on this effort has since successfully predicted which users were at risk and increased the number of wellness checks performed. But the company does not release data on the accuracy of its predictions or how many wellness checks turned out to be real emergencies. That former employee, meanwhile, no longer lets his daughter use Facebook. Qui\u00f1onero should have been perfectly placed to tackle these problems when he created the SAIL (later Responsible AI) team in April 2018. His time as the director of Applied Machine Learning had made him intimately familiar with the company\u2019s algorithms, especially the ones used for recommending posts, ads, and other content to users. It also seemed that Facebook was ready to take these problems seriously. Whereas previous efforts to work on them had been scattered across the company, Qui\u00f1onero was now being granted a centralized team with leeway in his mandate to work on whatever he saw fit at the intersection of AI and society. At the time, Qui\u00f1onero was engaging in his own reeducation about how to be a responsible technologist. The field of AI research was paying growing attention to problems of AI bias and accountability in the wake of high-profile studies showing that, for example, an algorithm was scoring Black defendants as more likely to be rearrested than white defendants who\u2019d been arrested for the same or a more serious offense. Qui\u00f1onero began studying the scientific literature on algorithmic fairness, reading books on ethical engineering and the history of technology, and speaking with civil rights experts and moral philosophers. Over the many hours I spent with him, I could tell he took this seriously. He had joined Facebook amid the Arab Spring, a series of revolutions against oppressive Middle Eastern regimes. Experts had lauded social media for spreading the information that fueled the uprisings and giving people tools to organize. Born in Spain but raised in Morocco, where he\u2019d seen the suppression of free speech firsthand, Qui\u00f1onero felt an intense connection to Facebook\u2019s potential as a force for good. Six years later, Cambridge Analytica had threatened to overturn this promise. The controversy forced him to confront his faith in the company and examine what staying would mean for his integrity. \u201cI think what happens to most people who work at Facebook\u2014and definitely has been my story\u2014is that there's no boundary between Facebook and me,\u201d he says. \u201cIt's extremely personal.\u201d But he chose to stay, and to head SAIL, because he believed he could do more for the world by helping turn the company around than by leaving it behind. \u201cI think if you're at a company like Facebook, especially over the last few years, you really realize the impact that your products have on people's lives\u2014on what they think, how they communicate, how they interact with each other,\u201d says Qui\u00f1onero\u2019s longtime friend Zoubin Ghahramani, who helps lead the Google Brain team. \u201cI know Joaquin cares deeply about all aspects of this. As somebody who strives to achieve better and improve things, he sees the important role that he can have in shaping both the thinking and the policies around responsible AI.\u201d At first, SAIL had only five people, who came from different parts of the company but were all interested in the societal impact of algorithms. One founding member, Isabel Kloumann, a research scientist who\u2019d come from the company\u2019s core data science team, brought with her an initial version of a tool to measure the bias in AI models. The team also brainstormed many other ideas for projects. The former leader in the AI org, who was present for some of the early meetings of SAIL, recalls one proposal for combating polarization. It involved using sentiment analysis, a form of machine learning that interprets opinion in bits of text, to better identify comments that expressed extreme points of view. These comments wouldn\u2019t be deleted, but they would be hidden by default with an option to reveal them, thus limiting the number of people who saw them. And there were discussions about what role SAIL could play within Facebook and how it should evolve over time. The sentiment was that the team would first produce responsible-AI guidelines to tell the product teams what they should or should not do. But the hope was that it would ultimately serve as the company\u2019s central hub for evaluating AI projects and stopping those that didn\u2019t follow the guidelines. Former employees described, however, how hard it could be to get buy-in or financial support when the work didn\u2019t directly improve Facebook\u2019s growth. By its nature, the team was not thinking about growth, and in some cases it was proposing ideas antithetical to growth. As a result, it received few resources and languished. Many of its ideas stayed largely academic. On August 29, 2018, that suddenly changed. In the ramp-up to the US midterm elections, President Donald Trump and other Republican leaders ratcheted up accusations that Facebook, Twitter, and Google had anti-conservative bias. They claimed that Facebook\u2019s moderators in particular, in applying the community standards, were suppressing conservative voices more than liberal ones. This charge would later be debunked , but the hashtag #StopTheBias , fueled by a Trump tweet, was rapidly spreading on social media. For Trump, it was the latest effort to sow distrust in the country\u2019s mainstream information distribution channels. For Zuckerberg, it threatened to alienate Facebook\u2019s conservative US users and make the company more vulnerable to regulation from a Republican-led government. In other words, it threatened the company\u2019s growth. Facebook did not grant me an interview with Zuckerberg, but previous reporting has shown how he increasingly pandered to Trump and the Republican leadership. After Trump was elected, Joel Kaplan, Facebook\u2019s VP of global public policy and its highest-ranking Republican, advised Zuckerberg to tread carefully in the new political environment. On September 20, 2018, three weeks after Trump\u2019s #StopTheBias tweet, Zuckerberg held a meeting with Qui\u00f1onero for the first time since SAIL\u2019s creation. He wanted to know everything Qui\u00f1onero had learned about AI bias and how to quash it in Facebook\u2019s content-moderation models. By the end of the meeting, one thing was clear: AI bias was now Qui\u00f1onero\u2019s top priority. \u201cThe leadership has been very, very pushy about making sure we scale this aggressively,\u201d says Rachad Alao, the engineering director of Responsible AI who joined in April 2019. It was a win for everybody in the room. Zuckerberg got a way to ward off charges of anti-conservative bias. And Qui\u00f1onero now had more money and a bigger team to make the overall Facebook experience better for users. They could build upon Kloumann\u2019s existing tool in order to measure and correct the alleged anti-conservative bias in content-moderation models, as well as to correct other types of bias in the vast majority of models across the platform. This could help prevent the platform from unintentionally discriminating against certain users. By then, Facebook already had thousands of models running concurrently, and almost none had been measured for bias. That would get it into legal trouble a few months later with the US Department of Housing and Urban Development (HUD), which alleged that the company\u2019s algorithms were inferring \u201cprotected\u201d attributes like race from users\u2019 data and showing them ads for housing based on those attributes\u2014an illegal form of discrimination. (The lawsuit is still pending.) Schroepfer also predicted that Congress would soon pass laws to regulate algorithmic discrimination , so Facebook needed to make headway on these efforts anyway. (Facebook disputes the idea that it pursued its work on AI bias to protect growth or in anticipation of regulation. \u201cWe built the Responsible AI team because it was the right thing to do,\u201d a spokesperson said.) But narrowing SAIL\u2019s focus to algorithmic fairness would sideline all Facebook\u2019s other long-standing algorithmic problems. Its content-recommendation models would continue pushing posts, news, and groups to users in an effort to maximize engagement, rewarding extremist content and contributing to increasingly fractured political discourse. Zuckerberg even admitted this. Two months after the meeting with Qui\u00f1onero, in a public note outlining Facebook\u2019s plans for content moderation, he illustrated the harmful effects of the company\u2019s engagement strategy with a simplified chart. It showed that the more likely a post is to violate Facebook\u2019s community standards, the more user engagement it receives, because the algorithms that maximize engagement reward inflammatory content. A chart titled \"natural engagement pattern\" that shows allowed content on the X axis, engagement on the Y axis, and an exponential increase in engagement as content nears the policy line for prohibited content. But then he showed another chart with the inverse relationship. Rather than rewarding content that came close to violating the community standards, Zuckerberg wrote, Facebook could choose to start \u201cpenalizing\u201d it, giving it \u201cless distribution and engagement\u201d rather than more. How would this be done? With more AI. Facebook would develop better content-moderation models to detect this \u201cborderline content\u201d so it could be retroactively pushed lower in the news feed to snuff out its virality, he said. A chart titled \"adjusted to discourage borderline content\" that shows the same chart but the curve inverted to reach no engagement when it reaches the policy line. The problem is that for all Zuckerberg\u2019s promises, this strategy is tenuous at best. Misinformation and hate speech constantly evolve. New falsehoods spring up; new people and groups become targets. To catch things before they go viral, content-moderation models must be able to identify new unwanted content with high accuracy. But machine-learning models do not work that way. An algorithm that has learned to recognize Holocaust denial can\u2019t immediately spot, say, Rohingya genocide denial. It must be trained on thousands, often even millions, of examples of a new type of content before learning to filter it out. Even then, users can quickly learn to outwit the model by doing things like changing the wording of a post or replacing incendiary phrases with euphemisms, making their message illegible to the AI while still obvious to a human. This is why new conspiracy theories can rapidly spiral out of control, and partly why, even after such content is banned, forms of it can persist on the platform. In his New York Times profile, Schroepfer named these limitations of the company\u2019s content-moderation strategy. \u201cEvery time Mr. Schroepfer and his more than 150 engineering specialists create A.I. solutions that flag and squelch noxious material, new and dubious posts that the A.I. systems have never seen before pop up\u2014and are thus not caught,\u201d wrote the Times. \u201cIt\u2019s never going to go to zero,\u201d Schroepfer told the publication. Meanwhile, the algorithms that recommend this content still work to maximize engagement. This means every toxic post that escapes the content-moderation filters will continue to be pushed higher up the news feed and promoted to reach a larger audience. Indeed, a study from New York University recently found that among partisan publishers\u2019 Facebook pages, those that regularly posted political misinformation received the most engagement in the lead-up to the 2020 US presidential election and the Capitol riots. \u201cThat just kind of got me,\u201d says a former employee who worked on integrity issues from 2018 to 2019. \u201cWe fully acknowledged [this], and yet we\u2019re still increasing engagement.\u201d But Qui\u00f1onero\u2019s SAIL team wasn\u2019t working on this problem. Because of Kaplan\u2019s and Zuckerberg\u2019s worries about alienating conservatives, the team stayed focused on bias. And even after it merged into the bigger Responsible AI team, it was never mandated to work on content-recommendation systems that might limit the spread of misinformation. Nor has any other team, as I confirmed after Entin and another spokesperson gave me a full list of all Facebook\u2019s other initiatives on integrity issues\u2014the company\u2019s umbrella term for problems including misinformation, hate speech, and polarization. A Facebook spokesperson said, \u201cThe work isn\u2019t done by one specific team because that\u2019s not how the company operates.\u201d It is instead distributed among the teams that have the specific expertise to tackle how content ranking affects misinformation for their part of the platform, she said. But Schroepfer told me precisely the opposite in an earlier interview. I had asked him why he had created a centralized Responsible AI team instead of directing existing teams to make progress on the issue. He said it was \u201cbest practice\u201d at the company. \u201c[If] it's an important area, we need to move fast on it, it's not well-defined, [we create] a dedicated team and get the right leadership,\u201d he said. \u201cAs an area grows and matures, you'll see the product teams take on more work, but the central team is still needed because you need to stay up with state-of-the-art work.\u201d When I described the Responsible AI team\u2019s work to other experts on AI ethics and human rights, they noted the incongruity between the problems it was tackling and those, like misinformation, for which Facebook is most notorious. \u201cThis seems to be so oddly removed from Facebook as a product\u2014the things Facebook builds and the questions about impact on the world that Facebook faces,\u201d said Rumman Chowdhury, whose startup, Parity , advises firms on the responsible use of AI, and was acquired by Twitter after our interview. I had shown Chowdhury the Qui\u00f1onero team\u2019s documentation detailing its work. \u201cI find it surprising that we\u2019re going to talk about inclusivity, fairness, equity, and not talk about the very real issues happening today,\u201d she said. \u201cIt seems like the \u2018responsible AI\u2019 framing is completely subjective to what a company decides it wants to care about. It\u2019s like, \u2018We\u2019ll make up the terms and then we\u2019ll follow them,\u2019\u201d says Ellery Roberts Biddle, the editorial director of Ranking Digital Rights, a nonprofit that studies the impact of tech companies on human rights. \u201cI don\u2019t even understand what they mean when they talk about fairness. Do they think it\u2019s fair to recommend that people join extremist groups, like the ones that stormed the Capitol? If everyone gets the recommendation, does that mean it was fair?\u201d \u201cWe\u2019re at a place where there\u2019s one genocide [Myanmar] that the UN has, with a lot of evidence, been able to specifically point to Facebook and to the way that the platform promotes content,\u201d Biddle adds. \u201cHow much higher can the stakes get?\u201d Over the last two years, Qui\u00f1onero\u2019s team has built out Kloumann\u2019s original tool, called Fairness Flow. It allows engineers to measure the accuracy of machine-learning models for different user groups. They can compare a face-detection model\u2019s accuracy across different ages, genders, and skin tones, or a speech-recognition algorithm\u2019s accuracy across different languages, dialects, and accents. Fairness Flow also comes with a set of guidelines to help engineers understand what it means to train a \u201cfair\u201d model. One of the thornier problems with making algorithms fair is that there are different definitions of fairness , which can be mutually incompatible. Fairness Flow lists four definitions that engineers can use according to which suits their purpose best, such as whether a speech-recognition model recognizes all accents with equal accuracy or with a minimum threshold of accuracy. But testing algorithms for fairness is still largely optional at Facebook. None of the teams that work directly on Facebook\u2019s news feed, ad service, or other products are required to do it. Pay incentives are still tied to engagement and growth metrics. And while there are guidelines about which fairness definition to use in any given situation, they aren\u2019t enforced. This last problem came to the fore when the company had to deal with allegations of anti-conservative bias. In 2014, Kaplan was promoted from US policy head to global vice president for policy, and he began playing a more heavy-handed role in content moderation and decisions about how to rank posts in users\u2019 news feeds. After Republicans started voicing claims of anti-conservative bias in 2016, his team began manually reviewing the impact of misinformation-detection models on users to ensure\u2014among other things\u2014that they didn\u2019t disproportionately penalize conservatives. All Facebook users have some 200 \u201ctraits\u201d attached to their profile. These include various dimensions submitted by users or estimated by machine-learning models, such as race, political and religious leanings, socioeconomic class, and level of education. Kaplan\u2019s team began using the traits to assemble custom user segments that reflected largely conservative interests: users who engaged with conservative content, groups, and pages, for example. Then they\u2019d run special analyses to see how content-moderation decisions would affect posts from those segments, according to a former researcher whose work was subject to those reviews. The Fairness Flow documentation, which the Responsible AI team wrote later, includes a case study on how to use the tool in such a situation. When deciding whether a misinformation model is fair with respect to political ideology, the team wrote, \u201cfairness\u201d does not mean the model should affect conservative and liberal users equally. If conservatives are posting a greater fraction of misinformation, as judged by public consensus, then the model should flag a greater fraction of conservative content. If liberals are posting more misinformation, it should flag their content more often too. But members of Kaplan\u2019s team followed exactly the opposite approach: they took \u201cfairness\u201d to mean that these models should not affect conservatives more than liberals. When a model did so, they would stop its deployment and demand a change. Once, they blocked a medical-misinformation detector that had noticeably reduced the reach of anti-vaccine campaigns, the former researcher told me. They told the researchers that the model could not be deployed until the team fixed this discrepancy. But that effectively made the model meaningless. \u201cThere\u2019s no point, then,\u201d the researcher says. A model modified in that way \u201cwould have literally no impact on the actual problem\u201d of misinformation. \u201cI don\u2019t even understand what they mean when they talk about fairness. Do they think it\u2019s fair to recommend that people join extremist groups, like the ones that stormed the Capitol? If everyone gets the recommendation, does that mean it was fair?\u201d Ellery Roberts Biddle, editorial director of Ranking Digital Rights This happened countless other times\u2014and not just for content moderation. In 2020, the Washington Post reported that Kaplan\u2019s team had undermined efforts to mitigate election interference and polarization within Facebook, saying they could contribute to anti-conservative bias. In 2018, it used the same argument to shelve a project to edit Facebook\u2019s recommendation models even though researchers believed it would reduce divisiveness on the platform, according to the Wall Street Journal . His claims about political bias also weakened a proposal to edit the ranking models for the news feed that Facebook\u2019s data scientists believed would strengthen the platform against the manipulation tactics Russia had used during the 2016 US election. And ahead of the 2020 election, Facebook policy executives used this excuse, according to the New York Times , to veto or weaken several proposals that would have reduced the spread of hateful and damaging content. Facebook disputed the Wall Street Journal\u2019s reporting in a follow-up blog post, and challenged the New York Times\u2019s characterization in an interview with the publication. A spokesperson for Kaplan\u2019s team also denied to me that this was a pattern of behavior, saying the cases reported by the Post, the Journal, and the Times were \u201call individual instances that we believe are then mischaracterized.\u201d He declined to comment about the retraining of misinformation models on the record. Many of these incidents happened before Fairness Flow was adopted. But they show how Facebook\u2019s pursuit of fairness in the service of growth had already come at a steep cost to progress on the platform\u2019s other challenges. And if engineers used the definition of fairness that Kaplan\u2019s team had adopted, Fairness Flow could simply systematize behavior that rewarded misinformation instead of helping to combat it. Often \u201cthe whole fairness thing\u201d came into play only as a convenient way to maintain the status quo, the former researcher says: \u201cIt seems to fly in the face of the things that Mark was saying publicly in terms of being fair and equitable.\u201d The last time I spoke with Qui\u00f1onero was a month after the US Capitol riots. I wanted to know how the storming of Congress had affected his thinking and the direction of his work. In the video call, it was as it always was: Qui\u00f1onero dialing in from his home office in one window and Entin, his PR handler, in another. I asked Qui\u00f1onero what role he felt Facebook had played in the riots and whether it changed the task he saw for Responsible AI. After a long pause, he sidestepped the question, launching into a description of recent work he\u2019d done to promote greater diversity and inclusion among the AI teams. I asked him the question again. His Facebook Portal camera, which uses computer-vision algorithms to track the speaker, began to slowly zoom in on his face as he grew still. \u201cI don\u2019t know that I have an easy answer to that question, Karen,\u201d he said. \u201cIt\u2019s an extremely difficult question to ask me.\u201d Entin, who\u2019d been rapidly pacing with a stoic poker face, grabbed a red stress ball. I asked Qui\u00f1onero why his team hadn\u2019t previously looked at ways to edit Facebook\u2019s content-ranking models to tamp down misinformation and extremism. He told me it was the job of other teams (though none, as I confirmed, have been mandated to work on that task). \u201cIt\u2019s not feasible for the Responsible AI team to study all those things ourselves,\u201d he said. When I asked whether he would consider having his team tackle those issues in the future, he vaguely admitted, \u201cI would agree with you that that is going to be the scope of these types of conversations.\u201d Near the end of our hour-long interview, he began to emphasize that AI was often unfairly painted as \u201cthe culprit.\u201d Regardless of whether Facebook used AI or not, he said, people would still spew lies and hate speech, and that content would still spread across the platform. I pressed him one more time. Certainly he couldn\u2019t believe that algorithms had done absolutely nothing to change the nature of these issues, I said. \u201cI don\u2019t know,\u201d he said with a halting stutter. Then he repeated, with more conviction: \u201cThat\u2019s my honest answer. Honest to God. I don\u2019t know.\u201d","title":"How Facebook got addicted to spreading misinformation"},{"location":"EL5131/Technology/Big%20Tech/Essays/essay0/","text":"Is the protection of privacy worthwhile? Source: https://www.theknowledgeloft.com/gp-essays/is-the-protection-of-privacy-worthwhilegp-essay-5/ Individual privacy has been at the forefront of debates for years with many across the globe constantly weighing the need for privacy against the need for governments to monitor what people are doing in order to prevent events of mass terror. With the rise in terror attacks in the past five years in particular, this discussion has become more critical than ever. The question that needs to be addressed is whether the protection of privacy and thus people\u2019s rights to not have their lives monitored is worth the risks of allowing terrorism, violence and crime to continue to grow, and my view is that the protection of privacy is worthwhile to the extent that it does not impinge on the need for governments to protect the safety and welfare of their people. The protection of privacy is worthwhile as a matter of principle, since it is the only thing that stands between an individual\u2019s freedom of expression and living in a lifeless dystopian state. The protection of privacy is inextricably tied up with the protection of individual freedom, with the loss of privacy effectively meaning the loss of people\u2019s rights to their lives, since every decision, choice and action taken from that point onwards would be monitored by the state. This effectively takes away people\u2019s freedom to choose as every decision must be held to the standards of the law and the people monitoring them, creating an environment where people will never be free to act and behave like themselves. Examples of this sad reality have been described throughout popular culture although with added dramatization such as the dreary depiction of life in George Orwell\u2019s renowned novel \u20181984\u2019 and the movie based on the novel which depicts the dark dystopian future that could await people if they choose to forgo their right to privacy. By giving states control over individual\u2019s privacy, we open the door for states to go further and demand the ability to control where individuals can go and control the skills and knowledge that everyone is allowed to learn. The trickle effects of relinquishing the right to privacy can be seen through dictatorial states such as North Korea where every aspect of everyone\u2019s lives are controlled by the state: from the education system and what is taught to the state media and even the food that are available to most people to eat. We also see this in the People\u2019s Republic of China, where the social credit system, initially meant to address the issue of ungraciousness and crime, has become a tool of mass surveillance and therefore of control. These are prime examples of what could follow should privacy be allowed to be taken away in the name of protection as governments and states might eventually be unsatisfied with just being able to monitor for potential threats, and would instead want the ability to prevent future threats from happening, something which can ultimately only be achieved by complete control of its populace in order to weed out criminal and radical ideological thinking. Thus, the protection of individual privacy is important as it would prevent the descent of democracies into police states as it would be the beginning of individual rights being sacrificed in order to address the issues of crime and terrorism, especially if the only way to do so is to completely control every aspect of people\u2019s lives. Why are individual freedoms desirable, then, or even worth protecting from the all-seeing eye of Big Brother? I venture to argue that individual freedoms, including the right to privacy and non-scrutiny, form the basis of happiness, which is presumably the ultimate goal of society. Even if individual freedoms are not explicitly curtailed by despotic regimes, but instead indirectly curtailed by sympathetic and paternalistic governments, this too might affect the happiness of individuals who otherwise might be able to live life in accordance with their own wishes. Contemporary philosopher Michel Foucault goes one step further in suggesting that the lack of privacy is directly connected to the loss of freedom and thus the loss of happiness through the concept of panopticism. Foucault suggests that external surveillance and the intrusion of privacy associated with this surveillance is ultimately internalised, and each person becomes his or her own guard. This greatly reduces people\u2019s freedom of expression, and the happiness associated with that freedom of living according to their own wishes. A study published by the Applied Research in Quality of Life suggests that an increase in freedoms correlates with an increase in happiness. This understanding and desire presumably underpin historical struggles for freedom, from the \u2018Baltic Way\u2019, where more than two million people formed a human chain across Estonia, Latvia and Lithuania to protest the despotism of the Soviet Union, to the French Revolution, and to how \u201cWallace fought for Scotland, left the name of Wallace to be found, like a wild flower, all over his dear country\u201d. This suggests that individual freedoms are worth fighting for, and as established earlier, the erosion of the right to privacy ultimately cumulates into the erosion of individual freedoms, and should thus be decried. The right to have a life that is private and non privy to the prying eyes of the state is thus essential despite the risks it entails as without privacy, individuals will not be able to live their lives the way the want and express their true selves, effectively destroying the uniqueness and imaginative joy that makes life worth living. However, with the rising threat of crime and violence, the protection of privacy might not be as worthwhile as the need to protect people from acts of mass terror and violence may outweigh this. The ability to monitor individuals would allow states to better address the issues of rising crime and terror by giving them an avenue to better track individuals with a greater risk of being involved in criminal activities. States will also be able to set up alert systems to highlight any actions by individuals that could possibly lead to potentially damaging terrorist or criminal activity such as the planning of crimes of terror attacks by people within their borders. Allowing governments access to sensitive information would have allowed law enforcement agencies to prevent many of the attacks perpetrated by radicals and criminals such as the shooting at a mosque in Christchurch in March 2019 and the multiple school shootings that have taken place in America over the years. In fact, concerns about privacy have actively impeded preemptive measures and investigations. Former Federal Bureau of Investigation chief James Comey argued that encrypted messaging platforms, meant to safeguard the privacy of individual users, deprive governments and intelligence agencies of crucial information, \u201cgiving terrorists a tremendous advantage against us\u201d. Letting go of the desire for privacy might be the necessary sacrifice that will allow governments to finally get a step ahead of these criminals and extremists as the individualised nature of international terrorism and crime today makes it harder and harder for defence agencies to track and protect us against these threats, which could come from anywhere, even from the people living right next door. Relinquishing the right to privacy might be the only way to allow states to properly address the ever changing and complex network of international crime and terror, and protect us from the many different threats that are brewing just under the surface of society. The protection of privacy is furthermore not worthwhile, as it may prevent people and states from taking responsibility for the actions and being held accountable. While privacy is largely individual, it also extends to states when they are being scrutinised by other nations. Allowing both states and individuals to hide behind the shield of privacy allows people of power to basically go about doing whatever they please without fear of condemnation, if they have significant enough means to keep news of their actions contained. For example, countries such as China have long been using the argument of privacy and national sovereignty to prevent the international community from properly investigating accusations of human rights violations against Chinese Muslims in Xinjiang that have been made against them. The country has used its power in the international community and political red tape to assert that what happens within the country is a private Chinese matter that the international community has no business in. Examples such as this illustrate how the idea of privacy can be used in order to prevent entities from being held accountable for their actions, instead hiding behind privacy as a means of burying and news about their actions. Thus, the protection of privacy is not worthwhile when privacy is just used as a tool to conceal the actions of people and states from the outside world in order to shield themselves from having to deal with the eventual repercussions of these actions, and continue to carry out these atrocities without fear of condemnation due to the protection that privacy give them. Overall, whether the protection of privacy is worthwhile depends heavily on the degree that our privacy would be taken away should we choose to stop defending it. Although the relaxing of privacy could have beneficial impacts to society by stopping people and countries from using it as a shield and be held accountable for their actions, and lead to states being more prepared and capable to address the threats that terrorism and crime have towards their communities, the complete relinquishment of privacy might in the long run lead to the loss of all individual freedoms and cause the countries of the world to descend into dystopian police states of our own making.","title":"Is the protection of privacy worthwhile?"},{"location":"EL5131/Technology/Big%20Tech/Essays/essay0/#is-the-protection-of-privacy-worthwhile","text":"Source: https://www.theknowledgeloft.com/gp-essays/is-the-protection-of-privacy-worthwhilegp-essay-5/ Individual privacy has been at the forefront of debates for years with many across the globe constantly weighing the need for privacy against the need for governments to monitor what people are doing in order to prevent events of mass terror. With the rise in terror attacks in the past five years in particular, this discussion has become more critical than ever. The question that needs to be addressed is whether the protection of privacy and thus people\u2019s rights to not have their lives monitored is worth the risks of allowing terrorism, violence and crime to continue to grow, and my view is that the protection of privacy is worthwhile to the extent that it does not impinge on the need for governments to protect the safety and welfare of their people. The protection of privacy is worthwhile as a matter of principle, since it is the only thing that stands between an individual\u2019s freedom of expression and living in a lifeless dystopian state. The protection of privacy is inextricably tied up with the protection of individual freedom, with the loss of privacy effectively meaning the loss of people\u2019s rights to their lives, since every decision, choice and action taken from that point onwards would be monitored by the state. This effectively takes away people\u2019s freedom to choose as every decision must be held to the standards of the law and the people monitoring them, creating an environment where people will never be free to act and behave like themselves. Examples of this sad reality have been described throughout popular culture although with added dramatization such as the dreary depiction of life in George Orwell\u2019s renowned novel \u20181984\u2019 and the movie based on the novel which depicts the dark dystopian future that could await people if they choose to forgo their right to privacy. By giving states control over individual\u2019s privacy, we open the door for states to go further and demand the ability to control where individuals can go and control the skills and knowledge that everyone is allowed to learn. The trickle effects of relinquishing the right to privacy can be seen through dictatorial states such as North Korea where every aspect of everyone\u2019s lives are controlled by the state: from the education system and what is taught to the state media and even the food that are available to most people to eat. We also see this in the People\u2019s Republic of China, where the social credit system, initially meant to address the issue of ungraciousness and crime, has become a tool of mass surveillance and therefore of control. These are prime examples of what could follow should privacy be allowed to be taken away in the name of protection as governments and states might eventually be unsatisfied with just being able to monitor for potential threats, and would instead want the ability to prevent future threats from happening, something which can ultimately only be achieved by complete control of its populace in order to weed out criminal and radical ideological thinking. Thus, the protection of individual privacy is important as it would prevent the descent of democracies into police states as it would be the beginning of individual rights being sacrificed in order to address the issues of crime and terrorism, especially if the only way to do so is to completely control every aspect of people\u2019s lives. Why are individual freedoms desirable, then, or even worth protecting from the all-seeing eye of Big Brother? I venture to argue that individual freedoms, including the right to privacy and non-scrutiny, form the basis of happiness, which is presumably the ultimate goal of society. Even if individual freedoms are not explicitly curtailed by despotic regimes, but instead indirectly curtailed by sympathetic and paternalistic governments, this too might affect the happiness of individuals who otherwise might be able to live life in accordance with their own wishes. Contemporary philosopher Michel Foucault goes one step further in suggesting that the lack of privacy is directly connected to the loss of freedom and thus the loss of happiness through the concept of panopticism. Foucault suggests that external surveillance and the intrusion of privacy associated with this surveillance is ultimately internalised, and each person becomes his or her own guard. This greatly reduces people\u2019s freedom of expression, and the happiness associated with that freedom of living according to their own wishes. A study published by the Applied Research in Quality of Life suggests that an increase in freedoms correlates with an increase in happiness. This understanding and desire presumably underpin historical struggles for freedom, from the \u2018Baltic Way\u2019, where more than two million people formed a human chain across Estonia, Latvia and Lithuania to protest the despotism of the Soviet Union, to the French Revolution, and to how \u201cWallace fought for Scotland, left the name of Wallace to be found, like a wild flower, all over his dear country\u201d. This suggests that individual freedoms are worth fighting for, and as established earlier, the erosion of the right to privacy ultimately cumulates into the erosion of individual freedoms, and should thus be decried. The right to have a life that is private and non privy to the prying eyes of the state is thus essential despite the risks it entails as without privacy, individuals will not be able to live their lives the way the want and express their true selves, effectively destroying the uniqueness and imaginative joy that makes life worth living. However, with the rising threat of crime and violence, the protection of privacy might not be as worthwhile as the need to protect people from acts of mass terror and violence may outweigh this. The ability to monitor individuals would allow states to better address the issues of rising crime and terror by giving them an avenue to better track individuals with a greater risk of being involved in criminal activities. States will also be able to set up alert systems to highlight any actions by individuals that could possibly lead to potentially damaging terrorist or criminal activity such as the planning of crimes of terror attacks by people within their borders. Allowing governments access to sensitive information would have allowed law enforcement agencies to prevent many of the attacks perpetrated by radicals and criminals such as the shooting at a mosque in Christchurch in March 2019 and the multiple school shootings that have taken place in America over the years. In fact, concerns about privacy have actively impeded preemptive measures and investigations. Former Federal Bureau of Investigation chief James Comey argued that encrypted messaging platforms, meant to safeguard the privacy of individual users, deprive governments and intelligence agencies of crucial information, \u201cgiving terrorists a tremendous advantage against us\u201d. Letting go of the desire for privacy might be the necessary sacrifice that will allow governments to finally get a step ahead of these criminals and extremists as the individualised nature of international terrorism and crime today makes it harder and harder for defence agencies to track and protect us against these threats, which could come from anywhere, even from the people living right next door. Relinquishing the right to privacy might be the only way to allow states to properly address the ever changing and complex network of international crime and terror, and protect us from the many different threats that are brewing just under the surface of society. The protection of privacy is furthermore not worthwhile, as it may prevent people and states from taking responsibility for the actions and being held accountable. While privacy is largely individual, it also extends to states when they are being scrutinised by other nations. Allowing both states and individuals to hide behind the shield of privacy allows people of power to basically go about doing whatever they please without fear of condemnation, if they have significant enough means to keep news of their actions contained. For example, countries such as China have long been using the argument of privacy and national sovereignty to prevent the international community from properly investigating accusations of human rights violations against Chinese Muslims in Xinjiang that have been made against them. The country has used its power in the international community and political red tape to assert that what happens within the country is a private Chinese matter that the international community has no business in. Examples such as this illustrate how the idea of privacy can be used in order to prevent entities from being held accountable for their actions, instead hiding behind privacy as a means of burying and news about their actions. Thus, the protection of privacy is not worthwhile when privacy is just used as a tool to conceal the actions of people and states from the outside world in order to shield themselves from having to deal with the eventual repercussions of these actions, and continue to carry out these atrocities without fear of condemnation due to the protection that privacy give them. Overall, whether the protection of privacy is worthwhile depends heavily on the degree that our privacy would be taken away should we choose to stop defending it. Although the relaxing of privacy could have beneficial impacts to society by stopping people and countries from using it as a shield and be held accountable for their actions, and lead to states being more prepared and capable to address the threats that terrorism and crime have towards their communities, the complete relinquishment of privacy might in the long run lead to the loss of all individual freedoms and cause the countries of the world to descend into dystopian police states of our own making.","title":"Is the protection of privacy worthwhile?"},{"location":"EL5131/Technology/Big%20Tech/Essays/essay1/","text":"Is there still privacy in today's world especially given the need for national security? Source: https://www.theknowledgeloft.com/gp-essays/gp-essay-45-is-there-still-privacy-in-todays-world-especially-given-the-need-for-national-security/ \u201cTo be left alone is the most precious thing one can ask of the modern world,\u201d quipped English writer Anthony Burgess. While alluding to solitude and being alone, this quote has seen greater relevance in modern times with respect to one\u2019s privacy. Privacy \u2013 the ability of an individual to keep information regarding him away from the knowledge of others \u2013 is becoming an increasingly debatable issue. Unfortunately, as much as people desire for their personal information to be kept solely to themselves, this is getting increasingly impossible in today\u2019s world as technology has allowed organisations such as governments to tap into personal information against the will of individuals for the purposes of national security. Nonetheless, contrary to popular belief that privacy no longer exists, there are many instances where privacy still remains, such as the cases of political privacy and laws to protect privacy rights of citizens. As such, it is erroneous to claim that there is no more privacy in today\u2019s world. In today\u2019s world, it appears as if privacy may need to be sacrificed for the sake of national security. After various catastrophic acts of terrorism, such as the notable 9/11 attacks against New York\u2019s World Trade Centre, it is quintessential for governments to fulfil their role of protecting their citizens by preventing such acts from happening again. In an increasingly globalised world characterised by greater movements of people and goods across international borders daily, national security needs to be improved to deter any potential threats to a country\u2019s security and the safety of its citizens. With improvements in surveillance technology to collect and analyse data of large numbers of people, governments can easily and clandestinely collect personal information for profiling. This will enable them to more effectively identify potential threats to national security and intervene more quickly. An example of this is the National Counterterrorism Centre which is a US government organisation responsible for national and international counterterrorism efforts. It has the authority to collect, store and analyse extensive data on US citizens, compiled from governmental and non-governmental sources, in order to detect any suspicious behaviour. Former National Security Agent, William Binney, also admitted that the organisation is able to intercept the phone calls, financial transactions and emails of US citizens. In such cases, even though personal information is being shared with governments against the knowledge or consent of individuals, it appears inevitable that the privacy of citizens has to be compromised for greater security and safety for society. Despite the need to intrude into people\u2019s privacy for more security, in a world where people are increasingly concerned about their privacy, governments around the world have also realised their onus to set up laws to protect the privacy of their citizens. In this day and age where technology allows personal information to be easily accessed by others, many nations worldwide have agreed that privacy is a right that should be granted to all people and that individuals should be allowed by law to defend this right. The International Covenant on Civil and Political Rights, adopted by the United Nations since 1966, has an article that protects privacy, which states that \u201cno one shall be subjected to arbitrary or unlawful interference with his privacy, family, home or correspondence\u201d. It also grants everyone the right to be protected by law against any such interference. As such, this treaty acknowledges that there should be regulations set in place to defend the personal information of individuals. To date, 168 states have agreed to the treaty, and have in turn established their own legislations to protect the privacy of their citizens. In Singapore, for instance, the Protection of Private Data Act was enforced in 2013 for this very purpose. It can be seen that privacy still exists in today\u2019s world with the protection of personal information through the help of laws and regulations set up by governments. Moreover, in an increasingly democratic world, the political privacy of ordinary citizens is especially important in ensuring fair and unbiased voting so that they are protected against intimidation by electoral candidates. As more and more nations begin to adopt the voting system in government elections, political privacy is an increasingly pertinent right for all citizens, as candidates voted into parliament are expected to serve all citizens equally, without any bias towards those in favour of them or discrimination against people who voted against them. This is only possible if the choices of individual voters are kept secret. A case in point would be my society, Singapore, where polling stations for elections are set up with individual booths where voters are given their own space to mark their ballot paper. One is also advised to keep his vote a secret. This is also the case for many countries worldwide. Hence, privacy still exists as citizens are able to keep their political choices secret and unknown by other people. All in all, despite privacy being a lawful right and having various means to protect it, it is an undeniable reality that people enjoy less privacy today than in the past, with technology that can potentially allow personal information to be disclosed to various organisations clandestinely, even if for grounds such as national security. That said, it is too extreme to assert that privacy no longer exists in today\u2019s world even if it is increasingly harder to achieve.","title":"Is there still privacy in today's world especially given the need for national security?"},{"location":"EL5131/Technology/Big%20Tech/Essays/essay1/#is-there-still-privacy-in-todays-world-especially-given-the-need-for-national-security","text":"Source: https://www.theknowledgeloft.com/gp-essays/gp-essay-45-is-there-still-privacy-in-todays-world-especially-given-the-need-for-national-security/ \u201cTo be left alone is the most precious thing one can ask of the modern world,\u201d quipped English writer Anthony Burgess. While alluding to solitude and being alone, this quote has seen greater relevance in modern times with respect to one\u2019s privacy. Privacy \u2013 the ability of an individual to keep information regarding him away from the knowledge of others \u2013 is becoming an increasingly debatable issue. Unfortunately, as much as people desire for their personal information to be kept solely to themselves, this is getting increasingly impossible in today\u2019s world as technology has allowed organisations such as governments to tap into personal information against the will of individuals for the purposes of national security. Nonetheless, contrary to popular belief that privacy no longer exists, there are many instances where privacy still remains, such as the cases of political privacy and laws to protect privacy rights of citizens. As such, it is erroneous to claim that there is no more privacy in today\u2019s world. In today\u2019s world, it appears as if privacy may need to be sacrificed for the sake of national security. After various catastrophic acts of terrorism, such as the notable 9/11 attacks against New York\u2019s World Trade Centre, it is quintessential for governments to fulfil their role of protecting their citizens by preventing such acts from happening again. In an increasingly globalised world characterised by greater movements of people and goods across international borders daily, national security needs to be improved to deter any potential threats to a country\u2019s security and the safety of its citizens. With improvements in surveillance technology to collect and analyse data of large numbers of people, governments can easily and clandestinely collect personal information for profiling. This will enable them to more effectively identify potential threats to national security and intervene more quickly. An example of this is the National Counterterrorism Centre which is a US government organisation responsible for national and international counterterrorism efforts. It has the authority to collect, store and analyse extensive data on US citizens, compiled from governmental and non-governmental sources, in order to detect any suspicious behaviour. Former National Security Agent, William Binney, also admitted that the organisation is able to intercept the phone calls, financial transactions and emails of US citizens. In such cases, even though personal information is being shared with governments against the knowledge or consent of individuals, it appears inevitable that the privacy of citizens has to be compromised for greater security and safety for society. Despite the need to intrude into people\u2019s privacy for more security, in a world where people are increasingly concerned about their privacy, governments around the world have also realised their onus to set up laws to protect the privacy of their citizens. In this day and age where technology allows personal information to be easily accessed by others, many nations worldwide have agreed that privacy is a right that should be granted to all people and that individuals should be allowed by law to defend this right. The International Covenant on Civil and Political Rights, adopted by the United Nations since 1966, has an article that protects privacy, which states that \u201cno one shall be subjected to arbitrary or unlawful interference with his privacy, family, home or correspondence\u201d. It also grants everyone the right to be protected by law against any such interference. As such, this treaty acknowledges that there should be regulations set in place to defend the personal information of individuals. To date, 168 states have agreed to the treaty, and have in turn established their own legislations to protect the privacy of their citizens. In Singapore, for instance, the Protection of Private Data Act was enforced in 2013 for this very purpose. It can be seen that privacy still exists in today\u2019s world with the protection of personal information through the help of laws and regulations set up by governments. Moreover, in an increasingly democratic world, the political privacy of ordinary citizens is especially important in ensuring fair and unbiased voting so that they are protected against intimidation by electoral candidates. As more and more nations begin to adopt the voting system in government elections, political privacy is an increasingly pertinent right for all citizens, as candidates voted into parliament are expected to serve all citizens equally, without any bias towards those in favour of them or discrimination against people who voted against them. This is only possible if the choices of individual voters are kept secret. A case in point would be my society, Singapore, where polling stations for elections are set up with individual booths where voters are given their own space to mark their ballot paper. One is also advised to keep his vote a secret. This is also the case for many countries worldwide. Hence, privacy still exists as citizens are able to keep their political choices secret and unknown by other people. All in all, despite privacy being a lawful right and having various means to protect it, it is an undeniable reality that people enjoy less privacy today than in the past, with technology that can potentially allow personal information to be disclosed to various organisations clandestinely, even if for grounds such as national security. That said, it is too extreme to assert that privacy no longer exists in today\u2019s world even if it is increasingly harder to achieve.","title":"Is there still privacy in today's world especially given the need for national security?"},{"location":"EL5131/Technology/Cancel%20Culture/Essays/essay0/","text":"\"Entertainment, not truth, is the priority of the media today,\" Comment. Source: https://www.theknowledgeloft.com/gp-essays/gp-essay-11-entertainment-not-truth-is-the-priority-of-the-media-today-comment/ Will Rogers once said, \u201cAll I know is just what I read in the papers, and that\u2019s an alibi for my ignorance.\u201d It is now fashionable to degrade the media for obscuring verifiable facts that is in the public interest to be made aware in favour of providing amusement and enjoyment to its audience. However, it is important to recognise that media is not a homogenous collective and the priorities of the media cannot be reduced to a simple dichotomy. From mainstream media to para-journalism, to New Media, various forms of media have varying priorities, and this is dependent on how they generate profit and the desires of their target audience. While mainstream media, which refers collectively to various large mass news media that are capable of influencing a large number of people, and both reflect and shape prevailing currents of thought, does at times obfuscate the truth, this may not be because it prioritises entertainment or strictly speaking the pleasure of its readership. It is arguably true that mainstream media sources have devoted more resources to entertainment news, rather than news that strictly concerns public interests. A quick perusal of The Straits Times suggests that more sections of the newspaper are dedicated to \u2018lifestyle news\u2019, which covers celebrity news, travel tips and food recommendations or recipes. The fact that major mainstream news sites like the British Broadcasting Company (BBC) and Cable News Network (CNN) ran widespread coverage on a plump rat being extracted from a manhole in Germany suggests that amusing readers has perhaps become a greater priority than ever before. Whether it has overtaken bringing the truth to light, however, is still under question. Furthermore, it should be acknowledged that mainstream media does also tend towards sensationalism and \u2018excessive coverage\u2019 in order to sustain their viewership. For instance, following the release of a report on carcinogenic processed meat by the World Health Organization, mainstream media coverage of the report and its contents were overblown and often distorted the findings of the report. Even The Straits Times is guilty of sensationalizing reports of crime. This is presumably because mainstream media sources, especially those which use online mediums, tend to sensationalize important news to get \u2018clicks and likes\u2019, which in turn drives up their advertisement revenue. That said, while sensationalism distorts the truth, this does not directly mean that entertainment is prioritised by the media source. In this case, the angle taken by mainstream media sources hardly brings any pleasure to its viewership due to the nature of the news reported. Nonetheless, for every demonstration of poor journalistic quality by mainstream media, the same mainstream media sources demonstrate their commitment to the truth. CNN correspondent Jim Acosta, for instance, was evicted from the White House in November 2018 in an admirable attempt to call out and question President Donald Trump about \u2018half-truths\u2019 peddled throughout the course of his presidency. Instead of a simple dichotomy between entertainment and truth, I venture to argue that mainstream media are simply peddling agendas, with entertainment and truth alternatively taking precedence depending on how it fits into their agenda. Applying the same set of criteria to para-journalism, which refers to journalism that is heavily coloured by the opinions of individual reporters, it appears that this form of media seems to prioritize entertainment over the truth. \u2018Tabloids\u2019 which fall under this category care little about public affairs, and instead veer into stories about everyday life: about its disruptions and exaltations, about crime, illness, the celebrity, the damsel in distress, the hero and the rescue. They are about unapologetically appealing to the emotions of the public, and glossing over the complexities of public life. Amal Clooney, renowned international humanitarian lawyer, enjoyed extensive coverage for \u2018showing off\u2019 her \u2018baby bump\u2019 and her \u2018courtroom attire\u2019 at a court hearing about the Yemeni humanitarian crisis. Clearly, para-journalists pay little heed to more important issues in their bid to feed the insatiable need of their audience for entertainment and gossip. Against the backdrop of a parliament flailing in the face of Brexit, tabloids like the Daily Express have instead chosen to cover Duchess of Sussex Meghan Markle and her many controversies, even touting her as \u2018Duchess Difficult\u2019. Are these media sources at least accurate in their reporting (never mind their obsession with covering the glitzy and glamorous) or do they compromise the truth in their pursuit of pleasure? It appears that tabloids often pay lip-service to the notion of truth \u2013 the feud narrative between Meghan Markle and Kate Middleton has been thoroughly debunked by Buckingham Palace, and rumours about her being difficult with staff have not been verified with empirical evidence. It appears that parajournalism is a form of media which has taken its desire for entertaining material to the extreme, and completely forsaken the truth in the process. Unlike mainstream media, which often pushes a political agenda to further their own interests, para-journalists simply benefit from a readership that constantly desires entertainment. It would thus make sense for such forms of media to prioritize entertainment over telling the truth. Finally, I venture to argue that New Media, which refers to interactive digital media that incorporates two-way communication, tends to prioritise telling the truth over the amusement of its readership. Whether it successfully does so is another question. Alternative media like Wikipedia and Wikileaks for instance, have presenting the truth at the heart of what they do. Wikipedia allows its curious readership to discover the truth about a wide range of topics under the sun, whereas Wikileaks publishes news leaks and classified media provided by anonymous sources with the intention of challenging narratives peddled by mainstream media. Evidently, rather than creating a fictional narrative that is meant to amuse, alternative media prefers to challenge conventional narratives with the intention of eventually uncovering the truth. This is because Wikipedia profits off telling the truth as users who trust its reliability and believe in its purpose donate an amount of USD $15 on average. That being said, social media platforms, which are also included under the umbrella term of \u2018New Media\u2019, are also in danger of compromising the truth for entertainment. The nature of social media platforms and their business model means that algorithms pick up on the preferences of individual users, and thereafter promote articles or advertisements that it believes these individual users are inclined towards. This extends towards other issues in the public sphere when algorithms used by social media platforms contribute to confirmation bias; users are fed articles that reaffirm their existing views, which to a certain extent reaffirms them and brings about pleasure, be it in the form of self-righteous anger or otherwise. 47% of Conservatives are likely to see Facebook posts aligned to their own views, according to a Pews Research report. Again, as mediums under the umbrella term of \u2018New Media\u2019 are so diverse in their motivations, whether New Media prioritises the truth over entertainment depends on how these motivations fit into their business model and align themselves with the profit incentive.","title":"\"Entertainment, not truth, is the priority of the media today,\" Comment."},{"location":"EL5131/Technology/Cancel%20Culture/Essays/essay0/#entertainment-not-truth-is-the-priority-of-the-media-today-comment","text":"Source: https://www.theknowledgeloft.com/gp-essays/gp-essay-11-entertainment-not-truth-is-the-priority-of-the-media-today-comment/ Will Rogers once said, \u201cAll I know is just what I read in the papers, and that\u2019s an alibi for my ignorance.\u201d It is now fashionable to degrade the media for obscuring verifiable facts that is in the public interest to be made aware in favour of providing amusement and enjoyment to its audience. However, it is important to recognise that media is not a homogenous collective and the priorities of the media cannot be reduced to a simple dichotomy. From mainstream media to para-journalism, to New Media, various forms of media have varying priorities, and this is dependent on how they generate profit and the desires of their target audience. While mainstream media, which refers collectively to various large mass news media that are capable of influencing a large number of people, and both reflect and shape prevailing currents of thought, does at times obfuscate the truth, this may not be because it prioritises entertainment or strictly speaking the pleasure of its readership. It is arguably true that mainstream media sources have devoted more resources to entertainment news, rather than news that strictly concerns public interests. A quick perusal of The Straits Times suggests that more sections of the newspaper are dedicated to \u2018lifestyle news\u2019, which covers celebrity news, travel tips and food recommendations or recipes. The fact that major mainstream news sites like the British Broadcasting Company (BBC) and Cable News Network (CNN) ran widespread coverage on a plump rat being extracted from a manhole in Germany suggests that amusing readers has perhaps become a greater priority than ever before. Whether it has overtaken bringing the truth to light, however, is still under question. Furthermore, it should be acknowledged that mainstream media does also tend towards sensationalism and \u2018excessive coverage\u2019 in order to sustain their viewership. For instance, following the release of a report on carcinogenic processed meat by the World Health Organization, mainstream media coverage of the report and its contents were overblown and often distorted the findings of the report. Even The Straits Times is guilty of sensationalizing reports of crime. This is presumably because mainstream media sources, especially those which use online mediums, tend to sensationalize important news to get \u2018clicks and likes\u2019, which in turn drives up their advertisement revenue. That said, while sensationalism distorts the truth, this does not directly mean that entertainment is prioritised by the media source. In this case, the angle taken by mainstream media sources hardly brings any pleasure to its viewership due to the nature of the news reported. Nonetheless, for every demonstration of poor journalistic quality by mainstream media, the same mainstream media sources demonstrate their commitment to the truth. CNN correspondent Jim Acosta, for instance, was evicted from the White House in November 2018 in an admirable attempt to call out and question President Donald Trump about \u2018half-truths\u2019 peddled throughout the course of his presidency. Instead of a simple dichotomy between entertainment and truth, I venture to argue that mainstream media are simply peddling agendas, with entertainment and truth alternatively taking precedence depending on how it fits into their agenda. Applying the same set of criteria to para-journalism, which refers to journalism that is heavily coloured by the opinions of individual reporters, it appears that this form of media seems to prioritize entertainment over the truth. \u2018Tabloids\u2019 which fall under this category care little about public affairs, and instead veer into stories about everyday life: about its disruptions and exaltations, about crime, illness, the celebrity, the damsel in distress, the hero and the rescue. They are about unapologetically appealing to the emotions of the public, and glossing over the complexities of public life. Amal Clooney, renowned international humanitarian lawyer, enjoyed extensive coverage for \u2018showing off\u2019 her \u2018baby bump\u2019 and her \u2018courtroom attire\u2019 at a court hearing about the Yemeni humanitarian crisis. Clearly, para-journalists pay little heed to more important issues in their bid to feed the insatiable need of their audience for entertainment and gossip. Against the backdrop of a parliament flailing in the face of Brexit, tabloids like the Daily Express have instead chosen to cover Duchess of Sussex Meghan Markle and her many controversies, even touting her as \u2018Duchess Difficult\u2019. Are these media sources at least accurate in their reporting (never mind their obsession with covering the glitzy and glamorous) or do they compromise the truth in their pursuit of pleasure? It appears that tabloids often pay lip-service to the notion of truth \u2013 the feud narrative between Meghan Markle and Kate Middleton has been thoroughly debunked by Buckingham Palace, and rumours about her being difficult with staff have not been verified with empirical evidence. It appears that parajournalism is a form of media which has taken its desire for entertaining material to the extreme, and completely forsaken the truth in the process. Unlike mainstream media, which often pushes a political agenda to further their own interests, para-journalists simply benefit from a readership that constantly desires entertainment. It would thus make sense for such forms of media to prioritize entertainment over telling the truth. Finally, I venture to argue that New Media, which refers to interactive digital media that incorporates two-way communication, tends to prioritise telling the truth over the amusement of its readership. Whether it successfully does so is another question. Alternative media like Wikipedia and Wikileaks for instance, have presenting the truth at the heart of what they do. Wikipedia allows its curious readership to discover the truth about a wide range of topics under the sun, whereas Wikileaks publishes news leaks and classified media provided by anonymous sources with the intention of challenging narratives peddled by mainstream media. Evidently, rather than creating a fictional narrative that is meant to amuse, alternative media prefers to challenge conventional narratives with the intention of eventually uncovering the truth. This is because Wikipedia profits off telling the truth as users who trust its reliability and believe in its purpose donate an amount of USD $15 on average. That being said, social media platforms, which are also included under the umbrella term of \u2018New Media\u2019, are also in danger of compromising the truth for entertainment. The nature of social media platforms and their business model means that algorithms pick up on the preferences of individual users, and thereafter promote articles or advertisements that it believes these individual users are inclined towards. This extends towards other issues in the public sphere when algorithms used by social media platforms contribute to confirmation bias; users are fed articles that reaffirm their existing views, which to a certain extent reaffirms them and brings about pleasure, be it in the form of self-righteous anger or otherwise. 47% of Conservatives are likely to see Facebook posts aligned to their own views, according to a Pews Research report. Again, as mediums under the umbrella term of \u2018New Media\u2019 are so diverse in their motivations, whether New Media prioritises the truth over entertainment depends on how these motivations fit into their business model and align themselves with the profit incentive.","title":"\"Entertainment, not truth, is the priority of the media today,\" Comment."},{"location":"PC5131/","text":"PC5131 Topics Rotational Mechanics Waves I Waves II (Superposition) Physical Optics Geometric Optics","title":"Home"},{"location":"PC5131/#pc5131","text":"","title":"PC5131"},{"location":"PC5131/#topics","text":"Rotational Mechanics Waves I Waves II (Superposition) Physical Optics Geometric Optics","title":"Topics"},{"location":"PC5131/Rotational%20Mechanics/","text":"Rotational Mechanics (yeet.) Chapter 1 of PC5131 is Rotational Mechanics, which is problematic. The topic is just a scam, since it is just all of Mechanics but with Angular (not the painful JS Framework). Surmised below is a list of equations which can probably help one understand Rotational Mechanics in the context of normal Mechanical concepts. Good Luck. Translational and Rotational Counterparts It's just everything we have already learnt, but they added angular in front of it. So let's just walk through a list of equations, and their rotational counterparts. Translational Rotational \\(v_f = v_i + at\\) \\(\\omega_f = \\omega_i + \\alpha t\\) \\(\\Delta x = v_i t + \\frac{1}{2} at^2 = \\frac{v_f+v_i}{2}t\\) \\(\\Delta\\theta=\\omega_it+\\frac{1}{2}\\alpha t^2 = \\frac{\\omega_i+\\omega_f}{2}t\\) \\(v_f^2 = v_i^2 + 2a\\Delta x\\) \\(\\omega_f^2 = \\omega_i^2 + 2\\alpha\\Delta\\theta\\) \\(F_{net} = ma\\) \\(\\tau_{net} = I\\alpha\\) \\(p = mv\\) \\(L = I\\omega\\) \\(J = F\\Delta t\\) \\(\\Delta L = \\tau\\Delta t\\) \\(W = F \\cdot s\\) \\(W = \\tau \\cdot \\Delta\\theta\\) \\(E_K = \\frac{1}{2}mv^2\\) \\(E_L = \\frac{1}{2}I\\omega^2\\) \\(P = \\frac{W}{t} = F \\cdot v\\) \\(P = \\tau \\cdot \\omega\\) Moment of Inertia, \\(I\\) Do note that \\(I\\) is slightly more cursed, and the following table demonstrates the definition of \\(I\\) : Situation \\(I\\) \\(\\beta\\) Moment of Inertia off from \\(I_{cm}\\) \\(I = I_{cm} + Md^2\\) - Moment of Inertia of Multiple Parts \\(I = \\sum I_{part}\\) - Hoop (from the centre) \\(I = MR^2\\) \\(1\\) Circular Disc (from the centre) \\(I = \\frac{1}{2}MR^2\\) \\(\\frac{1}{2}\\) Circular Disc (through diameter) \\(I = \\frac{1}{4}MR^2\\) \\(\\frac{1}{4}\\) Long Rod (from the centre) \\(I = \\frac{1}{12}ML^2\\) \\(\\frac{1}{12}\\) Long Rod (from end) \\(I = \\frac{1}{3}ML^2\\) \\(\\frac{1}{3}\\) Solid Sphere \\(I = \\frac{2}{5}MR^2\\) \\(\\frac{2}5\\) Hollow Sphere \\(I = \\frac{2}{3} MR^2\\) \\(\\frac{2}3\\) Shifting from Translational to Rotational We also note that we can relate all the quantities to each other via the variable \\(r\\) , as shown below: Translational Rotational \\(\\Delta x\\) \\(r\\Delta\\theta\\) \\(v\\) \\(r\\omega\\) \\(a\\) \\(r\\alpha\\) \\(I\\) \\(\\beta mr^2\\) \\(\\vec F \\times \\vec{r}\\) \\(\\vec \\tau\\) \\(\\vec r \\times \\vec p\\) \\(\\vec L\\) Conservation Cases Cases to be considered for collisions: Situation Translational Rotational Elastic Collision COE, COM applies COE, COAM applies Inelastic Collison COM applies, COE does not COAM applies, COE does not Completely Inelastic Collision (objects stick together) COM applies, COE does not COAM applies, COE does not Some force acting throughout the motion (e.g. Gravity) COE applies, COM does not COE applies, COAM does not Ramp Name Quantity Rotational Kinetic Energy \\(K_{rot} = \\frac{\\beta}{1+\\beta}mgh\\) Translational Kinetic Energy \\(K_{trans} = \\frac{1}{1+\\beta}mgh\\) Velocity of Centre of Mass \\(v_{cm} = \\sqrt{\\frac{2gh}{1 + \\beta}}\\) Acceleration of Centre of Mass \\(a_{cm} = \\frac{v_{cm}^2}{2\\Delta x} = \\frac{g\\sin\\theta}{1+\\beta}\\) Velocity at Top of Loop- de -Loop (Radius \\(R\\) ) \\(v_{loop} = \\sqrt{g(R-r)}\\) Minimum height, \\(h\\) for Loop- de -Loop \\(h = \\frac{5+\\beta}{2}(R-r)\\) Loop- de -Loop Loop- de -Loop for normal circular motion: \\[\\begin{align*}N &= \\frac{mv^2}R - mg = 0 \\\\ v^2 &= Rg \\\\ \\frac{1}{2}mv^2 &= mg(h-2R) \\\\ v^2 &= 2g(h-2R) = Rg \\\\ 2h - 4R &= R \\\\ h &= 2.5 R\\end{align*}\\] For rigid body of radius \\(r\\) : \\[\\begin{align*}N &= \\frac{mv^2}{R-r} - mg = 0 \\\\ v^2 &= g(R-r) \\\\ \\frac{1}{2}(1+\\beta)mv^2 &= mg(h+2r-2R) \\\\ (1+\\beta)g(R-r) &= 2g (h+2r-2R) \\\\ (1+\\beta)(R-r) &= 2h - 4(R-r) \\\\ 2h &= (5+\\beta)(R - r) \\\\ h &= \\frac{5+\\beta}{2}(R-r) \\end{align*}\\]","title":"Rotational Mechanics"},{"location":"PC5131/Rotational%20Mechanics/#rotational-mechanics","text":"(yeet.) Chapter 1 of PC5131 is Rotational Mechanics, which is problematic. The topic is just a scam, since it is just all of Mechanics but with Angular (not the painful JS Framework). Surmised below is a list of equations which can probably help one understand Rotational Mechanics in the context of normal Mechanical concepts. Good Luck.","title":"Rotational Mechanics"},{"location":"PC5131/Rotational%20Mechanics/#translational-and-rotational-counterparts","text":"It's just everything we have already learnt, but they added angular in front of it. So let's just walk through a list of equations, and their rotational counterparts. Translational Rotational \\(v_f = v_i + at\\) \\(\\omega_f = \\omega_i + \\alpha t\\) \\(\\Delta x = v_i t + \\frac{1}{2} at^2 = \\frac{v_f+v_i}{2}t\\) \\(\\Delta\\theta=\\omega_it+\\frac{1}{2}\\alpha t^2 = \\frac{\\omega_i+\\omega_f}{2}t\\) \\(v_f^2 = v_i^2 + 2a\\Delta x\\) \\(\\omega_f^2 = \\omega_i^2 + 2\\alpha\\Delta\\theta\\) \\(F_{net} = ma\\) \\(\\tau_{net} = I\\alpha\\) \\(p = mv\\) \\(L = I\\omega\\) \\(J = F\\Delta t\\) \\(\\Delta L = \\tau\\Delta t\\) \\(W = F \\cdot s\\) \\(W = \\tau \\cdot \\Delta\\theta\\) \\(E_K = \\frac{1}{2}mv^2\\) \\(E_L = \\frac{1}{2}I\\omega^2\\) \\(P = \\frac{W}{t} = F \\cdot v\\) \\(P = \\tau \\cdot \\omega\\)","title":"Translational and Rotational Counterparts"},{"location":"PC5131/Rotational%20Mechanics/#moment-of-inertia-i","text":"Do note that \\(I\\) is slightly more cursed, and the following table demonstrates the definition of \\(I\\) : Situation \\(I\\) \\(\\beta\\) Moment of Inertia off from \\(I_{cm}\\) \\(I = I_{cm} + Md^2\\) - Moment of Inertia of Multiple Parts \\(I = \\sum I_{part}\\) - Hoop (from the centre) \\(I = MR^2\\) \\(1\\) Circular Disc (from the centre) \\(I = \\frac{1}{2}MR^2\\) \\(\\frac{1}{2}\\) Circular Disc (through diameter) \\(I = \\frac{1}{4}MR^2\\) \\(\\frac{1}{4}\\) Long Rod (from the centre) \\(I = \\frac{1}{12}ML^2\\) \\(\\frac{1}{12}\\) Long Rod (from end) \\(I = \\frac{1}{3}ML^2\\) \\(\\frac{1}{3}\\) Solid Sphere \\(I = \\frac{2}{5}MR^2\\) \\(\\frac{2}5\\) Hollow Sphere \\(I = \\frac{2}{3} MR^2\\) \\(\\frac{2}3\\)","title":"Moment of Inertia, \\(I\\)"},{"location":"PC5131/Rotational%20Mechanics/#shifting-from-translational-to-rotational","text":"We also note that we can relate all the quantities to each other via the variable \\(r\\) , as shown below: Translational Rotational \\(\\Delta x\\) \\(r\\Delta\\theta\\) \\(v\\) \\(r\\omega\\) \\(a\\) \\(r\\alpha\\) \\(I\\) \\(\\beta mr^2\\) \\(\\vec F \\times \\vec{r}\\) \\(\\vec \\tau\\) \\(\\vec r \\times \\vec p\\) \\(\\vec L\\)","title":"Shifting from Translational to Rotational"},{"location":"PC5131/Rotational%20Mechanics/#conservation-cases","text":"Cases to be considered for collisions: Situation Translational Rotational Elastic Collision COE, COM applies COE, COAM applies Inelastic Collison COM applies, COE does not COAM applies, COE does not Completely Inelastic Collision (objects stick together) COM applies, COE does not COAM applies, COE does not Some force acting throughout the motion (e.g. Gravity) COE applies, COM does not COE applies, COAM does not","title":"Conservation Cases"},{"location":"PC5131/Rotational%20Mechanics/#ramp","text":"Name Quantity Rotational Kinetic Energy \\(K_{rot} = \\frac{\\beta}{1+\\beta}mgh\\) Translational Kinetic Energy \\(K_{trans} = \\frac{1}{1+\\beta}mgh\\) Velocity of Centre of Mass \\(v_{cm} = \\sqrt{\\frac{2gh}{1 + \\beta}}\\) Acceleration of Centre of Mass \\(a_{cm} = \\frac{v_{cm}^2}{2\\Delta x} = \\frac{g\\sin\\theta}{1+\\beta}\\) Velocity at Top of Loop- de -Loop (Radius \\(R\\) ) \\(v_{loop} = \\sqrt{g(R-r)}\\) Minimum height, \\(h\\) for Loop- de -Loop \\(h = \\frac{5+\\beta}{2}(R-r)\\)","title":"Ramp"},{"location":"PC5131/Rotational%20Mechanics/#loop-de-loop","text":"Loop- de -Loop for normal circular motion: \\[\\begin{align*}N &= \\frac{mv^2}R - mg = 0 \\\\ v^2 &= Rg \\\\ \\frac{1}{2}mv^2 &= mg(h-2R) \\\\ v^2 &= 2g(h-2R) = Rg \\\\ 2h - 4R &= R \\\\ h &= 2.5 R\\end{align*}\\] For rigid body of radius \\(r\\) : \\[\\begin{align*}N &= \\frac{mv^2}{R-r} - mg = 0 \\\\ v^2 &= g(R-r) \\\\ \\frac{1}{2}(1+\\beta)mv^2 &= mg(h+2r-2R) \\\\ (1+\\beta)g(R-r) &= 2g (h+2r-2R) \\\\ (1+\\beta)(R-r) &= 2h - 4(R-r) \\\\ 2h &= (5+\\beta)(R - r) \\\\ h &= \\frac{5+\\beta}{2}(R-r) \\end{align*}\\]","title":"Loop-de-Loop"},{"location":"PC5131/Waves%20I/","text":"Waves I (yeet.) Chapter 2 of PC5131 is Waves... I. This is effectively singular waves, rather than interfering waves. This includes normal wave properties and equations, wave intensity, string waves, polarisation, sound waves (longitudinal waves) and the Doppler Effect. This is relatively easier than the next topic (Waves II) so uh yay thank god only this much is tested in the Common Test on Monday. Anyways yeah good luck, and let's see what comes out of this, right? Aight. See yah. Basics Wave Quantities \\[\\begin{align*}\\omega &=2\\pi f = \\frac{2\\pi}{T}\\\\k &= \\frac{2\\pi}{\\lambda}\\\\v &= f\\lambda = \\frac{f}{T} = \\frac{\\omega}{k}\\end{align*}\\] Wave Equation \\[\\begin{align*}\\psi(x, t)&=A\\sin(\\phi_x -\\phi_t + \\phi_0)\\\\&=A\\sin(kx-\\omega t+\\phi_0)\\end{align*}\\] For wave moving backwards: \\[\\psi(x, t) = A\\sin(kx+\\omega t+\\phi_0)\\] Phase Difference \\[\\begin{align*}\\Delta\\phi &= \\phi_2 - \\phi_1 \\\\ &= (kx_2 - \\omega t + \\phi_0) - (kx_1-\\omega t+\\phi_0) \\\\ &= k(x_2 - x_1) \\\\ &= k\\Delta x \\\\ &= 2\\pi \\frac{\\Delta x}\\lambda \\\\ &= -\\omega \\Delta t \\\\ &=-2\\pi \\frac{\\Delta t}{T} \\end{align*}\\] This applies to both \\(\\Delta t\\) and \\(\\Delta x\\) , which is great. If the wave is moving backwards, the phase difference in terms of \\(\\Delta t\\) is positive, not negative. Intensity of Wave \\[\\begin{align*} I &= \\frac{P}{A}\\\\&=\\frac{P}{4\\pi r^2} \\\\ I &\\propto \\frac{1}{r^2} \\\\ \\frac{I_1}{I_2} &= \\left(\\frac{r_2}{r_1}\\right)^2 \\end{align*}\\] String Wave \\[v = \\sqrt{\\frac{T}{\\mu}}\\] Polarisation Polarising Filter \\[\\begin{align*}I_1 &= \\frac{I_0}{2}\\\\A_{n+1} &= A_n \\cos\\phi_{n+1,n}\\\\I_{n+1} &= I_n \\cos^2\\phi_{n+1,n}\\end{align*}\\] Brewster's Angle \\[\\begin{align*}n_a \\sin\\theta_i &= n_a \\sin\\theta_{reflect} \\\\ n_a \\sin\\theta_i &= n_b \\sin\\theta_{refract} \\\\ n_a \\sin\\theta_{reflect} &= n_b \\sin\\theta_{refract}\\end{align*}\\] From here, we note the following for Brewster's Angle , which is defined as \\(\\theta_b\\) . \\[\\begin{align*}\\theta_{reflect} &= \\theta_{brewster} = \\theta_b \\\\ \\theta_{refract} &= \\theta_{polarised} = \\theta_p \\\\ \\theta_{b} + \\theta_{p} &= \\frac{\\pi}2 \\\\ \\sin\\theta_p &= \\cos\\theta_b \\\\ n_a \\sin\\theta_{b} &= n_b \\sin\\theta_{p} = n_b\\cos\\theta_b \\\\ \\frac{\\sin\\theta_b}{\\cos\\theta_b} &= \\frac{n_b}{n_a} \\\\ \\tan\\theta_b &= \\frac{n_b}{n_a} \\\\ \\theta_b &= tan^{-1}\\left(\\frac{n_b}{n_a} \\right)\\end{align*}\\] Sound Speeds of Sound Condition Speed 0\u00b0C (Standard Temperature and Pressure) 330 m/s 30\u00b0C (Room Temperature and Pressure) 343 m/s T\u00b0C (Any Temperature) \\(331.5 \\times \\sqrt{1 + \\frac{T}{273.15}}\\) Doppler Effect \\[f' = f \\frac{c \\pm v_o}{c \\mp v_s}\\] Source Observer Numerator Denominator Remarks stationary stationary c c - approaching stationary \\(c\\) \\(c-v_s\\) - moving away stationary \\(c\\) \\(c+v_s\\) - stationary approaching \\(c+v_o\\) \\(c\\) - stationary, moving away \\(c-v_o\\) \\(c\\) - approaching approaching \\(c+v_o\\) \\(c-v_s\\) maximum moving away approaching \\(c+v_o\\) \\(c+v_s\\) - approaching moving away \\(c-v_o\\) \\(c-v_s\\) - moving away moving away \\(c-v_o\\) \\(c+v_s\\) minimum Just draw a damn diagram, for god's sake.","title":"Waves I"},{"location":"PC5131/Waves%20I/#waves-i","text":"(yeet.) Chapter 2 of PC5131 is Waves... I. This is effectively singular waves, rather than interfering waves. This includes normal wave properties and equations, wave intensity, string waves, polarisation, sound waves (longitudinal waves) and the Doppler Effect. This is relatively easier than the next topic (Waves II) so uh yay thank god only this much is tested in the Common Test on Monday. Anyways yeah good luck, and let's see what comes out of this, right? Aight. See yah.","title":"Waves I"},{"location":"PC5131/Waves%20I/#basics","text":"","title":"Basics"},{"location":"PC5131/Waves%20I/#wave-quantities","text":"\\[\\begin{align*}\\omega &=2\\pi f = \\frac{2\\pi}{T}\\\\k &= \\frac{2\\pi}{\\lambda}\\\\v &= f\\lambda = \\frac{f}{T} = \\frac{\\omega}{k}\\end{align*}\\]","title":"Wave Quantities"},{"location":"PC5131/Waves%20I/#wave-equation","text":"\\[\\begin{align*}\\psi(x, t)&=A\\sin(\\phi_x -\\phi_t + \\phi_0)\\\\&=A\\sin(kx-\\omega t+\\phi_0)\\end{align*}\\] For wave moving backwards: \\[\\psi(x, t) = A\\sin(kx+\\omega t+\\phi_0)\\]","title":"Wave Equation"},{"location":"PC5131/Waves%20I/#phase-difference","text":"\\[\\begin{align*}\\Delta\\phi &= \\phi_2 - \\phi_1 \\\\ &= (kx_2 - \\omega t + \\phi_0) - (kx_1-\\omega t+\\phi_0) \\\\ &= k(x_2 - x_1) \\\\ &= k\\Delta x \\\\ &= 2\\pi \\frac{\\Delta x}\\lambda \\\\ &= -\\omega \\Delta t \\\\ &=-2\\pi \\frac{\\Delta t}{T} \\end{align*}\\] This applies to both \\(\\Delta t\\) and \\(\\Delta x\\) , which is great. If the wave is moving backwards, the phase difference in terms of \\(\\Delta t\\) is positive, not negative.","title":"Phase Difference"},{"location":"PC5131/Waves%20I/#intensity-of-wave","text":"\\[\\begin{align*} I &= \\frac{P}{A}\\\\&=\\frac{P}{4\\pi r^2} \\\\ I &\\propto \\frac{1}{r^2} \\\\ \\frac{I_1}{I_2} &= \\left(\\frac{r_2}{r_1}\\right)^2 \\end{align*}\\]","title":"Intensity of Wave"},{"location":"PC5131/Waves%20I/#string-wave","text":"\\[v = \\sqrt{\\frac{T}{\\mu}}\\]","title":"String Wave"},{"location":"PC5131/Waves%20I/#polarisation","text":"","title":"Polarisation"},{"location":"PC5131/Waves%20I/#polarising-filter","text":"\\[\\begin{align*}I_1 &= \\frac{I_0}{2}\\\\A_{n+1} &= A_n \\cos\\phi_{n+1,n}\\\\I_{n+1} &= I_n \\cos^2\\phi_{n+1,n}\\end{align*}\\]","title":"Polarising Filter"},{"location":"PC5131/Waves%20I/#brewsters-angle","text":"\\[\\begin{align*}n_a \\sin\\theta_i &= n_a \\sin\\theta_{reflect} \\\\ n_a \\sin\\theta_i &= n_b \\sin\\theta_{refract} \\\\ n_a \\sin\\theta_{reflect} &= n_b \\sin\\theta_{refract}\\end{align*}\\] From here, we note the following for Brewster's Angle , which is defined as \\(\\theta_b\\) . \\[\\begin{align*}\\theta_{reflect} &= \\theta_{brewster} = \\theta_b \\\\ \\theta_{refract} &= \\theta_{polarised} = \\theta_p \\\\ \\theta_{b} + \\theta_{p} &= \\frac{\\pi}2 \\\\ \\sin\\theta_p &= \\cos\\theta_b \\\\ n_a \\sin\\theta_{b} &= n_b \\sin\\theta_{p} = n_b\\cos\\theta_b \\\\ \\frac{\\sin\\theta_b}{\\cos\\theta_b} &= \\frac{n_b}{n_a} \\\\ \\tan\\theta_b &= \\frac{n_b}{n_a} \\\\ \\theta_b &= tan^{-1}\\left(\\frac{n_b}{n_a} \\right)\\end{align*}\\]","title":"Brewster's Angle"},{"location":"PC5131/Waves%20I/#sound","text":"","title":"Sound"},{"location":"PC5131/Waves%20I/#speeds-of-sound","text":"Condition Speed 0\u00b0C (Standard Temperature and Pressure) 330 m/s 30\u00b0C (Room Temperature and Pressure) 343 m/s T\u00b0C (Any Temperature) \\(331.5 \\times \\sqrt{1 + \\frac{T}{273.15}}\\)","title":"Speeds of Sound"},{"location":"PC5131/Waves%20I/#doppler-effect","text":"\\[f' = f \\frac{c \\pm v_o}{c \\mp v_s}\\] Source Observer Numerator Denominator Remarks stationary stationary c c - approaching stationary \\(c\\) \\(c-v_s\\) - moving away stationary \\(c\\) \\(c+v_s\\) - stationary approaching \\(c+v_o\\) \\(c\\) - stationary, moving away \\(c-v_o\\) \\(c\\) - approaching approaching \\(c+v_o\\) \\(c-v_s\\) maximum moving away approaching \\(c+v_o\\) \\(c+v_s\\) - approaching moving away \\(c-v_o\\) \\(c-v_s\\) - moving away moving away \\(c-v_o\\) \\(c+v_s\\) minimum Just draw a damn diagram, for god's sake.","title":"Doppler Effect"}]}